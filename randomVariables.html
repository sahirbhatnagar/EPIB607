<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Random Variables | EPIB607</title>
<meta name="author" content="Sahir Bhatnagar and James A Hanley">
<meta name="description" content="8.1 Objectives  This central chapter addresses a fundamental concept, namely the variance of a random variable. It gives the laws governing the variance of a sum of 2, or (especially) \(n\) random...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 8 Random Variables | EPIB607">
<meta property="og:type" content="book">
<meta property="og:url" content="https://sahirbhatnagar.com/EPIB607/randomVariables.html">
<meta property="og:description" content="8.1 Objectives  This central chapter addresses a fundamental concept, namely the variance of a random variable. It gives the laws governing the variance of a sum of 2, or (especially) \(n\) random...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Random Variables | EPIB607">
<meta name="twitter:description" content="8.1 Objectives  This central chapter addresses a fundamental concept, namely the variance of a random variable. It gives the laws governing the variance of a sum of 2, or (especially) \(n\) random...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding-0.20/datatables.js"></script><link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet">
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script><link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet">
<script src="libs/selectize-0.12.0/selectize.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="templates/bs4_style.css">
<link rel="stylesheet" href="templates/ims-style.css">
<link rel="stylesheet" href="templates/corrections.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">EPIB607</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="schedule.html">Schedule</a></li>
<li><a class="" href="syllabus.html">Syllabus</a></li>
<li class="book-part">Descriptive Statistics</li>
<li><a class="" href="introdata.html"><span class="header-section-number">1</span> Introduction to Data</a></li>
<li><a class="" href="aesthetic-mapping.html"><span class="header-section-number">2</span> Visualizing data: Mapping data onto aesthetics</a></li>
<li><a class="" href="coordinate-systems-axes.html"><span class="header-section-number">3</span> Coordinate systems and axes</a></li>
<li><a class="" href="ggplot2-package-for-plots.html"><span class="header-section-number">4</span> ggplot2 package for plots</a></li>
<li><a class="" href="color-basics.html"><span class="header-section-number">5</span> Color scales</a></li>
<li class="book-part">Sampling Distributions</li>
<li><a class="" href="paras.html"><span class="header-section-number">6</span> Statistical Parameters</a></li>
<li><a class="" href="ChapProbability.html"><span class="header-section-number">7</span> Probability</a></li>
<li><a class="active" href="randomVariables.html"><span class="header-section-number">8</span> Random Variables</a></li>
<li><a class="" href="ChapNormal.html"><span class="header-section-number">9</span> The Normal Random Variable</a></li>
<li><a class="" href="ChapSampDist.html"><span class="header-section-number">10</span> The Sampling Distribution</a></li>
<li><a class="" href="CI.html"><span class="header-section-number">11</span> Parameter Intervals</a></li>
<li><a class="" href="foundations-bootstrapping.html"><span class="header-section-number">12</span> Confidence intervals with bootstrapping</a></li>
<li class="book-part">One Sample Inference</li>
<li><a class="" href="inference-one-mean.html"><span class="header-section-number">13</span> Inference for a single mean</a></li>
<li><a class="" href="ChapBinom.html"><span class="header-section-number">14</span> Binomial Random Variable</a></li>
<li><a class="" href="inference-one-prop.html"><span class="header-section-number">15</span> Inference for a single proportion</a></li>
<li class="book-part">Computing</li>
<li><a class="" href="install.html"><span class="header-section-number">16</span> Installing R and RStudio</a></li>
<li><a class="" href="basics.html"><span class="header-section-number">17</span> Basics of R and Rstudio</a></li>
<li><a class="" href="projects.html"><span class="header-section-number">18</span> RStudio Projects</a></li>
<li><a class="" href="functionsHOP.html"><span class="header-section-number">19</span> Functions</a></li>
<li><a class="" href="packages.html"><span class="header-section-number">20</span> Packages</a></li>
<li><a class="" href="import.html"><span class="header-section-number">21</span> Data Import and Export</a></li>
<li><a class="" href="transition-to-r-from-excel-stata-sas.html"><span class="header-section-number">22</span> Transition to R from Excel, Stata, SAS</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="randomVariables" class="section level1">
<h1>
<span class="header-section-number">8</span> Random Variables<a class="anchor" aria-label="anchor" href="#randomVariables"><i class="fas fa-link"></i></a>
</h1>

<div id="objectives-1" class="section level2">
<h2>
<span class="header-section-number">8.1</span> Objectives<a class="anchor" aria-label="anchor" href="#objectives-1"><i class="fas fa-link"></i></a>
</h2>
<div class="rmdnote">
<p>This <strong>central</strong> chapter addresses a <strong>fundamental concept</strong>, namely the <strong>variance of a random variable</strong>. It gives the laws governing the variance of a sum of 2, or (especially) <span class="math inline">\(n\)</span> random variables – and even more importantly – the laws governing <strong>the variance of a difference of two random variables.</strong> The latter is central, not just to simple contrasts involving 2 sample means or proportions, but also in the much wider world of regression, since the variance (sampling variability) of any regression slope can be viewed as the <strong>variance of a linear combination of</strong> random errors, or random deviations, or <strong>random variables</strong>. So, if there is one master formula to pay attention to and to own, it is the one for the variance of a linear combination of random variables. All others are special cases of this.</p>
</div>
<p>So, the <strong>specific objectives</strong> are to truly understand</p>
<ul>
<li><p>the concept of a random variable.</p></li>
<li><p>the concept of the expectation and variance of a random variable.</p></li>
<li><p>why it is that, when dealing with the sum of two or more independent random variables, it is not their standard deviations that sum (add), but rather their variances.</p></li>
<li><p>likewise, when dealing with the <strong>difference</strong> of two independent random variables, or some <strong>linear combination</strong> of <span class="math inline">\(n\)</span> independent random variables involving positive and negative weights, why it is that <strong>the component variances add</strong>, and <strong>with what weights</strong>.</p></li>
</ul>
</div>
<div id="random-variables" class="section level2">
<h2>
<span class="header-section-number">8.2</span> Random Variables<a class="anchor" aria-label="anchor" href="#random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>Recall the definition of a random variable:</p>
<div class="definition">
<p><span id="def:unlabeled-div-18" class="definition"><strong>Definition 8.1  (Random variable) </strong></span>A random variable refer to numerical values, typically the outcome of an observation, a measurement, or a function thereof.</p>
</div>
<p><br></p>
<p>Let’s review the two types of random variables that we will focus on in this course.</p>
<div class="definition">
<p><span id="def:unlabeled-div-19" class="definition"><strong>Definition 8.2  (Discrete Random Variable) </strong></span>A random variable that assumes only a finite (or countably infinite) number of distinct values. Discrete random variables have a finite or countably infinite number of possible values, each with positive or zero probability.</p>
</div>
<p><br></p>
<div class="definition">
<p><span id="def:unlabeled-div-20" class="definition"><strong>Definition 8.3  (Probability mass function) </strong></span>The probability mass function (PMF) of a discrete random variable <span class="math inline">\(Y\)</span> provides the possible values <span class="math inline">\(y\)</span> and their associated probabilities by <span class="math inline">\(\operatorname{P}(Y=y)\)</span>. The sum of all probabilities must sum to 1, i.e. <span class="math inline">\(\sum_{y} \operatorname{P}(Y=y) = 1\)</span>.</p>
</div>
<p><br></p>
<div class="definition">
<p><span id="def:unlabeled-div-21" class="definition"><strong>Definition 8.4  (Continuous Random Variable) </strong></span>A random variable is continuous if both of the following apply:<br>
1. Its set of possible values consists either of all numbers in a single interval on the number line (possibly infinite in extent, e.g., from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span>) or all numbers in a disjoint union of such intervals (e.g., [0,10] <span class="math inline">\({\displaystyle \cup }\)</span> [20, 30]).<br>
2. No possible value of the variable has positive probability, that is, <span class="math inline">\(\operatorname{P}(X = c)\)</span> = 0 for any possible value <span class="math inline">\(c\)</span>.</p>
</div>
<p><br></p>
<div class="definition">
<p><span id="def:unlabeled-div-22" class="definition"><strong>Definition 8.5  (Probability density function) </strong></span>Let <span class="math inline">\(Y\)</span> be a continuous random variable. Then a probability distribution or probability density function (pdf) of <span class="math inline">\(Y\)</span> is a function <span class="math inline">\(f(y)\)</span> such that for any two numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> with <span class="math inline">\(a \leq b\)</span>,</p>
<p><span class="math display">\[ \operatorname{P}(a \leq Y \leq b) = \int_a^b f(y) \partial y\;.\]</span>
That is, the probability that <span class="math inline">\(Y\)</span> takes on a value in the interval <span class="math inline">\([a, b]\)</span> is the area
above this interval and under the graph of the density function, as illustrated
in Figure <a href="randomVariables.html#fig:dens">8.1</a>. The graph of <span class="math inline">\(f(y)\)</span> is often referred to as the density curve.</p>
<div class="figure">
<span style="display:block;" id="fig:dens"></span>
<img src="inst/figures/density.png" alt="$\Prob(a \leq Y \leq b)$ = the area under the density curve between $a$ and $b$." width="348"><p class="caption">
Figure 8.1: <span class="math inline">\(\operatorname{P}(a \leq Y \leq b)\)</span> = the area under the density curve between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.
</p>
</div>
</div>
<p><br></p>
<p>Although any interval on the number line contains an infinite number of numbers, it can be shown that there is no way to create an infinite listing of all these values - there are just too many of them. The second condition describing a continuous random variable is perhaps counterintuitive, since it would seem to imply a total probability of zero for all possible values. As we will see in the Chapter on specific types of continous random variables, intervals of values have positive probability; the probability of an interval will decrease to zero as the width of the interval shrinks to zero.</p>
<p>One might argue that although in principle variables such as height, weight,
and temperature are continuous, in practice the limitations of our measuring
instruments restrict us to a discrete (though sometimes very finely subdivided)
world. However, continuous models often approximate real-world situations very
well, and continuous mathematics (the calculus) is frequently easier to work with
than the mathematics of discrete variables and distributions.</p>
</div>
<div id="expectation-of-a-random-variable" class="section level2">
<h2>
<span class="header-section-number">8.3</span> Expectation of a Random Variable<a class="anchor" aria-label="anchor" href="#expectation-of-a-random-variable"><i class="fas fa-link"></i></a>
</h2>
<div id="discrete-random-variable" class="section level3">
<h3>
<span class="header-section-number">8.3.1</span> Discrete Random Variable<a class="anchor" aria-label="anchor" href="#discrete-random-variable"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:unlabeled-div-23" class="definition"><strong>Definition 8.6  (Expected value of a discrete random variable) </strong></span>Let <span class="math inline">\(Y\)</span> be a discrete random variable with set of possible values <span class="math inline">\(D=\left\lbrace y_1, y_2, \ldots,y_k \right\rbrace\)</span> and corresponding probabilities for each value, e.g., <span class="math inline">\(y_1\)</span> with probability <span class="math inline">\(\operatorname{P}(y_1)\)</span>, <span class="math inline">\(y_2\)</span> with probability <span class="math inline">\(\operatorname{P}(y_2)\)</span>, <span class="math inline">\(y_3\)</span> with probability <span class="math inline">\(\operatorname{P}(y_3)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(y_k\)</span> with probability <span class="math inline">\(\operatorname{P}(y_k)\)</span>, then the expected value of the random variable <span class="math inline">\(Y\)</span> is:</p>
<p><span class="math display">\[\operatorname{E}(Y) =  \sum_{y \in D} y \times \operatorname{P}(y)\;.\]</span>
This expected value will exist provided that <span class="math inline">\(\sum_{y \in D} |y| \cdot \operatorname{P}(y) &lt; \infty\)</span>.</p>
</div>
<p><span class="math inline">\(\operatorname{E}(Y)\)</span> is a mean that uses expected (i.e. unobservable or theoretical or long run) relative frequencies <span class="math inline">\(\operatorname{P}(\cdot)\)</span>. Whereas <span class="math inline">\(\bar{y}\)</span> uses observed relative frequencies. You can think of <span class="math inline">\(\operatorname{E}(Y)\)</span> as center of mass of <span class="math inline">\(\operatorname{P}(\cdot)\)</span>.</p>
</div>
<div id="continuous-random-variable" class="section level3">
<h3>
<span class="header-section-number">8.3.2</span> Continuous Random Variable<a class="anchor" aria-label="anchor" href="#continuous-random-variable"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:unlabeled-div-24" class="definition"><strong>Definition 8.7  (Expected value of a continuous random variable) </strong></span>The expected value of a continuous random variable <span class="math inline">\(Y\)</span> with probability density function <span class="math inline">\(f(y)\)</span> is
<span class="math display">\[ \operatorname{E}(Y) = \int_{-\infty}^\infty y \cdot f(y) \partial y \;.\]</span>
This expected value will exist provided that <span class="math inline">\(\int_{-\infty}^\infty |y| \cdot f(y) \partial y &lt; \infty\)</span>.</p>
</div>
<p><br></p>
</div>
<div id="why-should-we-care-about-the-expectation" class="section level3">
<h3>
<span class="header-section-number">8.3.3</span> Why should we care about the expectation?<a class="anchor" aria-label="anchor" href="#why-should-we-care-about-the-expectation"><i class="fas fa-link"></i></a>
</h3>
<p>The expectation acts as a mean for a variable that has a conception repetition or an infinite sample size. The expected value of a random variable will usually be in terms of population parameters. We present some examples of where the expected value of a random variable is a quantity of interest.</p>
<div class="example">
<p><span id="exm:unlabeled-div-25" class="example"><strong>Example 8.1  </strong></span>Suppose, for example, that the death rate in any year is 1 out of every 1000 people,
and that another 2 out of 1000 suffer some kind of disability. Then we can display the
probability model for this insurance policy as shown in Table <a href="randomVariables.html#tab:probs-example">8.1</a></p>
<p>To see what the insurance company can expect, imagine that it insures exactly 1000
people. Further imagine that, in perfect accordance with the probabilities, 1 of the
policyholders dies, 2 are disabled, and the remaining 997 survive the year unscathed.
The company would pay $10,000 to one client and $5000 to each of 2 clients. That’s
a total of $20,000, or an average of 20000/1000 = $20 per policy. Since it is charging people $50 for the policy, the company expects to make a profit of $30 per customer. Not bad!</p>
<p>We can’t predict what will happen during any given year, but we can say what we expect to happen. To do this, we (or, rather, the insurance company) need the probability model (PMF). The expected value of a policy is a parameter of this model. In fact, it’s the mean. This isn’t an average of some data values, so we won’t estimate it. Instead, we assume that the probabilities are known and simply calculate the expected value from them. How did we come up with $20 as the expected value of a policy payout?</p>
</div>
<div class="inline-table"><table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:probs-example">Table 8.1: </span>Probability mass function for insurance policy
</caption>
<thead><tr>
<th style="text-align:left;">
Policyholder outcome
</th>
<th style="text-align:right;">
Payout y
</th>
<th style="text-align:left;">
Probability P(Y=y)
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Death
</td>
<td style="text-align:right;">
10000
</td>
<td style="text-align:left;">
1/1000
</td>
</tr>
<tr>
<td style="text-align:left;">
Disability
</td>
<td style="text-align:right;">
5000
</td>
<td style="text-align:left;">
2/1000
</td>
</tr>
<tr>
<td style="text-align:left;">
Neither
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
997/1000
</td>
</tr>
</tbody>
</table></div>
<p><br></p>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 8.2  (Expected number of children in a household) </strong></span>For the table shown below, the expected value is 2.49 children per household</p>
</div>
<div class="inline-table"><table class="table" style="margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:right;">
Number of children
</th>
<th style="text-align:right;">
Proportion of households
</th>
<th style="text-align:right;">
Product
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.335
</td>
<td style="text-align:right;">
0.335
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.245
</td>
<td style="text-align:right;">
0.490
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.180
</td>
<td style="text-align:right;">
0.540
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.126
</td>
<td style="text-align:right;">
0.504
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.072
</td>
<td style="text-align:right;">
0.360
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.031
</td>
<td style="text-align:right;">
0.186
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.009
</td>
<td style="text-align:right;">
0.063
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
0.002
</td>
<td style="text-align:right;">
0.016
</td>
</tr>
</tbody>
</table></div>
<p><br></p>
<div class="example">
<p><span id="exm:unlabeled-div-27" class="example"><strong>Example 8.3  (Unbiased Estimator) </strong></span>A point estimator <span class="math inline">\(\widehat{\theta}\)</span> is said to be an unbiased estimator of <span class="math inline">\(\theta\)</span> if
<span class="math display">\[\operatorname{E}(\widehat{\theta}) = \theta\;,\]</span> for every possible value of <span class="math inline">\(\theta\)</span>.</p>
<p>For example, let <span class="math inline">\(\hat{p}\)</span> be the proportion of successes in a sample of <span class="math inline">\(n\)</span> individuals. Then <span class="math inline">\(\operatorname{E}(\hat{p}) = \pi\)</span> where <span class="math inline">\(\pi\)</span> is the population proportion of successes. Therefore, <span class="math inline">\(\hat{p}\)</span> is an unbiased estimator of <span class="math inline">\(\pi\)</span>.</p>
</div>
</div>
</div>
<div id="expected-value-of-a-function-of-a-random-variable" class="section level2">
<h2>
<span class="header-section-number">8.4</span> Expected value of a function of a random variable<a class="anchor" aria-label="anchor" href="#expected-value-of-a-function-of-a-random-variable"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:unlabeled-div-28" class="definition"><strong>Definition 8.8  (Expected value of a function of a discrete random variable) </strong></span>Let <span class="math inline">\(Y\)</span> be a discrete random variable with set of possible values <span class="math inline">\(D=\left\lbrace y_1, y_2, \ldots,y_k \right\rbrace\)</span> and corresponding probabilities for each value, e.g., <span class="math inline">\(y_1\)</span> with probability <span class="math inline">\(\operatorname{P}(y_1)\)</span>, <span class="math inline">\(y_2\)</span> with probability <span class="math inline">\(\operatorname{P}(y_2)\)</span>, <span class="math inline">\(y_3\)</span> with probability <span class="math inline">\(\operatorname{P}(y_3)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(y_k\)</span> with probability <span class="math inline">\(\operatorname{P}(y_k)\)</span>. Furthermore, let <span class="math inline">\(g(Y)\)</span> be some real-valued function of <span class="math inline">\(Y\)</span>. Then the expected value of <span class="math inline">\(g(Y)\)</span> is:</p>
<p><span class="math display">\[\operatorname{E}(g(Y)) =  \sum_{y \in D} g(y) \times \operatorname{P}(y)\;.\]</span>
i.e. it is a weighted mean of the <span class="math inline">\(g(y)\)</span>’s, with <span class="math inline">\(\operatorname{P}(y)\)</span>’s as weights.</p>
</div>
<div class="rmdnote">
<p>In some instances, the expectation of <span class="math inline">\(g(Y)\)</span> is <span class="math inline">\(g(\operatorname{E}(Y))\)</span>, while in others it is more complex. Can you figure out when it is/is not in each of the following instances?</p>
</div>
<div id="examples-1" class="section level3">
<h3>
<span class="header-section-number">8.4.1</span> Examples<a class="anchor" aria-label="anchor" href="#examples-1"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<p><span class="math inline">\(Y\)</span> = Noon Temperature (C) in Montreal on a randomly selected day of the year;<br><span class="math inline">\(g(Y)\)</span> = Temperature (F) = 32 + (9/5) <span class="math inline">\(Y\)</span></p>
<ul>
<li><p><span class="math inline">\(Y\)</span> = Weight in Kg (or Height in cm) of a randomly selected person;<br>
g(Y) = Weight in Kg (or Height in inches)</p></li>
<li><p><span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are two random variables that might or might not be related;<br>
.<br>
if <span class="math inline">\(g(Y_1, Y_2) = Y_1 + Y_2,\)</span> then <span class="math inline">\(E[g(Y_1, Y_2)] = E[Y_1] + E[Y_2].\)</span><br>
.<br>
if <span class="math inline">\(g(Y_1, Y_2) = \frac{Y_1 + Y_2}{2},\)</span> then <span class="math inline">\(E[g(Y_1, Y_2)] = \frac{E[Y_1] + E[Y_2]}{2},\)</span><br>
.<br>
and, by analogy, for a sum or mean of <span class="math inline">\(n\)</span> related or unrelated random variables.</p></li>
<li><p><span class="math inline">\(Y\)</span> = diameter of a randomly chosen sphere;<br><span class="math inline">\(g(Y)\)</span> = Volume of sphere = <span class="math inline">\(\frac{\pi}{6} Y^3.\)</span></p></li>
<li><p><span class="math inline">\(Y\)</span> = fuel consumption, in liters/100km, of a randomly selected make of car;<br><span class="math inline">\(g(Y)\)</span> = miles per gallon or Km per liter (reciprocal)</p></li>
<li><p><span class="math inline">\(Y\)</span> = which of 3 <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/median-elevator.pdf"><strong>unequally spaced elevators</strong></a> shows up next.<br><span class="math inline">\(\operatorname{P}\)</span>(it is #1) = <span class="math inline">\(\operatorname{P}\)</span>(it is #2) = <span class="math inline">\(\operatorname{P}\)</span>(it’s #3) = 1/3.<br>
.<br><span class="math inline">\(g(Y)\)</span> = Distance to elevator. How to mimimize E(distance)?<br>
.<br><span class="math inline">\(g(Y)\)</span> = <strong>Squared Distance</strong> to elevator. How to mimimize E(<span class="math inline">\(g(Y)\)</span>)?</p></li>
<li><p>Random Variable Y with Expectation or Mean <span class="math inline">\(\mu\)</span><br><span class="math inline">\(g(Y) = (Y - \mu)^2\)</span>, the <strong>squared deviation from the mean</strong></p></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="variance-and-thus-sd-of-a-random-variable" class="section level2">
<h2>
<span class="header-section-number">8.5</span> Variance (and thus, SD) of a random variable<a class="anchor" aria-label="anchor" href="#variance-and-thus-sd-of-a-random-variable"><i class="fas fa-link"></i></a>
</h2>
<div id="definitions" class="section level3">
<h3>
<span class="header-section-number">8.5.1</span> Definitions<a class="anchor" aria-label="anchor" href="#definitions"><i class="fas fa-link"></i></a>
</h3>
<div class="definition">
<p><span id="def:unlabeled-div-29" class="definition"><strong>Definition 8.9  (Variance) </strong></span>The variance of the random variable <span class="math inline">\(Y\)</span> is given by <span class="math display">\[\operatorname{E}[(Y - \mu)^2].\]</span>
It is usually shortened to <span class="math inline">\(\operatorname{Var}(Y)\)</span>.</p>
<p>It, and its positive square root, called the <em>standard deviation</em> of <span class="math inline">\(Y\)</span>, or SD(<span class="math inline">\(Y\)</span>), are two of the most commonly used measures of variability or spread or uncertainty.</p>
</div>
<ul>
<li><p>Computationally, <span class="math inline">\(\operatorname{Var}(Y) = \operatorname{E}[(Y - \mu)^2]\)</span> = <span class="math inline">\(\sum(y - \mu)^2 \times f(y),\)</span> or <strong>Mean Squared Deviation</strong>, and</p></li>
<li><p>Standard Deviation, SD(<span class="math inline">\(Y) = \sqrt{(\operatorname{Var}(Y)}= \sqrt{\operatorname{E}[ (Y — \mu)^2]},\)</span> or <strong>Root Mean Squared Deviation</strong></p></li>
<li><p>In <strong>French</strong>, the Standard Deviation is called <a href="https://fr.wikipedia.org/wiki/%C3%89cart_type"><strong>écart type</strong></a>.<br>
This <a href="http://www.french-linguistics.co.uk/dictionary/type.html">French-&gt;English dictionary</a> translates (the noun) <code>écart</code> as space, gap, distance between objects, interval, gap between dates, difference between numbers. The adjective <code>type</code> translates as typical, standard. This adjective better describes the meaning than ‘standard’ does. See here for the <strong>history of the term</strong> <a href="https://en.wikipedia.org/wiki/Standard_deviation#History"><code>standard</code> deviation</a>.</p></li>
</ul>
<p>In Figure <a href="randomVariables.html#fig:ecart">8.2</a>, graphically and numerically illustrated, are three (of the many) ways to measure the variability of a random variable.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ecart"></span>
<img src="09-random-variables_files/figure-html/ecart-1.png" alt="6 symmetrically distributed random variables, and 3  ways of measuring their spreads about a common mean. [After Mosteller, Rourke and Thomas. Probability with statistical applications 2nd Ed, p205] " width="672"><p class="caption">
Figure 8.2: 6 symmetrically distributed random variables, and 3 ways of measuring their spreads about a common mean. [After Mosteller, Rourke and Thomas. Probability with statistical applications 2nd Ed, p205]
</p>
</div>
<p>In practice, the mean absolute deviation is often quite close to the SD, and certainly easier to explain to explain to non-statisticians. However, when computing was by hand and laborious, it took two passes through the data to compute it, whereas, the SD could be computed in one.</p>
<div class="example">
<p><span id="exm:unlabeled-div-30" class="example"><strong>Example 8.4  (Variance and SD of number of children &lt;= 12 years in households with at least 1 such child) </strong></span>The calculation are shown in the Table below</p>
</div>
<div class="inline-table"><table class="table" style="margin-left: auto; margin-right: auto;">
<thead><tr>
<th style="text-align:right;">
# children
</th>
<th style="text-align:right;">
deviation
</th>
<th style="text-align:right;">
deviation^2
</th>
<th style="text-align:right;">
P(y)
</th>
<th style="text-align:right;">
product
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
-1.494
</td>
<td style="text-align:right;">
2.232036
</td>
<td style="text-align:right;">
0.335
</td>
<td style="text-align:right;">
0.7477321
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
-0.494
</td>
<td style="text-align:right;">
0.244036
</td>
<td style="text-align:right;">
0.245
</td>
<td style="text-align:right;">
0.0597888
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.506
</td>
<td style="text-align:right;">
0.256036
</td>
<td style="text-align:right;">
0.180
</td>
<td style="text-align:right;">
0.0460865
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
1.506
</td>
<td style="text-align:right;">
2.268036
</td>
<td style="text-align:right;">
0.126
</td>
<td style="text-align:right;">
0.2857725
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
2.506
</td>
<td style="text-align:right;">
6.280036
</td>
<td style="text-align:right;">
0.072
</td>
<td style="text-align:right;">
0.4521626
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
3.506
</td>
<td style="text-align:right;">
12.292036
</td>
<td style="text-align:right;">
0.031
</td>
<td style="text-align:right;">
0.3810531
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
4.506
</td>
<td style="text-align:right;">
20.304036
</td>
<td style="text-align:right;">
0.009
</td>
<td style="text-align:right;">
0.1827363
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
5.506
</td>
<td style="text-align:right;">
30.316036
</td>
<td style="text-align:right;">
0.002
</td>
<td style="text-align:right;">
0.0606321
</td>
</tr>
</tbody>
</table></div>
<p><strong>Which is primary, Standard Deviation or Variance?</strong></p>
<ul>
<li>Although we first define variance and then take the square root to reach the SD, we should think of the SD as primary, at least for descriptive purposes. For example, the Total Fertility Rate which measures the average number of children per woman is just below 2.5 children per woman. Suppose the variation from country to country had a standard deviation of 1.2 and a variance of 1.44. It would be awkward to say the variance was 1.44 square children per square woman.</li>
</ul>
<p>However, there are <em>good mathematical reasons</em> to work with variance.</p>
</div>
<div id="why-use-the-variance" class="section level3">
<h3>
<span class="header-section-number">8.5.2</span> Why use the variance?<a class="anchor" aria-label="anchor" href="#why-use-the-variance"><i class="fas fa-link"></i></a>
</h3>
<div id="additivity" class="section level4">
<h4>
<span class="header-section-number">8.5.2.1</span> Additivity<a class="anchor" aria-label="anchor" href="#additivity"><i class="fas fa-link"></i></a>
</h4>
<p>The variance of the sum of two independent random variables is the sum of their variances, and even when the two variables are dependent the variability of their sum has a simple formula. SD’s dont add; their squares do. Or to quote the physicists, errors ‘<strong>Errors add in quadrature</strong>, like the lengths of the sides of Pythagoras’ triangle. It took mathematicians a long time to discover, this, and some of the blunders along the way are told in a fascinating chapter in this very readable book <a href="https://www-degruyter-com.proxy3.library.mcgill.ca/view/title/521193">The Seven Pillars of
Statistical Wisdom</a>.</p>
<div class="inline-figure"><img src="inst/figures/pyth.png" width="321"></div>
</div>
<div id="the-central-limit-theorem" class="section level4">
<h4>
<span class="header-section-number">8.5.2.2</span> The Central Limit Theorem<a class="anchor" aria-label="anchor" href="#the-central-limit-theorem"><i class="fas fa-link"></i></a>
</h4>
<p>The limiting behavior of a random variable that is the sum of a large number of independent random variables depends on the variances of these random variables.</p>
</div>
</div>
</div>
<div id="variance-and-sd-of-a-function-of-a-random-variable" class="section level2">
<h2>
<span class="header-section-number">8.6</span> Variance and SD of a function of a random variable<a class="anchor" aria-label="anchor" href="#variance-and-sd-of-a-function-of-a-random-variable"><i class="fas fa-link"></i></a>
</h2>
<p>If we go back to some of the examples listed above we can reason out what the law must be:</p>
<ul>
<li>
<span class="math inline">\(Y\)</span> = Noon Temperature (C) in Mtl on a randomly selected day of the year; <span class="math inline">\(g(Y)\)</span> = Temperature (F) = 32 + (9/5) <span class="math inline">\(Y\)</span><br>
If the SD was say 10 C, then surely the SD is 18 F. After all, Temperature is Temperature, so you are not changing the fundamental variable, but rather changing the scale of the temperature variable. So, the SD scales up by 9/5, and so, being an average square, the variance scales up by <span class="math inline">\((9/5)^2\)</span>. If you are going the other way, from the larger F scale to the smaller C scale, the scalings are (5/9) for the SD, and <span class="math inline">\((5/9)^2\)</span> for the variance. More generally,<br><span class="math display">\[SD[constant  \times RV] = constant  \times SD[RV] \]</span><br><span class="math display">\[Var[constant  \times RV] = constant^2  \times Var[RV]\]</span>
</li>
</ul>
<p>This example also shows another law related to spread: shifting left or by a constant amount (eg. suppose the conversion was <span class="math inline">\(F = 10 + (9/5)C\)</span> instead of was <span class="math inline">\(F = 32 + (9/5)C,\)</span> it would not alter the spread. Thus,</p>
<p><span class="math display">\[SD[ RV + constant ] = SD[RV]\]</span><br><span class="math display">\[Var[ RV + constant ] = Var[RV]\]</span></p>
<ul>
<li>
<p><span class="math inline">\(Y\)</span> = Weight in Kg (or Height in cm) of a randomly selected person; g(Y) = Weight in Kg (or Height in inches). This involves just a scaling, with no shift. So if the SD were 10 Kg, it would be 22 lbs, and the variances in the 2 scales would be <span class="math inline">\(100 \ Kg^2\)</span> and <span class="math inline">\(484 \ lb^2\)</span>.</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> = Years of publication of all the books in the McGill Library, with Years measured from AD (Anno Dominini, ‘in the year of the Lord’). The SD would be the same if we measured the Year from 1439 AD (<span class="math inline">\(W = Y\)</span> - 1439) when Gutenberg was the first European to use movable type, or from 1492 AD when Christopher Columbus reached North America. What if, instead, we calculated the age of each book in the year 2020, i.e. as <span class="math inline">\(W = 2020 - Y\)</span>? The scale would now be reversed. Instead of being at the extreme left, the older books would not be at the right hand of the scale, and vice versa. But the spread would still be the same, even though the shape of the new distribution would be the mirror image of the old one. What if we measured age in decades, i.e., <span class="math inline">\(W = \frac{2020 - Y}{10}\)</span> or centuries i.e., <span class="math inline">\(W = \frac{2020 - Y}{100}\)</span>? The SDs would be scaled down by 10 and by 100, and the variances by <span class="math inline">\(10^2\)</span> and <span class="math inline">\(100^2.\)</span></p></li>
<li><p><span class="math inline">\(Y\)</span> = Ocean Depth at a randomly chosen location, measured in metres. If the origin is the ocean floor, all depths will be positive; if it is the surface of the ocean, they will all be negative. The spread, the SD and the Variance stay the same, but the shapes of the distributions are mirror images of each other.</p></li>
<li><p><span class="math inline">\(Y\)</span> = diameter of a randomly chosen sphere; <span class="math inline">\(g(Y)\)</span> = Volume of sphere = <span class="math inline">\(\frac{\pi}{6} Y^3.\)</span> This one is more complicated – as you might have guessed from just trying to compute the expectation. The fact that the scaling is different at different values of <span class="math inline">\(Y\)</span> complicates matters. There is an approximate formula, that depends on the scaling at some ‘representative’ value of <span class="math inline">\(Y\)</span>, typically the mean or mode. For more on this, see the examples in this <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/jh_dt_tas_2006.pdf">expository piece</a>.</p></li>
<li><p><span class="math inline">\(Y\)</span> = fuel consumption, in liters/100km, of a randomly selected make of car; <span class="math inline">\(g(Y)\)</span> = Km per liter (reciprocal). There is no exact closed form, but the approximation works well because the values are well away from zero, and not too spread out.</p></li>
</ul>
</li>
</ul>
</div>
<div id="sums-means-differences-of-random-variables" class="section level2">
<h2>
<span class="header-section-number">8.7</span> Sums, means, differences of random variables<a class="anchor" aria-label="anchor" href="#sums-means-differences-of-random-variables"><i class="fas fa-link"></i></a>
</h2>
<div id="a-sum-of-2-or-n-random-variables" class="section level3">
<h3>
<span class="header-section-number">8.7.1</span> A sum of 2 or <span class="math inline">\(n\)</span> random variables<a class="anchor" aria-label="anchor" href="#a-sum-of-2-or-n-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>To keep it simple, and to allow us to see what is going on, let’s consider two very simple random variables (RV’s), each taking just 2 values, and with equal probabilities. You can check for yourself later that the same law applies to random variables that take on more than 2 values, and with uneven probability distributions. The key is that the 2 variables are <em>independent</em> of each other.</p>
<p>Figures <a href="randomVariables.html#fig:var1">8.3</a> and <a href="randomVariables.html#fig:var2">8.4</a> show the two RV’s. <span class="math inline">\(RV_1\)</span>  (6  or  12)  and  <span class="math inline">\(RV_2\)</span>  (8  or  16)</p>
<p><span class="math display">\[Sum = RV_1 \ (6 \ or \ 12) \ + \ RV_2 \ (8 \ or \ 16) \ = \ 14 \ or \ 20 \ or \ 22 \ or \ 28.\]</span><br>
We will come back much later to the choices of the specific values for each of the random variables; for our purposes here, the main point is that both equally-likely values are an even number apart, so the SDs are integers, and all calculations involve integers. Note that an RV with equally likely values 1 and 7 (or 3 and 9, or -1 and + 5) has the same SD as the RV with equally likely values 6 and 12: it is the <em>distance between them</em> that determines the SD. Note also, that with half of the values at one extreme and half at the other, all values are exactly one SD from the mean.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:var1"></span>
<img src="09-random-variables_files/figure-html/var1-1.png" alt="Variance of RV1" width="672"><p class="caption">
Figure 8.3: Variance of RV1
</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:var2"></span>
<img src="09-random-variables_files/figure-html/var2-1.png" alt="Variance of RV2" width="672"><p class="caption">
Figure 8.4: Variance of RV2
</p>
</div>
<p>We now imagine taking a random value of <span class="math inline">\(RV_1\)</span> and a random value of <span class="math inline">\(RV_2\)</span> and summing them. There are 4 possible sums, and in this case they are all distinct (this isn’t always be the case, but the values of the two RV’s in <em>this example</em> were deliberately chosen to make them all distinct, and to avoid grouping RV combinations with the same sum.) A probability tree (next panel) helps to see the 4 possibilities, but here we add an extra feature: we let the lengths of the branches denote the values of the RVs.</p>
<p>In our example, the 4 equally likely sums are 14, 20, 22 and 28, and their mean is <span class="math inline">\(\frac{14+20+22+28}{4} = 21.\)</span> That the mean (expected value) of the sum equals the sum of the 2 individual means or expected values, is hardly surprising (it is even true if the 2 RV’s were not independent). The 4 equally likely deviations from this 21 are -7, -1, +1, and +7, and so, from 1st principles, the Variance of the sum of the 2 RVs is
<span class="math display">\[\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \frac{100}{4} = 25.\]</span><br>
Thus, the SD of the sum of the 2 RVs is <span class="math inline">\(\sqrt{25}\)</span> = 5.</p>
<p>Fortunately, we don’t have to go back to 1st principles to calculate the SD of the sum of 2 RVs – but we do NOT do so by adding the 2 SDs. It is the <strong>variances</strong> that add.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="09-random-variables_files/figure-html/unnamed-chunk-6-1.png" alt="Variance (and thus SD) of the SUM of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the sum from its expectation is decomposed into its 2 components. This, (by the expansion rule for (a+b) squared that you learned in high school), each squared deviation becomes a sum of 2 squares, and a 'cross-product'. But the 4 cross-products cancel each other, and you are left with the sums of two squares, the original 3-squared and the  original 4-squared, i.e. the variances of RV1 and RV2." width="672"><p class="caption">
Figure 8.5: Variance (and thus SD) of the SUM of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the sum from its expectation is decomposed into its 2 components. This, (by the expansion rule for (a+b) squared that you learned in high school), each squared deviation becomes a sum of 2 squares, and a ‘cross-product’. But the 4 cross-products cancel each other, and you are left with the sums of two squares, the original 3-squared and the original 4-squared, i.e. the variances of RV1 and RV2.
</p>
</div>
<p>Whereas one numerical example doesn’t prove the the variances of a sum of independent RVs is the sum of their variances rule, you can check out other more complicated examples with more values, and uneven distributions – or use simulation – and satisfy yourself that it is a general rule. Indeed, the formal mathematical statistics proof uses the exact same method as that used in the panel, except that it replaces each number by a symbol. So we can sum up the forgoing numerical example by saying:</p>
<p><span class="math display">\[ SD_1 = 3; SD_2 =4;\  but \ SD[Sum] \ \ne \ 3 + 4. \ \ \  SD[Sum] \ = \ \sqrt{3^2 + 4^2} = 5.\]</span></p>
<p>Now you can see why mathematical statisticians like to work with squared SD’s. And you can see why we chose the nice variance values 3 and 4. Just like for the right-angled triangles in Pythagoras’ theorem, where the length of the hypotenuse is the square root of the squares of the lengths of the sides, so it is also with orthogonal or independent random variables: the SD of their sum is the square root of the sum of the squared SD’s of the individual RVs.</p>
<p>Since the theorem works for the sum of 2 independent RV’s, it also works for the sum of 3, and for the sum of <span class="math inline">\(n\)</span> such RVs.</p>
</div>
<div id="measurement-errors" class="section level3">
<h3>
<span class="header-section-number">8.7.2</span> Measurement Errors<a class="anchor" aria-label="anchor" href="#measurement-errors"><i class="fas fa-link"></i></a>
</h3>
<p>These are an important issue in the non-experimental – and even the experimental – sciences. The simplest case (called the <strong>‘classical’ error model</strong>) is where the errors are independent of the true value, so that the <strong>variance in the observed (error-containing) values is the sum of the variance of the ‘true’ (errorless) values, and the variance of the errors.</strong></p>
<p>Even though many people think that random measurement errors <strong>cancel out</strong>, especially in large datasets, <strong>they do not</strong>. Even when they affect the <span class="math inline">\(Y\)</span> on the left side side of a regression model, they add ‘noise’ to the slope estimates. But when they affect an <span class="math inline">\(X\)</span> on the right hand side of a regression model, or even a correlation, their effects are more insidious. See for example, pages 19-21 of <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/hw_measurement2019.pdf#page=19">these Notes</a> and exercises 6, 8, 9, 18 and 21 that follow.</p>
<p>The other measurement error model, called ‘Berkson’ error, described <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Surveys/EffectsXerrorsNotesFromALM.pdf">here:</a>, is less common, and has less nasty effects.</p>
<p>The following diagram shows the classical error model, and one of the important metrics to measure the extent of the error, namely the <strong>intra-class correlation coefficient</strong>. The ‘ICC’ is relevant no matter whether the variable is on the left or right had side of the regression model. Even though we named the random variable <span class="math inline">\(Y\)</span>, it does not mean that measurement issues apply only to <span class="math inline">\(Y\)</span> variables. In fact, errors in <span class="math inline">\(X\)</span> variables have nastier effects. We chose the name <span class="math inline">\(Y\)</span> because we don’t treat an errorless <span class="math inline">\(X\)</span> in a regression model as a random variable, and we are seldom interested in inferences regarding it!</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="09-random-variables_files/figure-html/unnamed-chunk-7-1.png" alt="Random Measurement Error (E) added to a Random Variable. On the left is the distribution of a Random Variable Y, with each stickfigure representing a very large number of individuals. On the right is the distribution of the Random Variable Y', where Y' = Y + E, and E is independent of Y, and has a 2-point distribution, namely -0.5 and +0.5, with equal probabilities. (Here, the provenance/origin of each Y' value is shown by its colour, but in practice we would not have that luxury of knowing what the 'true' [errorless] value was). This is the variation we get to observe/measure. Of the variance of 1.93, some  1.68 of it is 'real'/'genuine', and the remainder, 0.25 is measurement error. The genuine variance of interest, 1.68, expressed as a proportion of the observable variance 1.93, namely 1.68/(1.68 + 0.25) = 0.87 or 87%. The proportion that is real is called the INTRA-CLASS CORRELATION (ICC) or intra-class correlation coefficient, and is an important indicator of the quality of the measurement of Y." width="672"><p class="caption">
Figure 8.6: Random Measurement Error (E) added to a Random Variable. On the left is the distribution of a Random Variable Y, with each stickfigure representing a very large number of individuals. On the right is the distribution of the Random Variable Y’, where Y’ = Y + E, and E is independent of Y, and has a 2-point distribution, namely -0.5 and +0.5, with equal probabilities. (Here, the provenance/origin of each Y’ value is shown by its colour, but in practice we would not have that luxury of knowing what the ‘true’ [errorless] value was). This is the variation we get to observe/measure. Of the variance of 1.93, some 1.68 of it is ‘real’/‘genuine’, and the remainder, 0.25 is measurement error. The genuine variance of interest, 1.68, expressed as a proportion of the observable variance 1.93, namely 1.68/(1.68 + 0.25) = 0.87 or 87%. The proportion that is real is called the INTRA-CLASS CORRELATION (ICC) or intra-class correlation coefficient, and is an important indicator of the quality of the measurement of Y.
</p>
</div>
<p><strong>The concept of an ICC depends on the law that ‘variances add.’</strong></p>
</div>
<div id="mean-of-2-or-n-random-variables" class="section level3">
<h3>
<span class="header-section-number">8.7.3</span> Mean of 2 or <span class="math inline">\(n\)</span> random variables<a class="anchor" aria-label="anchor" href="#mean-of-2-or-n-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>AND, if we know how to compute the SD of a SUM of <span class="math inline">\(n\)</span> independent RV’s, we then automatically know how to compute the SD of a MEAN of <span class="math inline">\(n\)</span> independent RV’s. Recall from an earlier example, the SD of a set of temperatures measured in the larger degrees F scale: then the
SD of the same set of temperatures measured in the smaller degrees C scale is just 5/9ths of the one in the F scale.</p>
<p>Going from a <strong>sum</strong> of <span class="math inline">\(n\)</span> RVs to a <strong>mean</strong> of <span class="math inline">\(n\)</span> RVs involves going to a scale that is (1/n)-th the spread of the sum scale.</p>
<p>So, in the above example, with <span class="math inline">\(n\)</span> = 2, the SD of the mean of the 2 RVs is (1/2) the SD of the sum, i.e.,</p>
<p><span class="math display">\[ SD\bigg(\frac{RV_1 +RV_2}{2}\bigg) = \frac{SD(RV_1+RV_2)}{2} = \frac{5}{2}.\]</span></p>
<p><strong>SPECIAL CASE</strong> (quite common) where <strong>SDs are identical</strong>:</p>
<p>Up to now, to keep things general, we used <span class="math inline">\(n\)</span> non-identical – but independent – random variables. If we
consider the Variance and the sum of <span class="math inline">\(n\)</span> IDENTICAL – and independent – random variables, so the <span class="math inline">\(n\)</span> Variances (each abbreviated to Var) are all equal, the laws simplify</p>
<p>First, since the variances add, we have that</p>
<p><span class="math display">\[ \operatorname{Var}(RV_1 + RV_2 + \dots + RV_n) = \operatorname{Var}_1 + \operatorname{Var}_2 + \dots + \operatorname{Var}_n = n \times \ each \ \operatorname{Var}.\]</span></p>
<p>and so, taking square roots,</p>
<p><span class="math display">\[ SD( \ RV_1 + RV_2 + \dots + RV_n \ ) = \sqrt{ \ n \times \ each \ \operatorname{Var}} = \sqrt{n} \ \times \ each \ SD\]</span></p>
<p>When we go from <strong>sum</strong> of <span class="math inline">\(n\)</span> IDENTICAL independent RVs to a <strong>mean</strong> of <span class="math inline">\(n\)</span> IDENTICAL RVs, we go to a scale that is (1/n)-th the spread of the sum scale. So, again, just as when we went from the larger degrees F scale to the smaller degrees C scale, we have</p>
<p><span class="math display">\[ SD\bigg(\frac{RV_1 + RV_2 + \dots + RV_n}{n}\bigg) = \frac{\sqrt{n} \ \times \ each \ SD}{n} = \frac{common \ SD}{\sqrt{n}} .\]</span>
Sometimes, we will need to work with variances. When we do, we use this law:</p>
<p><span class="math display">\[ \operatorname{Var}\bigg(\frac{RV_1 + RV_2 + \dots + RV_n}{n}\bigg) = \frac{common \ \operatorname{Var}}{n} .\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-31" class="example"><strong>Example 8.5  (Lengths of words in a book) </strong></span>The number of dashes in of each row in the first panel of Figure <a href="randomVariables.html#fig:bookA">8.7</a> is the number of letters in a randomly selected word from the book: dashes are for better visibility. The words (rows) are sorted by length, form shortest to longest. One cannot judge the full distribution just from this limited set, but (even though shape doesn’t matter much in the big scheme of things) you get a sense of its shape. In the entire book, the mean word length is about 4.5 letters, and the SD is
2.4 letters. (The fact that the distance between the minimum word length (1 letter) and the mean word length is less than 2 SDs hints that the full distribution has a long right tail.)</p>
<p>In each row in the colored panels, some 4 (or 9) randomly sampled words are (like Galton’s peas) pushed right up against each other, without spaces, and shown in a mono-spaced font, so that where the ‘line’ ends indicates the total number of letters in the 4 (9) words. Since this small number of rows (possibilities) is too small to give a good sense of the sampling distribution, the smooth purple histograms were calculated exactly.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bookA"></span>
<img src="09-random-variables_files/figure-html/bookA-1.png" alt="Illustrations of SD's of Sums and Means of n = 1, 4 and 9 independent and identically distributed random variables. Each RV is the length of a randomly selected word from a certain book. [Below, we will compare the mean word length in this book with the mean in a book by a competitor]. The distributions in purple were computed theoretically, using convolutions. Each row shows 1 'realization' of each of the  n random variables, with each word in a different color. The rows are sorted according to the values of the total [or mean] numbers of letters (chars) in the sample of n words. In the panels where n is 4 or 9, the leftmost n-1 characters of the n concatenated words are cropped, but the total/mean length  is correct. The top panel lists the 'per word' variation of all of the words in the book, and its SD, sometimes called the 'unit' variability. You can also think of the length of each unit as the mean of a sample of size n = 1. The second panel shows that to reduce the sampling variation (the SD) of the mean by half, one needs to quadruple the n. The third panel shows that to reduce the sampling variation (the SD) to 1/3, one needs to multiply the n by 9. Note, in passing, that at $n$ = 9, via the Central Limit Theorem, and the fact that the original distribution is 'CLT friendly' (the mode is not at either extreme, and the tails don't extend indefinitely), the shape of the sampling distribution is already close to Gaussian." width="672"><p class="caption">
Figure 8.7: Illustrations of SD’s of Sums and Means of n = 1, 4 and 9 independent and identically distributed random variables. Each RV is the length of a randomly selected word from a certain book. [Below, we will compare the mean word length in this book with the mean in a book by a competitor]. The distributions in purple were computed theoretically, using convolutions. Each row shows 1 ‘realization’ of each of the n random variables, with each word in a different color. The rows are sorted according to the values of the total [or mean] numbers of letters (chars) in the sample of n words. In the panels where n is 4 or 9, the leftmost n-1 characters of the n concatenated words are cropped, but the total/mean length is correct. The top panel lists the ‘per word’ variation of all of the words in the book, and its SD, sometimes called the ‘unit’ variability. You can also think of the length of each unit as the mean of a sample of size n = 1. The second panel shows that to reduce the sampling variation (the SD) of the mean by half, one needs to quadruple the n. The third panel shows that to reduce the sampling variation (the SD) to 1/3, one needs to multiply the n by 9. Note, in passing, that at <span class="math inline">\(n\)</span> = 9, via the Central Limit Theorem, and the fact that the original distribution is ‘CLT friendly’ (the mode is not at either extreme, and the tails don’t extend indefinitely), the shape of the sampling distribution is already close to Gaussian.
</p>
</div>
<p>As you would have expected, the purple <strong>sampling distributions narrow with increasing sample size</strong>, but the narrowing is <strong>not by a factor of <span class="math inline">\(n\)</span>,</strong> but by a <strong>factor of <span class="math inline">\(\sqrt{n}\)</span>.</strong> All the billions of <em>possible means</em> of samples of size <span class="math inline">\(n\)</span> = 4
<em>would have</em> a SD of 2.4/<span class="math inline">\(\sqrt{4}\)</span> = 2.4/2 = 1.2. The <em>possible means</em> of samples of size <span class="math inline">\(n\)</span> = 9 <em>would have</em> a SD of 2.4/<span class="math inline">\(\sqrt{9}\)</span> = 2.4/3 = 0.8.
<strong>Note the careful choice of the words</strong> in <em>italics</em>: in reality, you will only observe <strong>one</strong> of the billions of possibilities, so the sampling distribution is <em>imaginary</em> and thus the SD is also <em>imaginary</em> and so the SD of the <strong>conceptual</strong> sampling distribution is (<em>would be</em>) an <em>imaginary</em> 1.2 or 0.8. The only reason we are able to show the purple distributions is because of the laws of statistics, applied to all the words in the book, so we know the mean and the unit SD.</p>
</div>
<div id="difference-of-2-random-variables" class="section level3">
<h3>
<span class="header-section-number">8.7.4</span> Difference of 2 Random Variables<a class="anchor" aria-label="anchor" href="#difference-of-2-random-variables"><i class="fas fa-link"></i></a>
</h3>
<p>To keep it simple, let’s consider the two very simple random variables (RV’s), each taking just 2 values, and with equal probabilities, and independent of each other. But suppose now that we are interested in their difference</p>
<p><span class="math display">\[Difference = RV_1 \ (6 \ or \ 12) \ - \ RV_2 \ (8 \ or \ 16) \ = \ -10 \ or \ -4 \ or \ -2 \ or \ +4.\]</span></p>
<p>Now, imagine taking a random value of <span class="math inline">\(RV_1\)</span> and subtracting from it a random value of <span class="math inline">\(RV_2\)</span>. There are 4 possible differences, and in this deliberately constricted example, they are all distinct. A probability tree (next panel) helps to see the 4 possibilities, and the lengths of the branches denote the values of the RVs.</p>
<p>The 4 equally likely differences are -10, -4, -2 and +4, and so their mean is <span class="math inline">\(\frac{-10 -4 -2 +4}{4} = -3.\)</span> That the mean (expected value) of the difference equals the difference of the 2 individual means or expected values, is hardly surprising (and it is even true if the 2 RV’s were not independent). The 4 equally likely deviations from this -3 are -7, -1, +1, and +7, and so, <strong>from 1st principles</strong>, the <strong>Variance of the difference of the 2 RVs is</strong>
<span class="math display">\[\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \frac{100}{4} = 25.\]</span><br>
Thus, the <strong>SD of the difference of the 2 RVs is</strong> <span class="math inline">\(\sqrt{25}\)</span> = 5.</p>
<p>Fortunately, we don’t have to go back to 1st principles <strong>to calculate the SD of the differences of 2 RVs – but we do NOT do so by adding the 2 SDs</strong>. IT IS THE VARIANCES THAT ADD!</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="09-random-variables_files/figure-html/unnamed-chunk-10-1.png" alt="Variance (and thus SD) of the DIFFERENCE, RV1 - RV2, of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the difference from its expectation is decomposed into its 2 components. Each each squared deviation becomes a sum of 2 squares, and a 'cross-product'. But the 4 cross-products cancel each other, and you are left with the SUMS of two squares, the original 3-squared and the  original 4-squared, i.e. the variances of RV1 and RV2." width="672"><p class="caption">
Figure 8.8: Variance (and thus SD) of the DIFFERENCE, RV1 - RV2, of the two independent random variables, RV1 and RV2, shown above. Each of the 4 equally likely deviations of the difference from its expectation is decomposed into its 2 components. Each each squared deviation becomes a sum of 2 squares, and a ‘cross-product’. But the 4 cross-products cancel each other, and you are left with the SUMS of two squares, the original 3-squared and the original 4-squared, i.e. the variances of RV1 and RV2.
</p>
</div>
<p>Again one numerical example doesn’t prove the ‘the variances of a difference of two independent RVs is the sum of their variances’ rule, but you can check out other more complicated examples with more values, and uneven distributions – or use simulation – and satisfy yourself that it is a general rule.</p>
<p>So we can sum up the forgoing numerical example by saying:</p>
<p><span class="math display">\[ SD_1 = 3; SD_2 =4;\  but \ SD[Difference] \ \ne \ 3 + 4. \ \ \  SD[Difference] \ = \ \sqrt{3^2 + 4^2} = 5.\]</span><br>
It turns out that, from what we already knew about the sum of 2 independent RVs, we have anticipated this law. We didn’t need to go through all the formulae from scratch again. The reason had to do with the ‘mirror image’ distributions, such as the depths of the ocean, we saw above. The spread (SD, or variance) is the same, whether one writes/reads/computes from left to right, or right to left! In other words, the variance of the random variable <span class="math inline">\(-RV_2\)</span> is the same as that of the random variable <span class="math inline">\(RV_2.\)</span> So, by writing <span class="math inline">\(RV_1 - RV_2\)</span> as a sum, and using the law for the variance of a sum, we arrive at
<span class="math display">\[Var[RV_1 - RV_2] = Var[RV_1 + (-RV_2)] = Var[RV_1] + Var[(-RV_2)] = Var[RV_1] + Var[RV_2].\]</span></p>
</div>
</div>
<div id="linear-combinations-of-random-variables-regression-slopes" class="section level2">
<h2>
<span class="header-section-number">8.8</span> Linear combinations of random variables (regression slopes)<a class="anchor" aria-label="anchor" href="#linear-combinations-of-random-variables-regression-slopes"><i class="fas fa-link"></i></a>
</h2>
<p>In <em>non-experimental</em> research especially, the focus is typically on a fitted regression slope/coefficient, rather that on the simple difference
<span class="math inline">\(\bar{y}_1\)</span> - <span class="math inline">\(\bar{y}_0\)</span> between the means of the <span class="math inline">\(y\)</span>s observed at each of two investigator-chosen values (<span class="math inline">\(X=1\)</span> and <span class="math inline">\(X=0\)</span>) of the determinant (<span class="math inline">\(X\)</span>) being studied.</p>
<p>Even if the estimator does not have a closed form, the fitted slope(s)/coefficient(s) are linear combinations of the <span class="math inline">\(y\)</span>’s and the <span class="math inline">\(x\)</span>’s. Thus, since each of the <span class="math inline">\(n\)</span> <span class="math inline">\(y\)</span>’s contains a random element, the slope (<span class="math inline">\(\hat{\beta}\)</span>) is an <span class="math inline">\(x\)</span>-based linear combination of <span class="math inline">\(n\)</span> random variables.</p>
<p>Thus one can view all variances (and thus all standard errors) in a unified way, and not have to learn separate laws for separate chapters. To see how this unified view avoids the typical ‘silo’ approach to statistical tecnniques, see <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/UniversalSampleSize.pdf">Sample Size, Precision and Power Calculations: A Unified Approach</a>. [Software developers who thrive on separate ‘niche’ markets are threatened by this parsimonious approach, just as are those who write 800-page textbooks with separate chapters for t-tests,l proportions, regression, multiple regression, logiostic regression, Cox regression, survival analysis,etc.]</p>
<p>In the past, when first introduced to simple linear regression, it was common to learn the estimator formula and the Variance formula by heart, and use them to compute the fitted slope and the the standard error for a fitted slope by hand,
<span class="math display">\[\hat{\beta} = \frac{\sum (y-\bar{y})(x-\bar{x})}{ \sum (x-\bar{x})^2} \ ; \ \ Var[ \hat{\beta} ] = \frac{\sigma_e^2}{ \sum (x-\bar{x})^2} \ ; \ SE[ \hat{\beta} ] \ = \sqrt{Var[ \hat{\beta} ]} \ .\]</span>
In the variance formula, <span class="math inline">\(\sigma_e^2\)</span> is the variance in the ‘errors’ in the <span class="math inline">\(y\)</span>’s. In practice, we have to estimate this quantirty, but in our example, for disdactic purposes, we will pretend to ‘know’ its value.</p>
<p>Sadly, <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/SimpleMultipleLinearRegressionSampleSize.pdf">these formidable formulaa hide what is going on</a>.</p>
<p>To truly understand what is going on, lets consider a <strong>very simple example</strong> where we can <em>see</em> what is happening. A student from a country that uses the Fahrenheit (F) system moves to Montreal, and wishes to know how to translate the outside temperature, expressed as the number of degrees Celsius (C) and shown in the Metro, and heard on radio stations, back into the F scale (s)he is familiar with. The student knows that the conversion is of the form F = <span class="math inline">\(\alpha\)</span> + <span class="math inline">\(\beta \times C\)</span> but, rather than look them up on Google, decides to estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> from pairs of (C,F) readings, taking the C readings directly from the Metro screen, and the F ones from his/her own portable thermometer. Suppose that the C readings are displayed to 1 decimal place, but that the F thermometer only displays the F to the nearest integer.</p>
<p>Now, knowing that one it just takes 2 data points to draw a line, the student takes F measurements on two occasions, once when the displayed temperature is 12.5 C and one when it is 17.5 C. (The student didn’t know that when the real temperature was very close to xx.5 F, it had a 50% chance of being rounded up to xx+1 F, thereby creating an error of +0.5 F, and a 50% chance of being rounded down to xx-1 F, and producing an error of -0.5 F. Thus, the variance of each error is <span class="math inline">\((-0.5)^2 \times (1/2)\)</span> + <span class="math inline">\((+0.5)^2 \times (1/2)\)</span> = <span class="math inline">\(0.5^2,\)</span> and the SD is 0.5. (In the computer exercises, we will treat a broader type of random errors in the F readings).</p>
<p>Given that these two C settings correspond to 54.5 F and 63.5 F, but that the true temperature may be slightly on one side or the other of thse two values, what are the possible <span class="math inline">\(\beta\)</span> estimates? And, how variable would they be?</p>
<p>The 4 possibilities, shown as the slopes of the 4 fitted lines shown in black below, are (64-55)/5, (64-54)/5, (63-55)/5, and (63-54)/5, or, when sorted, 1.6, 1.8, 1.8 and 2.0 degrees F per degree C, each with probability 1/4, so that the variance of the equally likely slopes is
<span class="math display">\[Var[ \hat{\beta} ] =  \frac{(1.6 - 1.8)^2 + (1.8 - 1.8)^2  + (1.8 - 1.8)^2 + (2.0 - 1.8)^2}{4} = \frac{1}{50} \ =  \ 0.02,\]</span>
and the SE is <span class="math inline">\(\sqrt{0.02} = 0.14\)</span> degrees F per degree C.</p>
<p>Also shown in the diagram is the <strong>‘anatomy’ of the ‘random slope’</strong>. The possible slopes are displayed as a single expression in which the 2 random elements (i.e the 2 random variables, or the two ‘errors’ e<span class="math inline">\(_1\)</span> and e<span class="math inline">\(_2\)</span>) are isolated. The random slope is in the form of a constant (9/5) plus another constant (1/5) times the difference of two independent random errors. Applying all of the variance rules above, we have that
<span class="math display">\[Var[random  \ slope] = (1/5)^2 \times ( Var[e_1] + Var[e_2])  \ = \ \frac{1}{50} \ .\]</span></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="09-random-variables_files/figure-html/unnamed-chunk-12-1.png" alt="The 4  lines (in black) fitted to the 4 possible and equally likely pairs of data points (The  &lt;unknown to us&gt; 'true' values are shown in blue). By algebraicly isolating the contributions of the 2 random errors in F to the variation in the 4 slopes, the variance of the (random) slopes is easily computed using the laws for the variance of a combination of 2 independent random variables." width="672"><p class="caption">
Figure 8.9: The 4 lines (in black) fitted to the 4 possible and equally likely pairs of data points (The <unknown to us> ‘true’ values are shown in blue). By algebraicly isolating the contributions of the 2 random errors in F to the variation in the 4 slopes, the variance of the (random) slopes is easily computed using the laws for the variance of a combination of 2 independent random variables.
</unknown></p>
</div>
<p>The important point of this simple regression example is that even though in practice there would be many more data points, the principle/law used to calculate the sampling variation of a slope based on any number of datapoints remains the same: the fitted slope is still an <span class="math inline">\(x\)</span>-based linear combination of <span class="math inline">\(y\)</span>’s, (in this case, a closed form combination) and thus an <span class="math inline">\(x\)</span>-based linear combination of random errors – or more broadly of random deviations from the <span class="math inline">\(x\)</span>-conditional means of the ‘<span class="math inline">\(Y\)</span>’ variables. We will return later to all of the factors that influence the narrowness/width of sampling distributions generally, but you can maybe already see from the ‘algebraicly isolated’ representation of the slope that the more datapoints – and the wider apart they are on the <span class="math inline">\(x\)</span> axis and the smaller the magnitudes of the errors — the more reproducible the slope. The influence of this latter factors is less evident in the textbook formula for the Variance and the SE. This little exercise (next) should help you figure out how the factors come into it. <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/UniversalSampleSize.pdf">This piece</a> also focuses on these isssues in a transparent way. See the exercise on this.</p>
</div>
<div id="exercises-2" class="section level2">
<h2>
<span class="header-section-number">8.9</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-2"><i class="fas fa-link"></i></a>
</h2>
<ol start="2" style="list-style-type: decimal">
<li>Suppose you get into the <strong>life insurance</strong> business in a small way, just taking on one client. The client pays you a premium of $100 at the beginning of each year for 5 years. If the client dies within the next 5 years, you will pay client’s estate $20,000. Thus, at the end of 5 years, your possible earnings from this single client, along with the associated (actuarily-based) probablities are:</li>
</ol>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="va">possible.earnings</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">19900</span>,<span class="op">-</span><span class="fl">19500</span>,<span class="fl">100</span><span class="op">)</span>, <span class="fl">500</span> <span class="op">)</span> 
<span class="va">probability</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">183</span>,<span class="fl">186</span>,<span class="fl">189</span>,<span class="fl">191</span>,<span class="fl">193</span>,<span class="fl">99058</span><span class="op">)</span><span class="op">/</span><span class="fl">100000</span> 

<span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span><span class="va">possible.earnings</span>,<span class="va">probability</span><span class="op">)</span>
<span class="co">#&gt;      possible.earnings probability</span>
<span class="co">#&gt; [1,]            -19900     0.00183</span>
<span class="co">#&gt; [2,]            -19800     0.00186</span>
<span class="co">#&gt; [3,]            -19700     0.00189</span>
<span class="co">#&gt; [4,]            -19600     0.00191</span>
<span class="co">#&gt; [5,]            -19500     0.00193</span>
<span class="co">#&gt; [6,]               500     0.99058</span></code></pre></div>
<ul>
<li>Compute the expected earnings
<ul>
<li>Compute the variance (and thus the SD) of the possible earnings (a) using the definition (b) using the computational shortcut</li>
<li>Compute the ‘risk’, the SD as a percentage of the mean, as do investors ranking how risky various stocks are.</li>
<li>In statistics, and especially in applied statistics, what is the name for the SD as a percentage of the mean?</li>
</ul>
</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>
<p><strong>Errors caused by rounding</strong>. Suppose one has to analyze a large number of 3 digit numbers. To make the job easier, one rounds each number to the nearest 10, e.g.,<br><code>460 &lt;-- 460 461 462 463 464 ; 465 466 467 468 469 --&gt; 470.</code><br>
If the ending numbers of the unrounded data were uniformly distributed (each ending digit has a probability of 1/10), calculate:</p>
<ul>
<li>the average error per (rounded) number</li>
<li>the average absolute error per (rounded) number</li>
<li>the square root of the average squared error per (rounded)
number [‘root mean squared error’, or ‘RMSE’ for short]</li>
</ul>
</li>
<li>
<p><strong>Correcting for guessing</strong> on multiple choice exams.<br>
Suppose one wishes to estimate via a multiple choice examination [with <span class="math inline">\(k\)</span> answers to choose from for each question], what proportion <span class="math inline">\(\pi\)</span> of questions a student <strong>knows</strong> the answer to (excuse the dangling preposition!). Imagine that <span class="math inline">\(\pi\)</span> refers to the N (&gt;&gt; n) questions in the much larger bank of questions from which the <span class="math inline">\(n\)</span> exam questions are randomly selecetd.</p>
<ul>
<li>Show that the simple proportion <span class="math inline">\(p\)</span> of correctly answered questions gives a biased (over) estimate of <span class="math inline">\(\pi\)</span> if the student simply randomly guesses among the <span class="math inline">\(k\)</span> answers on questions where (s)he doesn’t know the answer. Do this by calculating the expected value of p (i.e. the average mark per question) when each answer is marked 1 if correct and 0 if not. (<em>Hint</em>: a tree diagram may help).</li>
<li>One can ‘de-bias’ the estimate by giving each correct answer a mark of 1 and each incorrect answer a negative mark. What negative mark (penalty) will provide an unbiased estimate of <span class="math inline">\(\pi\)</span>? Begin by finding the expected mark per question, then set it to <span class="math inline">\(\pi\)</span> and solve for the penalty. (<em>Hint</em>: If you prefer, use concrete values of <span class="math inline">\(\pi\)</span> and <span class="math inline">\(k\)</span> to see what penalty is needed.)</li>
</ul>
</li>
<li><p>Half the purchases of eggs in a market are for 6 eggs and half are for 12. What percentage of purchases are for a quantity that is more than 1 SD from the mean? less than 1 SD?</p></li>
<li><p>Half the people in a population have 2 organs of a certain type and half have none. What is the standard deviation of the number of organs a randomly selecetd person has?</p></li>
<li><p>Verify the variances displayed in the above Figure showing the distribution of a random variable, before and after measurement errors are added to it. Then subtract the smaller variance from the larger one to estimate the error variance. Finally show, by a separate calculation, why your answer ‘fits’ with the details of how the error-containing variable was constructed.</p></li>
<li><p>Consider children of parents who both carry a single copy of the CF gene. (In the absence of ..) How many of their offspring will have 0, 1 or 2 copies?</p></li>
<li><p>Half of a large number of orders were placed on Tuesday and half on Thursday. The combined orders were all jumbled together and shipped in 3 equal sized shipments on Monday Wednesday and Friday of the following week, arriving the same day they shipped. Calculate the mean and standard deviation of the number of days between ordering and arrival. Use a probability tree to depict the randomness, and to show the calculations.</p></li>
<li><p>Refer to the example where the student tries to estimate the scaling factor between degrees C and degrees F.</p></li>
</ol>
<ul>
<li>What if the student took the F readings at two C values that are 10 C (rather than 5 C) apart?, i.e. at 12.5 C and 22.5 C?
<ul>
<li>How would the Variance and the SE be altered?</li>
<li>What if the student took the F readings at four equally spaced C values 5 C apart, i.e., at 7.5 C, 12.5 C, 17.5C and 22.5 C?.
+If you were limited to <span class="math inline">\(n\)</span> C values, how would you decide where to place them?</li>
<li>What if, rather than <span class="math inline">\(0.5^2\)</span>, the ‘errors’ in F had a variance of <span class="math inline">\(1^2\)</span> or <span class="math inline">\(2^2\)</span>?</li>
</ul>
</li>
</ul>
<ol start="11" style="list-style-type: decimal">
<li>Random Variable: Are the following DISCRETE or CONTINUOUS?</li>
</ol>
<ul>
<li>
<em>How long</em> you have to wait for bus / elevator / surgery/ download to complete</li>
<li>the <em>blood group</em> of n = 1 randomly selected person</li>
<li>
<em>how many tries</em> before pass a course</li>
<li>
<em>how many</em> of n = 20 randomly selected persons will return questionnaire in pilot study</li>
<li>
<em>length of song</em> on a CD</li>
<li>
<em>mean cholesterol level</em> in sample of n = 30 randomly selected persons</li>
<li>
<em>how hot</em> it is going to be today</li>
<li>
<em>how much</em> snow we will get next winter</li>
<li>
<em>time</em> someone called (on answering machine)</li>
<li>
<em>value of test-statistic</em> if 2 populations sampled from have the same mean</li>
<li>
<em>how much</em> ice McDonalds puts in soft drink</li>
<li>
<em>how many</em> calories in hamburger</li>
<li>
<em>how many</em> numbers you get correct in 6/49?</li>
<li>
<em>where</em> roulette wheel stops</li>
<li>
<em>how many</em> “wrong number” calls received</li>
<li>
<em>how many</em> keys you have to try before get the right one</li>
<li>
<em>how much</em> water consumed by 100 homes</li>
</ul>
</div>
<div id="summary" class="section level2">
<h2>
<span class="header-section-number">8.10</span> Summary<a class="anchor" aria-label="anchor" href="#summary"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li><p>The concepts of a random variable, and of its expectation and variance, underpin all of statistical inference. That is why this chapter is so central, even if we don’t apply the laws by hand.</p></li>
<li><p>The laws governing the variance of the sum and the mean of <span class="math inline">\(n\)</span> random variables are the basis for Standard Errors (SEs) of statistics (parameter estimates based on aggregated data).</p></li>
<li><p>When assessing the sampling variability or a sum or mean of independent random variables, it is not their standard deviations that sum (add), but rather their variances.</p></li>
<li><p>This is why we have the <span class="math inline">\(\sqrt{n}\)</span> law in Statistics. The SE of a statistic is directly proportional to <span class="math inline">\(\sqrt{n}\)</span> if we are dealing with a sum, and inversely proportional to <span class="math inline">\(\sqrt{n}\)</span> (or proportional to 1/<span class="math inline">\(\sqrt{n}\)</span> ) if we are dealing with a mean.</p></li>
<li><p>Since proportions are means (albeit of RVs that just take on two possibvle values), the same laws apply to them as well.</p></li>
<li><p>This law was not understood/appreciated until recent centuries. Statistical historial Stephen Stigler has a very nice example, in <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/c323/pyx.pdf">this article</a>, and retold in his book <a href="https://www-degruyter-com.proxy3.library.mcgill.ca/view/title/521193">The Seven Pillars of Statistical Wisdom</a>, where failure to understand it wrong gave people quite a bit of leeway to cheat.</p></li>
<li><p>It’s also why statisticians are forced to work with variances when developing the properties of estimators. But as an end user, you will typically work with the square roots of these, and speak about the number of children per parent rather than square children per square parent.</p></li>
<li><p>The law governing the variance of a difference of two random variables is even more important, since we are more often interested in contrasts than the level in a single context.</p></li>
<li><p>Whether we are add or subtract independent random variables, the result is more variable than its parts.</p></li>
<li><p>A regression slope can be represented as a linear combination (with varying positive and negative combining weights) of random variables, and so the variance and SD of the sampling distribution of a slope are gioverned by these same fundamental laws.</p></li>
<li><p>Although the main focus was on Variances and SDs, along the way in these above sub-sections, you saw the Central Limit Theorem (CLT) trying to exert itself. Although the narrowness/width of a sampling distribution is measured by a variance or SD, the <em>CLT focuses more on its shape</em>. It is not possible to give a general rule for the <span class="math inline">\(n\)</span> at which the CTL will ensure a sufficiently Gaussian shape for a sampling distribution. How ‘close’ to Gaussian any particular sampling distribution is depends on the ‘parent’ RV and the <span class="math inline">\(n\)</span>, but also on what you consider is ‘close enough for Government work’.</p></li>
<li><p>It is not critical that we ‘do’ several exercises on the theory (laws) in this chapter. After all, you will seldom have to manually do the SE calculations based on these laws – the statistical software will do it for you. But, you do need to understand the factors that make the SE’s big or small, and the concepts involved in the propagation – and reduction – of errors. There are several exercises in the computing session that will allow you to ‘see’ the laws in action, so that you can adopt them as guiding principles for study design, and for appreciating the ‘precision’ with which you can chase (take a shot at) statistical parameters.</p></li>
</ul>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="ChapProbability.html"><span class="header-section-number">7</span> Probability</a></div>
<div class="next"><a href="ChapNormal.html"><span class="header-section-number">9</span> The Normal Random Variable</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#randomVariables"><span class="header-section-number">8</span> Random Variables</a></li>
<li><a class="nav-link" href="#objectives-1"><span class="header-section-number">8.1</span> Objectives</a></li>
<li><a class="nav-link" href="#random-variables"><span class="header-section-number">8.2</span> Random Variables</a></li>
<li>
<a class="nav-link" href="#expectation-of-a-random-variable"><span class="header-section-number">8.3</span> Expectation of a Random Variable</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#discrete-random-variable"><span class="header-section-number">8.3.1</span> Discrete Random Variable</a></li>
<li><a class="nav-link" href="#continuous-random-variable"><span class="header-section-number">8.3.2</span> Continuous Random Variable</a></li>
<li><a class="nav-link" href="#why-should-we-care-about-the-expectation"><span class="header-section-number">8.3.3</span> Why should we care about the expectation?</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#expected-value-of-a-function-of-a-random-variable"><span class="header-section-number">8.4</span> Expected value of a function of a random variable</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#examples-1"><span class="header-section-number">8.4.1</span> Examples</a></li></ul>
</li>
<li>
<a class="nav-link" href="#variance-and-thus-sd-of-a-random-variable"><span class="header-section-number">8.5</span> Variance (and thus, SD) of a random variable</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#definitions"><span class="header-section-number">8.5.1</span> Definitions</a></li>
<li><a class="nav-link" href="#why-use-the-variance"><span class="header-section-number">8.5.2</span> Why use the variance?</a></li>
</ul>
</li>
<li><a class="nav-link" href="#variance-and-sd-of-a-function-of-a-random-variable"><span class="header-section-number">8.6</span> Variance and SD of a function of a random variable</a></li>
<li>
<a class="nav-link" href="#sums-means-differences-of-random-variables"><span class="header-section-number">8.7</span> Sums, means, differences of random variables</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#a-sum-of-2-or-n-random-variables"><span class="header-section-number">8.7.1</span> A sum of 2 or \(n\) random variables</a></li>
<li><a class="nav-link" href="#measurement-errors"><span class="header-section-number">8.7.2</span> Measurement Errors</a></li>
<li><a class="nav-link" href="#mean-of-2-or-n-random-variables"><span class="header-section-number">8.7.3</span> Mean of 2 or \(n\) random variables</a></li>
<li><a class="nav-link" href="#difference-of-2-random-variables"><span class="header-section-number">8.7.4</span> Difference of 2 Random Variables</a></li>
</ul>
</li>
<li><a class="nav-link" href="#linear-combinations-of-random-variables-regression-slopes"><span class="header-section-number">8.8</span> Linear combinations of random variables (regression slopes)</a></li>
<li><a class="nav-link" href="#exercises-2"><span class="header-section-number">8.9</span> Exercises</a></li>
<li><a class="nav-link" href="#summary"><span class="header-section-number">8.10</span> Summary</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>EPIB607</strong>" was written by Sahir Bhatnagar and James A Hanley. It was last built on 2021-12-07.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
