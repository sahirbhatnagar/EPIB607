\documentclass[10pt]{beamer}


%\input{slides_header.tex}
\input{/home/sahir/git_repositories/EPIB607/slides/slides_header2.tex}
\graphicspath{{/home/sahir/git_repositories/EPIB607/slides/figure/}}

%\let\oldShaded\Shaded
%\let\endoldShaded\endShaded
%\renewenvironment{Shaded}{\footnotesize\oldShaded}{\endoldShaded}

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}


\usepackage{xparse}
\NewDocumentCommand\mylist{>{\SplitList{;}}m}
{
	\begin{itemize}
		\ProcessList{#1}{ \insertitem }
	\end{itemize}
}
\NewDocumentCommand\mynum{>{\SplitList{;}}m}
{
	\begin{enumerate}
		\ProcessList{#1}{ \insertitem }
	\end{enumerate}
}
\newcommand\insertitem[1]{\item #1}

\newcommand\FrameText[1]{%
	\begin{textblock*}{\paperwidth}(0pt,\textheight)
		\raggedright #1\hspace{.5em}
\end{textblock*}}

\begin{document}

	<<setup, include=FALSE>>=
library(knitr)
knitr::opts_chunk$set(cache=FALSE, message = FALSE, tidy = FALSE, warning = FALSE,
echo = FALSE, 
#fig.width = 8, 
fig.asp = 0.75, 
fig.align = 'center', 
#out.width = "0.50\\linewidth", 
size = 'tiny')

# the kframe environment does not work with allowframebreaks, so remove it
#knit_hooks$set(document = function(x) {
#gsub('\\\\(begin|end)\\{kframe\\}', '', x)
#})

library(tidyverse)
library(NCStats)
options(digits = 2)


#knitr::opts_chunk$set(background = '#FFFF00')
library(tools) #needed for include_graphics2 function
source("/home/sahir/git_repositories/EPIB607/slides/bin/include_graphics2.R")
@

%\title{Introduction to Regression Trees}
%\author{Sahir Bhatnagar \inst{1}}
%\author[shortname]{Sahir Rai Bhatnagar, PhD Candidate (Biostatistics) }
%\institute[shortinst]{Department of Epidemiology, Biostatistics and Occupational Health}

	\title{015 - Midterm Review}
\author{EPIB 607 - FALL 2020}
\institute{
	Sahir Rai Bhatnagar\\
	Department of Epidemiology, Biostatistics, and Occupational Health\\
	McGill University\\
	
	\vspace{0.1 in}
	
	\texttt{sahir.bhatnagar@mcgill.ca}\\
	%\texttt{\url{https://sahirbhatnagar.com/EPIB607/}}
}

\date{slides compiled on \today}

\maketitle



%\begin{frame}[label=toc]{Table of Contents}
%	\tableofcontents
%\end{frame}

%\AddButton

\section{Exam Details}
\begin{frame}{Exam Details}
	
	\begin{itemize}
		\setlength\itemsep{.51em}
		\item \textbf{When:} Friday October 23, 3:35pm - 5:35pm.
		\item This is a 2 hour, open book exam. Any material on myCourses (EPIB607/613) and personal notes are permitted.
		\item You are not permitted to use the internet and you must work alone. Using the internet or obtaining help from anyone else is considered Cheating as per \href{https://www.mcgill.ca/students/srr/academicrights/integrity/cheating}{Article 17 of the Code of Student Conduct and Disciplinary Procedures}
%	\item The exam is out of 70. There are 7 questions, each worth 10 points. Write down all your answers in the provided booklet. 
		\item Provide units and state your assumptions when applicable. Label axes and write answers in complete sentences when appropriate.
		\item The format of the exam will follow the assignments. That is, you will be required to complete a series of questions in an RMarkdown document and knit to pdf. Your solutions for each question must then be uploaded to Crowdmark. A template will be provided.
		\item You will be given 1 extra hour to upload your solutions.
		%\item Some commonly used $z$ and $t$ quantiles will be provided in a Table. If a question requires the use of $z$ or $t$ probabilites/quantiles not found in the provided Table, write the corresponding R code instead. When possible, you should complete the calculations.
		
	\end{itemize}
	
\end{frame}

\begin{frame}{Topics to be covered}
	
	\begin{enumerate}
		\setlength\itemsep{.51em}
		\item Data visualization (histograms, boxplots, scatterplots, line plots), Tidy Data, Color Palettes
		\item Descriptive statistics (mean, median, range, IQR, sd, correlation)
		\item Normal Curve Calculations
		\item Sampling Distributions, CLT, Bootstrap
		\item Confidence intervals and p-values for one sample mean and one sample proportion
		\item Hypothesis Testing
		\item Power and Sample size calculations
	\end{enumerate}
	
\end{frame}

\section{Data visualization}

\begin{frame}{Aesthetics}
	\begin{itemize}
		\setlength\itemsep{.51em}
		\item Aesthetics
		\framedgraphic{scales.jpg}
		\pause 
		\item Commonly used aesthetics in data visualization: position, shape, size, color, line width, line type. Some of these aesthetics can represent both continuous and discrete data (position, size, line width, color) while others can only represent discrete data (shape, line type)	
	\end{itemize}
\end{frame}

\begin{frame}{Types of Graphs}
	\begin{itemize}
		\item Review the types of graphs created in the assignments.
		\item You should be able to critique a graph and propose appropriate graphics for a given dataset. Be mindful of the research question. The graphic should try to answer the research question. 
		\item \url{https://serialmentor.com/dataviz/directory-of-visualizations.html}
		\item \url{https://www.data-to-viz.com/}
	\end{itemize}
\end{frame}


\begin{frame}[fragile]{Boxplots with qualitative palette}
<<echo = TRUE, fig.asp=0.601>>=
library(oibiostat); data("famuss")
library(ggplot2)
library(colorspace)

ggplot(famuss, aes(x = actn3.r577x, y = bmi, fill = actn3.r577x)) + 
geom_boxplot() + 
colorspace::scale_fill_discrete_qualitative()
@
\end{frame}


\begin{frame}[fragile]{Conditional distribution of genotype \textit{given} race}
<<echo = TRUE, fig.asp=0.681>>=
sjPlot::plot_xtab(famuss$race, famuss$actn3.r577x, margin = "row")
@
\end{frame}





	\begin{frame}[fragile]{Scatter plots with sequential palette}
	<<echo = TRUE, fig.asp=0.681>>=
	ggplot(famuss, aes(x = height, y = weight, color = bmi)) + 
	geom_point() + 
	colorspace::scale_color_continuous_sequential(palette = "Viridis")
	@
\end{frame}



\begin{frame}{Variable Types}
	\begin{itemize}
		\setlength\itemsep{1.5em}
		\item quantitative/numerical continuous (1.3, 5.7, 83, $1.5\times 10^{-2}$)
		\item quantitative/numerical discrete (1,2,3,4)
		\item qualitative/categorical unordered (dog, cat, fish)
		\item qualitative/categorical ordered (good, fair, poor)
	\end{itemize}
	
	
	
\end{frame}




\begin{frame}[fragile]{Color Palettes: Cynthia Brewer}
	
	<<echo=TRUE>>=
	pacman::p_load(RColorBrewer)
	RColorBrewer::display.brewer.all()
	@
	
	
\end{frame}



\begin{frame}[fragile]{Color Palettes: viridis}
	
	
	
	\framedgraphic{viridis.png}
	
	
\end{frame}

\section{Tidy Data}

\begin{frame}{Tidy data}
	
	\begin{itemize}
		\setlength\itemsep{.51em}
		\item Each variable forms a column.
		\item Each observation forms a row.
		\item Each type of observational units forms a table
		\item Tidy data is ready for regression routines and plotting
	\end{itemize}
	
	
	\framedgraphic{tidy.png}
	
\end{frame}




\begin{frame}[fragile]{Example: Is it tidy?}
	
	\centering
	\includegraphics[scale=0.8]{hivtable.pdf}
	
	
\end{frame}

\section{Descriptive statistics}


\begin{frame}{Descriptive statistics}
	\begin{itemize}
		\setlength\itemsep{1.5em}
		\item Boxplots, histograms, density plot
		\item IQR, median, mode, mean, min, max, range
		\item Q1, Q3
		\item Skewness (long left/right tail)
		\item Correlation
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{Descriptive stats by group}
	<<echo = TRUE, fig.asp=0.601, size = 'small'>>=
	library(oibiostat); data("famuss")
	library(dplyr)
	
	famuss %>% 
	  dplyr::group_by(actn3.r577x) %>%
	  dplyr::summarise(mean_bmi = mean(bmi), 
	                   sd_bmi = sd(bmi))

	@
	
\end{frame}


\begin{frame}[fragile]{Subsetting data}
	<<echo = TRUE, fig.asp=0.601, size = 'scriptsize'>>=
	library(oibiostat); data("famuss")
	library(dplyr)
	
	f.male <- famuss %>% 
	             dplyr::filter(sex == "Male")
	
	
	f.male.cauc <- famuss %>% 
	                  dplyr::filter(sex == "Male" & race == "Caucasian")

    f.bmi.low <- famuss %>% 
                     dplyr::filter(bmi <= 23)
	
	@
	
\end{frame}


\begin{frame}{Standard error (SE) of a sample statistic}
	\begin{itemize}
		\item Recall: When we are talking about the variability of a
		\textbf{statistic}, we use the term \textbf{standard error} (not
		standard deviation). The standard error of the sample mean is $\sigma/\sqrt{n}$.
	\end{itemize}
	
	
	\begin{remarkm}[SE vs. SD]
		\begin{center}
			In quantifying the instability of the sample mean ($\bar{y}$) statistic,
			we talk of SE of the mean (SEM) \\ \ \\
			SE($\bar{y}$) describes how far $\bar{y}$ could (typically) deviate from $\mu$; \\ \ \\
			SD($y$) describes how far an individual $y$ (typically) deviates from $\mu$ (or from $\bar{y}$).
		\end{center}
	\end{remarkm}	
	
	
\end{frame}



\section{Sampling Distributions, CLT, Confidence Intervals and p-values}


\begin{frame}{Parameters,  Samples,  and  Statistics}
	\begin{itemize}
		\item \textbf{Paramter}: An  unknown  numerical  constant  pertaining  to  a  population/universe,  or  in  a  statistical  model. 
		\begin{itemize}
			\item $\mu$: population mean $\qquad\qquad$ $\pi$: population proportion
		\end{itemize}

		\item \textbf{Statistic}: A  numerical  quantity  calculated  from  a  sample. The  empirical counterpart of the parameter,  used  to  \textit{estimate}  it.

		\begin{itemize}
			\item $\bar{y}$: sample mean $\qquad\qquad$ $p$: sample proportion
		\end{itemize}
	\end{itemize}
	
	\pause
	\Wider[4em]{
		\centering
		\includegraphics[scale=0.35]{MeansFig1.png}
	}
	
	
\end{frame}


\frame{\frametitle{Samples must be random} 
	
	\begin{itemize}
		\item 	The validity of inference will depend on the
		way that the sample was collected. If a sample was collected badly, no amount of
		statistical sophistication can rescue the study. \\ \ \\ 
		
		\item Samples should be \textbf{random}. That is, there should be no systematic set of
		characteristics that is related to the scientific question of interest that causes some
		people to be more likely to be sampled than others. The simplest type of randomization
		selects members from the population with equal probability (a uniform distribution). \\ \
		\\ 
		
		\pause
		
		\item 	\textbf{Do not cheat by}  
		\begin{itemize}
			\item 	Taking 5 people from the \emph{same} household to estimate
			
			\begin{itemize}
				\item proportion of Québécois who don't have a family doctor
				\item who saw a medical doctor last year
				\item average rent
			\end{itemize}  
			
			
			
			
			\item Sampling the depth of the ocean \emph{only around Montreal} to estimate \begin{itemize}
				\item proportion of  Earth's  surface  covered  by  water
			\end{itemize} 
			
		\end{itemize}
	\end{itemize}
}






\frame{\frametitle{Sampling Distributions} 
	
	\begin{defm}[Sampling Distribution]
		\begin{itemize}
			\setlength\itemsep{1.5em}
			\item The sampling distribution of a statistic is the distribution of values taken by the statistic in \textbf{all possible samples of the same size} from the same population.
			\item The standard deviation of a sampling distribution is called a \textbf{standard error}
		\end{itemize} 
	\end{defm}
	
	
	
}


\begin{frame}[fragile]{Sampling Distributions}
	
	<<fig.asp = 0.518, fig.cap = 'Ideal world. Sampling distributions are obtained by drawing repeated samples from the population, computing the statistic of interest for each, and collecting (an infinite number of) those statistics as the sampling distribution'>>=
	source("../bin/sampling_dist_figure.R")
	@
	
\end{frame}


\frame{\frametitle{Why are sampling distributions important?} 
	
	\begin{itemize}
		\setlength\itemsep{2em} 
		\item They  tell  us  how  far  from  the  target  (true  value  of  the  parameter)  our  statistical  \emph{shot}  at  it  (i.e.  the  statistic  calculated  form  a  sample)  is  likely  to  be,  or, to  have  been.  \pause 
		
		\item Thus,  they  are  used  in  confidence  intervals  for  parameters. Specific  sampling  distributions  (based  on  a null value  for  the  parameter)  are also  used  in  statistical  tests  of  hypotheses.
		
	\end{itemize}	
	
	
}


\begin{frame}[fragile]{Sampling distribution: mean depth of the ocean}
	
	<<echo=FALSE, eval=TRUE>>=
	water_results <- read.csv("EPIB607_FALL2018_water_exercise - water.csv", as.is=TRUE)
	water_results <- water_results[,1:6]
	water_results <- water_results[complete.cases(water_results), ]
	#water_results[,1:6]
	# count the number of students who provided a mean and proportion
	N.r <- nrow(water_results)
	#water_results[,"Mean.5.depths"]
	@
	
	<<echo=FALSE, eval=TRUE>>=
	d.BREAKS <- seq(1000,6000,500)
	par(mfrow=c(2,1), mai = c(0.45,0.45,0.45,0.1))
	hist(water_results[,"Mean.5.depths"], 
	xlim = c(0,6000),
	ylim = c(0, N.r/1.5), 
	breaks = d.BREAKS,
	xlab = "Students' Estimates of Mean Ocean Depth (m)",
	main = "n = 5")
	abline(v=3700, col = "#009E73", lty = 2)
	text(3800, 45, expression(mu))
	text(4100, 46, "=3700m")
	hist(water_results[,"Mean.20.depths"], 
	xlim = c(0,6000),
	ylim = c(0, N.r/1.5), 
	breaks = d.BREAKS,
	xlab = "Students' Estimates of Mean Ocean Depth (m)",
	main = "n = 20")
	abline(v=3700, col = "#009E73", lty = 2)
	text(3800, 45, expression(mu))
	text(4100, 46, "=3700m")
	@
	
\end{frame}



\begin{frame}[fragile]{Sampling distribution: proportion covered by water}
	
	<<echo=FALSE, eval=TRUE>>=
	water_results <- read.csv("EPIB607_FALL2018_water_exercise - water.csv", as.is=TRUE)
	water_results <- water_results[,1:6]
	water_results <- water_results[complete.cases(water_results), ]
	#water_results[,1:6]
	# count the number of students who provided a mean and proportion
	N.r <- nrow(water_results)
	#water_results[,"Mean.5.depths"]
	@
	
	
	<<echo=FALSE>>=
	par(mfrow=c(2,1), mai = c(0.45,0.45,0.45,0.1))
	plot(table(water_results[,"PropnW.5.locations"]), 
	xlim = c(0,1),
	xlab = "Students' Estimates of Proportion Covered by Water",
	main = "n = 5", 
	ylim = c(0, N.r/1.5), 
	ylab = "Frequency")
	abline(v=0.71, col = "#009E73", lty = 2)
	text(0.72, 40, expression(mu))
	text(0.76, 41, "=0.71")
	plot(table(water_results[,"PropnW.20.locations"]), 
	xlim = c(0,1),
	xlab = "Students' Estimates of Proportion Covered by Water",
	main = "n = 20", 
	ylim = c(0, N.r/1.5), 
	ylab = "Frequency")
	abline(v=0.71, col = "#009E73", lty = 2)
	text(0.72, 40, expression(mu))
	text(0.76, 41, "=0.71")
	@
	
\end{frame}




\begin{frame}[fragile]{Normal Distribution: For probabilities we use $pnorm$}
	
	
	<<probs2, echo = TRUE, fig.width = 3, fig.asp = 0.618, fig.align = 'center', out.width = "60%">>=
	stats::pnorm(q = 130, mean = 100, sd = 13)
	@
	
	
	<<probs3, echo = TRUE, fig.width = 3, fig.asp = 0.618, fig.align = 'center', out.width = "60%">>=
	mosaic::xpnorm(q = 130, mean = 100, sd = 13)
	@
	
	
	\begin{itemize}
		\item \texttt{pnorm} returns the integral from $-\infty$ to $q$ for a $\mathcal{N}(\mu, \sigma)$
		\item \texttt{pnorm} goes from \textit{quantiles} (think $Z$ scores) to probabilities
	\end{itemize}
	
\end{frame}



\begin{frame}[fragile]{Normal Distribution: For quantiles we use $qnorm$}
	
	
	
	<<probs4, echo = TRUE, fig.width = 3, fig.asp = 0.618, fig.align = 'center', out.width = "60%">>=
	stats::qnorm(p = 0.0104, mean = 100, sd = 13)
	@
	
	
	
	<<probs5, echo = TRUE, fig.width = 3, fig.asp = 0.618, fig.align = 'center', out.width = "60%">>=
	mosaic::xqnorm(p = 0.0104, mean = 100, sd = 13)
	@
	
	
	
	\small{
		\begin{itemize}
			\item \texttt{qnorm} answers the question: What is the Z-score of the $p$th percentile of the normal distribution?
			
			\item \texttt{qnorm} goes from \textit{probabilities} to quantiles 
		\end{itemize}
	}
\end{frame}

\begin{frame}[fragile]{Empirical Rule or 68-95-99.7\% Rule}
	
	\framedgraphic{6899rule.png}
	
\end{frame}



\begin{frame}[fragile]{Quadruple the work, half the benefit}
	
	\framedgraphiccaption{ROOToceanAll.png}{When the sample size increases from 4 to 16, the spread of the sampling distribution for the mean is reduced by a half, i.e., the range is cut in half. This is known as the curse of the $\sqrt{n}$}
\end{frame}


\frame{\frametitle{The Central Limit Theorem (CLT)} 
	
	\begin{itemize}
		\item The sampling distribution of $\bar{y}$ is, for a large enough $n$, close to Gaussian in shape no matter what the shape of the distribution of individual $Y$ values. 
		\item This phenomenon is referred to as the CENTRAL LIMIT THEOREM 
		\item The CLT applied also to a \underline{sample proportion}, \underline{slope}, \underline{correlation}, or any other statistic created by \underline{aggregation of individual observations}
	\end{itemize}
	
	\begin{thm}[Central Limit Theorem]
		\begin{center}
			if $Y \sim ???(\mu_Y, \sigma_Y)$, then \\ \ \\
			$\bar{y} \sim \mathcal{N}(\mu_Y, \sigma_Y / \sqrt{n})$
		\end{center}
	\end{thm}
	
	\vspace{1.25cm}
	%pause
}

\begin{frame}{Confidence Interval}
	
	\begin{defm}[Confidence Interval]
		A level $C$ confidence interval for a parameter has two parts:
		\begin{enumerate}
			\item An interval calculated from the data, \underline{usually} of the form $$\textrm{estimate} \pm \textrm{margin of error}$$ where the estimate is a sample statistic and the margin of error represents the accuracy of our guess for the parameter.
			\item A confidence level $C$, which gives the probability that the interval will capture the true parameter value in \textit{different possible samples}. That is, the confidence level is the success rate for the method
		\end{enumerate}
	\end{defm}
	
	%\framedgraphic{6899rule.png}
	
\end{frame}

\frame{\frametitle{Confidence Interval: A simulation study}
	
	\vspace*{-0.1in}
	
	\begin{figure}
		\begin{center}
			\epsfig{figure=CIplots.eps,width=3.2in,height=2.7in}
			\caption{\small{True parameter value is 2 (red line). Each horizontal black line represents a 95\% CI from a sample and contains the true parameter value. The blue CIs do not contain the true parameter value. 95\% of all samples give an interval that contains the population parameter.}}
		\end{center}
	\end{figure}
}


\begin{frame}{Interpreting a frequentist confidence interval}
	\begin{itemize}
		\setlength\itemsep{1em}
		\item The confidence level is the success rate of the method that produces the interval. \pause
		\item We don't know whether the 95\% confidence interval from a \underline{particular
			sample} is one of the 95\% that capture $\theta$ (the unknown population parameter), or one of the unlucky 5\% that miss. \pause
		\item To say that we are \underline{95\% confident} that the unknown value of $\theta$
		lies between $U$ and $L$ is shorthand for ``We got these numbers using a
		method that gives correct results 95\% of the time.''
	\end{itemize}
\end{frame}

\begin{frame}[fragile]{68\% Confidence interval using \texttt{qnorm}}
	
	<<echo=FALSE>>=
	allLocations <- read.csv("earth-locations-20180914.csv")
	allLocations$water  = 1*(allLocations$alt < 0)
	
	# water only
	depthsOfWater = allLocations[allLocations$water==1,]
	depthsOfWater$depth = -depthsOfWater$alt
	panel = 1
	if(panel==1) y = round(depthsOfWater$depth/100)
	f = table(y)
	x=as.numeric(dimnames(f)[[1]])
	Y=0:max(x) 
	FREQ=approx(x,f,Y)$y
	n.bins=length(FREQ)
	max.Y = max(Y); 
	max.X = max.Y
	M = 1.05*max(f)
	FREQ[1+Y] =  FREQ/sum(FREQ)
	AVE = sum(Y*FREQ)
	SD = sqrt(sum( FREQ*(Y-AVE)^2 ) )
	alreadyOrig = FREQ
	already = FREQ
	max.n = 16; 
	show=c(1,2,3,4,5,5,6,7,8,9,16)
	YLIM=sqrt(max.n/(panel^2.5))*max(FREQ)*c(-0.11,0.75)
	XLAB=c("OCEAN DEPTH (units = 100m)","LAND ELEVATION")
	
	SE <- 4.20
	@
	
	
	\vspace*{-0.09in}
	
	\Wider[2em]{
		<<fig.cap="68\\% Confidence interval calculated using  \\mbox{\\texttt{qnorm(p = c(0.16,0.84), mean = 37, sd = 4.2)}}", fig.asp=0.618>>=
		# with 68% ci
		plot(Y,already,pch=19,lwd=1,col="white",
		type="l",ylim=YLIM, xlim=c(0,max.X),
		ylab="Density", xlab=XLAB[panel] )
		polygon(c(0,Y),c(0,FREQ),
		col=c("#56B4E9","bisque","grey98")[panel],
		border="grey10",lwd=1)
		# legend("topright", legend = "Y: Depths of the ocean", 
		#        col = "#56B4E9", pch = 15, pt.cex = 2)
		curve(dnorm(x,mean = AVE,sd = SE), add = TRUE, lwd = 2) 
		# text(1.5*AVE,0.08,
		#      paste("means of samples of size",toString(16)),
		#      adj = c(0,0.5),
		#      col = viridis::inferno(9, end = 0.7, direction = 1)[1],
		#      cex = 0.95)
		lower.x <- qnorm(p = c(0.16), AVE, SE)
		upper.x <- qnorm(p = c(0.84), AVE, SE)
		legend("topright", legend = c("Y: Depths of the ocean",
		"Sampling distribution for \n means of samples of size 16",
		sprintf("68%% Confidence interval for \n the mean depth of the ocean:\n [%.f, %.f]",
		lower.x, upper.x)),
		col = c("#56B4E9","black",RColorBrewer::brewer.pal(9, "Set1")[4]), 
		y.intersp = 2,
		bty = "n",
		pch = c(15,NA,15), lty = c(NA,1,NA), pt.cex = 2, lwd = c(NA,2,NA))
		segments(AVE,0, AVE, 0.10,lty="dotted")
		# text(AVE*1.05,  0.15*YLIM[1],toString(round(AVE,0)), adj=c(0.5,1),
		#      cex=0.85 )
		# text(0.95*AVE,0.15*YLIM[1],expression(mu), adj=c(0.5,1.25),
		#      cex=0.95 )
		# text(.99*AVE,0.07*YLIM[1],"=", adj=c(0.5,1.25),
		#      cex=0.95 )
		# arrows(x0 = AVE, x1 = AVE + SE, y0 = 0.055, length = 0.05)
		# segments(AVE + SE,0, AVE + SE, 0.06,lty="dotted")
		# #text(x = AVE*1.05, y = 0.051, '{', srt = 90, cex = 1.5, family = 'Helvetica Neue UltraLight')
		# text((AVE)*1.25,0.06,latex2exp::TeX("$\\sigma / \\sqrt{16}$"), adj=c(0.5,1.25),
		#      cex=0.95 )
		
		step <- (upper.x - lower.x) / 100
		bounds <- c(lower.x, upper.x)
		cord.x <- c(lower.x,seq(lower.x,upper.x,step),upper.x)
		cord.y <- c(0,dnorm(seq(lower.x,upper.x,step),AVE,SE),0)
		polygon(cord.x,cord.y,col=RColorBrewer::brewer.pal(9, "Set1")[4])
		text(lower.x, -0.005, round(lower.x, 0))
		text(upper.x, -0.005, round(upper.x, 0))
		segments(AVE,0, AVE, 0.10,lty="dotted")
		@
		
	}
\end{frame}



\begin{frame}[fragile]{95\% Confidence interval using \texttt{qnorm}}
	
	\vspace*{-0.09in}
	
	
	\Wider[2em]{
		<<fig.cap="95\\% Confidence interval calculated using  \\mbox{\\texttt{qnorm(p = c(0.025,0.975), mean = 37, sd = 4.2)}}", fig.asp=0.618>>=
		# with 95% ci
		plot(Y,already,pch=19,lwd=1,col="white",
		type="l",ylim=YLIM, xlim=c(0,max.X),
		ylab="Density", xlab=XLAB[panel] )
		polygon(c(0,Y),c(0,FREQ),
		col=c("#56B4E9","bisque","grey98")[panel],
		border="grey10",lwd=1)
		# legend("topright", legend = "Y: Depths of the ocean", 
		#        col = "#56B4E9", pch = 15, pt.cex = 2)
		curve(dnorm(x,mean = AVE,sd = SE), add = TRUE, lwd = 2) 
		# text(1.5*AVE,0.08,
		#      paste("means of samples of size",toString(16)),
		#      adj = c(0,0.5),
		#      col = viridis::inferno(9, end = 0.7, direction = 1)[1],
		#      cex = 0.95)
		lower.x <- qnorm(p = c(0.025), AVE, SE)
		upper.x <- qnorm(p = c(0.975), AVE, SE)
		legend("topright", legend = c("Y: Depths of the ocean",
		"Sampling distribution for \n means of samples of size 16",
		sprintf("95%% Confidence interval for \n the mean depth of the ocean:\n [%.f, %.f]",
		lower.x, upper.x)),
		col = c("#56B4E9","black",RColorBrewer::brewer.pal(9, "Set1")[5]), 
		y.intersp = 2,
		bty = "n",
		pch = c(15,NA,15), lty = c(NA,1,NA), pt.cex = 2, lwd = c(NA,2,NA))
		
		# text(AVE*1.05,  0.15*YLIM[1],toString(round(AVE,0)), adj=c(0.5,1),
		#      cex=0.85 )
		# text(0.95*AVE,0.15*YLIM[1],expression(mu), adj=c(0.5,1.25),
		#      cex=0.95 )
		# text(.99*AVE,0.07*YLIM[1],"=", adj=c(0.5,1.25),
		#      cex=0.95 )
		# arrows(x0 = AVE, x1 = AVE + SE, y0 = 0.055, length = 0.05)
		# segments(AVE + SE,0, AVE + SE, 0.06,lty="dotted")
		# #text(x = AVE*1.05, y = 0.051, '{', srt = 90, cex = 1.5, family = 'Helvetica Neue UltraLight')
		# text((AVE)*1.25,0.06,latex2exp::TeX("$\\sigma / \\sqrt{16}$"), adj=c(0.5,1.25),
		#      cex=0.95 )
		
		step <- (upper.x - lower.x) / 100
		bounds <- c(lower.x, upper.x)
		cord.x <- c(lower.x,seq(lower.x,upper.x,step),upper.x)
		cord.y <- c(0,dnorm(seq(lower.x,upper.x,step),AVE,SE),0)
		polygon(cord.x,cord.y,col=RColorBrewer::brewer.pal(9, "Set1")[5])
		text(lower.x, -0.005, round(lower.x, 0))
		text(upper.x, -0.005, round(upper.x, 0))
		segments(AVE,0, AVE, 0.10,lty="dotted")
		@
		
	}
	
\end{frame}

\frame{\frametitle{Example: Inference for a single population mean} So what
	does the CI allow us to learn about $\mu$??
	\begin{itemize}
		\setlength\itemsep{2em}
		\item It tells us that if we repeated this procedure again and again
		(collecting a sample mean, and constructing a 95\% CI), 95\% of the
		time, the CI would \textit{cover} $\mu$. \pause 
		\item That is, with 95\% probability, the \textit{procedure}
		will include the true value of $\mu$. Note that we are making \underline{a probability statement about the CI}, not about the parameter. \pause
		\item Unfortunately, \textcolor{blue}{we do not know whether the true value of $\mu$ is
			contained in the CI in the particular experiment that we have
			performed.}
	\end{itemize}
}



\section{Bootstrap}

\begin{frame}{Motivation for the Bootstrap}
	\begin{itemize}
		\setlength\itemsep{2em}
		\item The $\pm$ and \texttt{qnorm} methods to calculate a CI both require the CLT
	\end{itemize}
	
	\pause
	
	\vspace*{0.2in}
	
	\Large \textcolor{myblue}{Q: What happens if the CLT hasn't `kicked in`? Or you don't believe the CLT?} \\ \ \\
	\pause 
	\Large \textcolor{myblue}{Q: What happens if there is no formula available to calculate a CI?} \\ \ \\
	\pause 
	\Large \textcolor{red}{A: Bootstrap} \\ \ \\
\end{frame}



\begin{frame}[fragile]{Ideal world: known sampling distribution}
	
	<<fig.asp = 0.518, fig.cap = '\\scriptsize{Ideal world. Sampling distributions are obtained by drawing repeated samples from the population, computing the statistic of interest for each, and collecting (an infinite number of) those statistics as the sampling distribution}'>>=
	source("../bin/sampling_dist_figure.R")
	@
	
\end{frame}



\begin{frame}[fragile]{Reality: use the bootstrap distribution instead}
	
	
	<<fig.asp = 0.518, fig.cap = '\\scriptsize{Bootstrap world. The bootstrap distribution is obtained by drawing repeated samples from an estimate of the population, computing the statistic of interest for each, and collecting those statistics. The distribution is centered at the observed statistic ($\\bar{y}$), not the parameter ($\\mu$).}', eval = FALSE>>=
	#rm(list=ls())
	#source("bootstrap_figure.R")
	@
	
	\framedgraphiccaption{boot_diag.pdf}{\scriptsize{Bootstrap world. The bootstrap distribution is obtained by drawing repeated samples from an estimate of the population, computing the statistic of interest for each, and collecting those statistics. The distribution is centered at the observed statistic ($\bar{y}$), not the parameter ($\mu$).}}
	
\end{frame}


\begin{frame}[fragile]{Main idea: simulate your own sampling distribution}
	
	<<echo = FALSE, eval = FALSE>>=
	source("https://raw.githubusercontent.com/sahirbhatnagar/EPIB607/master/labs/
	003-ocean-depths/automate_water_task.R")
	@
	
	<<echo = c(7,8), message = FALSE, warning = FALSE, tidy = FALSE, fig.width = 7, fig.asp = 0.618>>=
	source("https://raw.githubusercontent.com/sahirbhatnagar/EPIB607/master/labs/003-ocean-depths/automate_water_task.R")
	index.n.20 <- c(2106,2107,2108,2109,2110,2111,2112,
	2113,2114,2115,2116,2117,2118,2119,
	2120,2121,2122,2123,2124,2125)
	depths.n.20 <- automate_water_task(index = index.n.20, 
	student_id = 260194225, type = "depth")
	depths.n.20$alt = round(depths.n.20$alt/100,0)
	mm <- mean(depths.n.20$alt)
	B <- 10000; N <- nrow(depths.n.20)
	R <- replicate(B, {
	dplyr::sample_n(depths.n.20, size = N, replace = TRUE) %>%
	dplyr::summarize(r = mean(alt)) %>%
	dplyr::pull(r)
	})
	CI_95 <- quantile(R, probs = c(0.025, 0.975))
	hist(R, breaks = 50, col = "#56B4E9",
	main="",
	xlab = "mean depth of the ocean (100m) from \neach bootstrap sample")
	abline(v = mm, lty =1, col = "red", lwd = 4)
	abline(v = CI_95[1], lty =2, col = "black", lwd = 4)
	abline(v = CI_95[2], lty =2, col = "black", lwd = 4)
	library(latex2exp)
	#text(mm*1.10, 1150, TeX("$\\bar{y} = 36$"))
	legend("topleft", legend = c(TeX("$\\bar{y} = 36$"),sprintf("95%% CI: [%.f, %.f]",CI_95[1], CI_95[2])), 
	lty = c(1,1), 
	col = c("red","black"), lwd = 4)
	@
	
\end{frame}

\begin{frame}{Bootstrap can be used for other statistics (e.g. $R^2$)}
	\centering
	\includegraphics[scale=0.29]{bootcorr.png}
	
	\vspace{0.1in}
	
	\tiny \href{https://www.dropbox.com/s/cxiq70zxxtyxlb5/EfronDiaconisBootstrap.pdf?dl=0}{source: Bootstrap article in Scientific American}
\end{frame}


\section{One sample mean}


\begin{frame}{$\sigma$ known vs. unknown}
	\begin{center}
		\begin{tabular}{|l|c|c|} \hline
			$\sigma$& known & unknown \\ \hline Data & $\{y_1,y_2,...,y_n\}$ &
			$\{y_1,y_2,...,y_n\}$\\
			& & \\
			Pop'n param & $\mu$ & $\mu$\\
			& & \\
			Estimator & $\overline{y} = \frac{1}{n}\sum_{i=1}^n y_i$ & $\overline{y} = \frac{1}{n}\sum_{i=1}^n y_i$ \\
			& & \\
			SD & $\sigma$ & $s = \sqrt{\frac{\sum_{i=1}^n(y_i-\overline{y})^2}{n-1}}$ \\
			& & \\
			SEM & $\sigma/\sqrt{n}$ & $s / \sqrt{n}$ \\
			& & \\
			$(1-\alpha)100$\% CI & $\overline{y} \pm z^\star_{1-\alpha/2}$(SEM) & $\overline{y} \pm t^\star_{1-\alpha/2, (n-1)}$(SEM) \\
			& & \\
			test statistic & $\frac{\overline{y}-\mu_0}{\textrm{SEM}}\sim \mathcal{N}(0,1)$ &
			$\frac{\overline{y}-\mu_0}{\textrm{SEM}}\sim t_{(n-1)}$ \\
			\hline
		\end{tabular}
	\end{center}
\end{frame}


\begin{frame}{Assumptions}
	\Wider[3em]{
		\begin{center}
			\begin{tabular}{|l|c|c|c|} \hline
				& $z$ & $t$ & Bootstrap \\ 
				\hline 
				SRS & \cmark &\cmark &	\cmark\\
				& & & \\
				Normal population & \cmark$^\star$ & \cmark$^\star$ &  \xmark\\
				& & &\\
				needs CLT &  \cmark$^\star$ & \cmark$^\star$ &  \xmark\\
				& & &\\
				$\sigma$ known  & \cmark & \xmark & \xmark\\
				& & &\\
				Sampling dist. center at & $\mu$ & $\mu$ & $\bar{y}$\\
				& & &\\
				SD & $\sigma$ & $s$ & $s$ \\
				& & &\\
				SEM & $\sigma/\sqrt{n}$ & $s / \sqrt{n}$ & SD(bootstrap statistics) \\
				\hline
			\end{tabular}
			
			\footnotetext[1]{*If population is Normal then CLT is not needed. If population is not Normal then CLT is needed.}
		\end{center}
	}
\end{frame}

\section{One sample proportion}

\begin{frame}
	\begin{itemize}
		\item Binomial calculations
		\item Nomogram, Clopper-Pearson CI
		\item Normal approximation
	\end{itemize}
\end{frame}


\section{p-values}

\begin{frame}
	\frametitle{$p$-values and statistical tests}
	
	
	%\vspace{18pt}
	\begin{defm}[$p$-value]
		A \textbf{probability concerning the observed data}, calculated under a \textbf{Null Hypothesis} assumption, i.e., assuming that the only factor operating is sampling or measurement variation. 
	\end{defm}
	
	\begin{itemize} 
		\item[\underline{Use}] To assess the evidence provided by the sample data
		in relation to a pre-specified claim or `hypothesis' concerning some parameter(s) or data-generating process. 
		\item[\underline{Basis}] As with a confidence interval, it makes use of the concept of a \textit{distribution}. 
		\item[\underline{Caution}] A $p$-value is NOT the probability that the null `hypothesis' is true
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{More about the $p$-value}
	\small
	%\begin{footnotesize}
	\begin{itemize}
		\setlength\itemsep{.3em}
		\item The $p$-value is a \textbf{probability concerning data}, \textbf{conditional on the Null Hypothesis being true}. \pause
		\item \textbf{It is not the probability that Null Hypothesis is true}, \textit{conditional on the data.} \pause
		%\item Very few MDs mix up complement of specificity (i.e. probability of a `positive'  test result when in fact patient does not have disease in question) with positive predictive value (i.e. probability that a patient who has had a `positive'  test result does  have disease in question).
		\begin{align*}
			p_{value} & = P(\textrm{this or more extreme data}| H_0) \\
			& \neq P(H_0|\textrm{this or more extreme data}).
		\end{align*}
		
		\pause 
		\item Statistical tests are often coded as statistically significant or not according to whether results are extreme or not with respect to a reference (null)  distribution.  But a test result  is just one piece of data, and needs to be considered \textit{along with  rest of evidence} before coming to a `conclusion.' \item \textbf{Likewise with statistical `tests': the $p$-value is just one more piece of \textit{evidence}, hardly enough to `conclude' anything}. 
		%\item The probability that the DNA from the blood  of a randomly selected (innocent) person would match that from blood on crime-scene glove was 
		%$p=10^{-17}$. \textit{Do not equate this} Prob[data $|$ innocent] \textit{with its transpose}:
		%writing ``data'' as shorthand for ``this or more extreme data'', we need to be aware that 
		%$$p_{value} = Prob[ \ data \  | \  H_0 ] \neq Prob[ \ H_0 \  |  \ data ].$$
	\end{itemize}
	%\end{footnotesize}
\end{frame}

\begin{frame}
	\frametitle{Close relationship between $p$-value and CI}
	\begin{center}
		\includegraphics[width=1.65in]{P-CI.pdf}
	\end{center} 
	\begin{footnotesize}
		\begin{itemize}
			\item
			(Upper graph) If upper limit of 95\% CI\textit{ just touches} null value, then
			the 2 sided $p$-value is 0.05 (or 1 sided $p$-value is 0.025). 
			\item
			(Lower graph) If upper limit \textit{excludes} null value, then
			the 2 sided $p$-value is less than 0.05 (or 1 sided $p$-value is less than 0.025). 
			\item
			(Graph not shown) If  CI \textit{includes} null value, then the 2-sided $p$-value is greater than (the conventional) 0.05, and thus observed statistic is ``not statistically significantly different'' from hypothesized null value. 
		\end{itemize}
	\end{footnotesize}
\end{frame}


\section{Power and sample size}

\begin{frame}[fragile]{Power = $1 - \beta$}
	
	\vspace*{-0.2in}
	
	\begin{defm}[Power = $1-\beta$]
		The probability that a fixed level $\alpha$ significance test will reject $H_0$ when a particular alternative value of the parameter is true is called the \textbf{power} of the test to detect the alternative. 
	\end{defm}
	
	
	\vspace*{-0.08in}
	
	\centering
	\includegraphics[scale=0.31]{HypTest3-3.pdf}
	
	
\end{frame}

\begin{frame}{Power and Sample Size: 3 questions}
	
	\begin{enumerate}
		\setlength\itemsep{1em}
		\item How much water a supplier could add to the milk before they have a 10\% , 50\%, 80\%
		chance of getting caught, i.e., of the buyer detecting the cheating ? \pause
		\item Assume a 99:1 mix of milk and water. What are the chances of detecting cheating if the buyer uses samples $n$=10, 15 or 20 rather than just 5 measurements? \pause
		\item At what $n$ does the chance of detecting cheating reach 80\%? (\textit{a commonly used, but arbitrary, criterion used in sample-size planning by investigators seeking funding for their proposed research})
	\end{enumerate}
	
\end{frame}


\begin{frame}[fragile]{If the supplier added 1\% water to the milk}
	<<echo=FALSE, eval=TRUE>>=
	source("https://raw.githubusercontent.com/sahirbhatnagar/EPIB607/master/slides/bin/plot_null_alt.R")
	n <- 5
	s <- 0.0080
	mu0 <- -0.540
	mha <- 0.99*-0.540
	cutoff <- mu0 + qnorm(0.95) * s / sqrt(n)
	power_plot(n = n, s = s,  
	mu0 = mu0, mha = mha, 
	cutoff = cutoff,
	alternative = "greater",
	xlab = "Freezing point (degrees C)")
	@
\end{frame}



\begin{frame}
	
	\begin{center}
		\includegraphics[scale=0.45]{ProbDetectingWaterInMilk.pdf} 
	\end{center}
	
	\vspace*{-0.18in}
	
	{ \footnotesize
		The probabilities in red were calculated using the formula:
		\texttt{stats::pnorm(cutoff, mean = mu.mixture, sd = SEM, lower.tail=FALSE)}
	}
\end{frame}


\begin{frame}
	\begin{center}
		\includegraphics[scale=0.5]{SampleSize1pctWaterAdded.pdf} 
	\end{center}
\end{frame}

\begin{frame}[fragile]{The balancing formula}
	<<echo=FALSE, eval=TRUE>>=
	n <- 13.6
	s <- 0.0080
	mu0 <- -0.540
	mha <- 0.99*-0.540
	cutoff <- mu0 + qnorm(0.95) * s / sqrt(n)
	power_plot(n = n, s = s,  
	mu0 = mu0, mha = mha, 
	cutoff = cutoff,
	alternative = "greater",
	xlab = "Freezing point (degrees C)")
	I = 1
	axis(2)
	text(-0.5325,600,"qnorm(0.8, \nlower.tail=FALSE)=\n-0.84",
	cex=.65,family="mono",adj=c(0,0.5),col="red")
	x = (cutoff+mha)/2
	text(x-.001, 330,
	"0.84 * SEM",col="red",
	adj=c(-0.1,0.5),cex=0.65)
	cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
	"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
	text(-0.548,100,"qnorm(0.05, \nlower.tail=FALSE)=\n 1.645",
	cex=0.65,family="mono",adj=c(0,0),col=cbPalette[6])
	text(x-.001, 130, "1.645 * SEM",col=cbPalette[6], 
	adj=c(1.1,0),cex=0.65)
	arrows(mu0,   100,
	mha,100,length=0.05,
	code=3,angle=20,lwd=1.5)
	text(mha-.0054,80, latex2exp::TeX("Delta = 1.645*SEM + 0.84*SEM"),
	adj=c(-0.1,0.5), cex=0.75)
	@
\end{frame}


\begin{frame}{What sample size needed?}
	
	\begin{itemize}
		\setlength\itemsep{1em}
		\item The `balancing formula', in SEM terms, is simply the $n$ where
		$$ 1.645 \times SEM + 0.84 \times SEM = \Delta.$$
		Replacing each of the  SEMs (assumed equal, because we assumed the variability
		is approx. the same under both scenarios) by $\sigma/\sqrt{n}$,  i.e.,
		
		$$ 1.645 \times \sigma/\sqrt{n} + 0.84 \times \sigma/\sqrt{n} = \Delta.$$
		
		and solving for $n$, one gets
		
		$$  n = (1.645 + 0.84)^2  \times \bigg\{ \frac{\sigma}{\Delta} \bigg\}^2 = 
		(1.645 + 0.84)^2  \times \bigg\{ \frac{Noise}{Signal} \bigg\}^2 .$$
	\end{itemize}
	
\end{frame}

\begin{frame}{What sample size needed? General Formula}
	
	\begin{itemize}
		\setlength\itemsep{2em}
		\item Two sided alternative:
		$$\Delta = z_{1-\alpha/2} \times SEM + z_{1-\beta} \times SEM$$
		
		\item One sided alternative:
		$$\Delta = z_{1-\alpha} \times SEM + z_{1-\beta} \times SEM$$
	\end{itemize}
	
\end{frame}



\begin{frame}[fragile]{Session Info}
	\tiny
	
	<<echo=FALSE, comment = NA, size = 'tiny'>>=
	print(sessionInfo(), locale = FALSE)
	@
	
\end{frame}

\end{document}
