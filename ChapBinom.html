<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 14 Binomial Random Variable | EPIB607</title>
<meta name="author" content="Sahir Bhatnagar and James A Hanley">
<meta name="description" content="14.1 Objectives Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 14 Binomial Random Variable | EPIB607">
<meta property="og:type" content="book">
<meta property="og:url" content="https://sahirbhatnagar.com/EPIB607/ChapBinom.html">
<meta property="og:description" content="14.1 Objectives Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 14 Binomial Random Variable | EPIB607">
<meta name="twitter:description" content="14.1 Objectives Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="libs/datatables-binding-0.20/datatables.js"></script><link href="libs/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="libs/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="libs/dt-core-1.11.3/js/jquery.dataTables.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet">
<script src="libs/nouislider-7.0.10/jquery.nouislider.min.js"></script><link href="libs/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet">
<script src="libs/selectize-0.12.0/selectize.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="templates/bs4_style.css">
<link rel="stylesheet" href="templates/ims-style.css">
<link rel="stylesheet" href="templates/corrections.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">EPIB607</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="schedule.html">Schedule</a></li>
<li><a class="" href="syllabus.html">Syllabus</a></li>
<li class="book-part">Descriptive Statistics</li>
<li><a class="" href="introdata.html"><span class="header-section-number">1</span> Introduction to Data</a></li>
<li><a class="" href="aesthetic-mapping.html"><span class="header-section-number">2</span> Visualizing data: Mapping data onto aesthetics</a></li>
<li><a class="" href="coordinate-systems-axes.html"><span class="header-section-number">3</span> Coordinate systems and axes</a></li>
<li><a class="" href="ggplot2-package-for-plots.html"><span class="header-section-number">4</span> ggplot2 package for plots</a></li>
<li><a class="" href="color-basics.html"><span class="header-section-number">5</span> Color scales</a></li>
<li class="book-part">Sampling Distributions</li>
<li><a class="" href="paras.html"><span class="header-section-number">6</span> Statistical Parameters</a></li>
<li><a class="" href="ChapProbability.html"><span class="header-section-number">7</span> Probability</a></li>
<li><a class="" href="randomVariables.html"><span class="header-section-number">8</span> Random Variables</a></li>
<li><a class="" href="ChapNormal.html"><span class="header-section-number">9</span> The Normal Random Variable</a></li>
<li><a class="" href="ChapSampDist.html"><span class="header-section-number">10</span> The Sampling Distribution</a></li>
<li><a class="" href="CI.html"><span class="header-section-number">11</span> Parameter Intervals</a></li>
<li><a class="" href="foundations-bootstrapping.html"><span class="header-section-number">12</span> Confidence intervals with bootstrapping</a></li>
<li class="book-part">One Sample Inference</li>
<li><a class="" href="inference-one-mean.html"><span class="header-section-number">13</span> Inference for a single mean</a></li>
<li><a class="active" href="ChapBinom.html"><span class="header-section-number">14</span> Binomial Random Variable</a></li>
<li><a class="" href="inference-one-prop.html"><span class="header-section-number">15</span> Inference for a single proportion</a></li>
<li class="book-part">Computing</li>
<li><a class="" href="install.html"><span class="header-section-number">16</span> Installing R and RStudio</a></li>
<li><a class="" href="basics.html"><span class="header-section-number">17</span> Basics of R and Rstudio</a></li>
<li><a class="" href="projects.html"><span class="header-section-number">18</span> RStudio Projects</a></li>
<li><a class="" href="functionsHOP.html"><span class="header-section-number">19</span> Functions</a></li>
<li><a class="" href="packages.html"><span class="header-section-number">20</span> Packages</a></li>
<li><a class="" href="import.html"><span class="header-section-number">21</span> Data Import and Export</a></li>
<li><a class="" href="transition-to-r-from-excel-stata-sas.html"><span class="header-section-number">22</span> Transition to R from Excel, Stata, SAS</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ChapBinom" class="section level1">
<h1>
<span class="header-section-number">14</span> Binomial Random Variable<a class="anchor" aria-label="anchor" href="#ChapBinom"><i class="fas fa-link"></i></a>
</h1>

<div id="objectives-2" class="section level2">
<h2>
<span class="header-section-number">14.1</span> Objectives<a class="anchor" aria-label="anchor" href="#objectives-2"><i class="fas fa-link"></i></a>
</h2>
<p>Distributions of individual values take their shapes and spreads from the features of the setting, and thus do not follow any general laws. The shapes and the spreads of distributions of statistical summaries and parameter estimates made from aggregates of individual observations tend to be more regular and more predictable, and thus more law-abiding.</p>
<p>So, the <strong>specific objectives</strong> in this chapter are to truly understand</p>
<ul>
<li><p>the distinction between a natural and investigator-made distributions, and between observable and conceptual ones.</p></li>
<li><p>why we should not automatically associate certain distributions with certain types of random variables</p></li>
<li><p>why we need to understand the pre-requisites for random variables following the distributions they do</p></li>
<li><p>why we rely so much on the Normal distribution, and why it is so ‘Central’ to statistical inference concerning parameters.</p></li>
<li><p>why the pre-occupation with checking ‘Normality’ (Gaussian-ness) is misguided</p></li>
<li><p>why Normality is not even relevant when the ‘variable’ is not ‘random’, and appears on the right hand side of a regression model.</p></li>
<li><p>the few contexts where shape does matter</p></li>
</ul>
<p>To get a full list of the named distributions available in <code>R</code> you can use the following command: <code><a href="https://rdrr.io/r/stats/Distributions.html">help(Distributions)</a></code></p>
</div>
<div id="bernoulli" class="section level2">
<h2>
<span class="header-section-number">14.2</span> Bernoulli<a class="anchor" aria-label="anchor" href="#bernoulli"><i class="fas fa-link"></i></a>
</h2>
<p>The simplest random variable is one that take just 2 possible values, such as YES/NO, MALE/FEMALE, 0/1, ON/OFF, POSITIVE/NEGATIVE, PRESENT/ABSENT, EXISTS/DOES NOT, etc.</p>
<p>This random variable <span class="math inline">\(Y\)</span> is governed by just one parameter, namely the probability, <span class="math inline">\(\pi\)</span>, that it takes on the YES (or ‘1’) value. Of course you can reverse the scale, and speak about the probability of a NO (or ‘0’) result.</p>
<p>It is too bad that when <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Wikipedia</a>, which has a unified way of showing the main features of statistical distributions, does not follow its own principles and show a graph of various versions of a Bernoulli distribution. So here is such a graph.</p>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-1-1.png" alt="Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis." width="864"><p class="caption">
Figure 11.2: Various Bernoulli random variables/distributions. We continue our convention of using the letter Y (instead of X) as the generic name for a random variable. Moreover, in keeping with this view, all of the selected Bernoulli distributions are plotted with their 2 possible values shown on the vertical axis.
</p>
</div>
<p>Please, when reading the Wikipedia entry, replace all instances of <span class="math inline">\(X\)</span> and <span class="math inline">\(x\)</span> by <span class="math inline">\(Y\)</span> and <span class="math inline">\(y\)</span>. Note also that we will use <span class="math inline">\(\pi\)</span> where Wikipedia, and some textbooks, use <span class="math inline">\(p\)</span>. As much as we can, we use Greek letters for parameters and Roman letters for their empirical (sample) counterparts. Also, to be consistent, if the random variable itslef is called <span class="math inline">\(Y\)</span>, then it makes sense to use <span class="math inline">\(y\)</span> as the possible relaizations of it, rather than the illogical <span class="math inline">\(k\)</span> that Wikipedia uses.]</p>
<p>In the sidebar, Wikipedia shows the probability mass function (<em>pmf</em>, the probabilities that go with the possible <span class="math inline">\(Y\)</span> values) in two separate rows, but in the text the <em>pmf</em> is also shown more concisely, as (in our notation)</p>
<p><span class="math display">\[f(y) = \pi^y (1-\pi)^{1-y}, \ \ y = 0, 1.\]</span>
If we wish to align with how the <code>R</code> software names features of distributions, we might want to switch from <span class="math inline">\(f\)</span> to <span class="math inline">\(d\)</span>. <code>R</code> uses <span class="math inline">\(d\)</span> because it harmonizes with the probability <span class="math inline">\(d\)</span>ensity function (<em>pdf</em>) notation that its uses for random variables on an interval scale, even though some statistical ‘purists’ see that as mixing terminology: they use the term probablility mass function for discrete random variables, and probablity density function for ones on an interval scale.</p>
<p><span class="math display">\[d_{Bernoulli}(y) = \pi^y (1-\pi)^{1-y}.\]</span></p>
<p>Sadly, Bernoulli does not get its own entry in <code>R</code>’s list of named distributions, presumably because it is a special case of a binomial distribution, one where <span class="math inline">\(n\)</span> = 1.
So we have to call <code>dbinom(x,size,prob)</code> to get the
density (probability mass) function of the binomial distribution with parameters <code>size</code> (<span class="math inline">\(n\)</span>) and <code>prob</code> (<span class="math inline">\(\pi\)</span>), and set <span class="math inline">\(n\)</span> to 1.</p>
<p>The 3 arguments to <code>dbinom(x,size,prob)</code> are:</p>
<ul>
<li>
<code>x</code>: a vector of quantiles (here just 0 or/and 1),</li>
<li>
<code>size</code>: the number of ‘trials’ (our ‘<span class="math inline">\(n\)</span>’, so 1 for Bernoulli),
and</li>
<li>
<code>prob</code>: the probability of ‘success’ on each ‘trial’. We think of it as the probability that a realization of $Y4, i.e, <span class="math inline">\(y\)</span> will equal 1, or as <span class="math inline">\(\pi.\)</span>)</li>
</ul>
<p>Thus, <code>dbinom(x=0,size=1,prob=0.3)</code> yields 0.7, while <code>dbinom(x=1,size=1,prob=0.3)</code> yields 0.3 and <code>dbinom(x=c(0,1),size=1,prob=0.3)</code> yields the vector 0.7, 0.3.</p>
<p>Incidentally, please do not adopt the convention that <span class="math inline">\(x\)</span> (or our <span class="math inline">\(y\)</span>) is the number of ‘successes’ in <span class="math inline">\(n\)</span> trials. It is the number of ‘positives’ in a sample of <span class="math inline">\(n\)</span> independent draws from a population in which a proportion <span class="math inline">\(\pi\)</span> are positive.</p>
<div id="expectation-and-variance-of-a-bernoulli-random-variable" class="section level3">
<h3>
<span class="header-section-number">14.2.1</span> Expectation and Variance of a Bernoulli random variable<a class="anchor" aria-label="anchor" href="#expectation-and-variance-of-a-bernoulli-random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>Shortening <span class="math inline">\(Prob(Y=y)\)</span> to <span class="math inline">\(P_y\)</span>, we have</p>
<ul>
<li>From first principles,
<span class="math display">\[E[Y] = 0 \times P_0 + 1 \times P_1 = 0 \times (1-\pi) + 1 \times \pi = \pi,\]</span>
while
<span class="math display">\[V[Y] = (0-\pi)^2 \times P_0 + (1-\pi)^2 \times P_1  = \pi^2(1-\pi) + (1-\pi)^2\pi =  \underline{\pi(1-\pi)}.\]</span>
</li>
</ul>
<p>This functional form for the (‘unit’) variance is not entirely surprising: it is obvious from the selected distributions whon that the most concentrated Bernoulli distributions are the ones where the proportion <span class="math inline">\((\pi)\)</span> of Y = 1 values is either close to 1 or to zero, and that the most spread out Bernoulli distributions are the ones where <span class="math inline">\(\pi\)</span> is close to 1/2. And, and a function of <span class="math inline">\(\pi\)</span>, the Variance must be symmetric about <span class="math inline">\(\pi\)</span> = 1/2.</p>
<p>The fact that the greatest uncertainty (‘entropy’, lack of order or predictability) is when <span class="math inline">\(\pi\)</span> = 1/2 is one of the factors that makes sports contests more engaging when teams or players are well matched. Later, when we come to study what influences the imprecision of sample surveys, we will see that for a given sample size, the imprecision is largest when <span class="math inline">\(\pi\)</span> is closer to 1/2.</p>
<p><strong>Why focus on the <em>variance</em> of a Bernoulli random variable?</strong> because, later, when we use the more intesting binomial distribution, we can call on first prionciples to recall what its expectation and variance are. A Binomial random variable is the sum of <span class="math inline">\(n\)</span> independently distributed Bernoulli random variables, all with the same expectation <span class="math inline">\(\pi\)</span> and <strong>unit variance</strong> <span class="math inline">\(\pi(1-\pi).\)</span> Thus its expectation (<span class="math inline">\(E\)</span>) and variance (<span class="math inline">\(V\)</span>) are the <strong>sums of these ‘unit’ versions</strong>, i.e., <span class="math inline">\(E[binom.sum] = n \times \pi\)</span> and <span class="math inline">\(V[binom.sum] = n \times \pi(1-\pi).\)</span> Moreover, again from first principles, we can deduce that if instead of a sample <em>sum</em>, we are interested in a sample <em>mean</em> (here the <em>mean</em> of the 0’s and 1’s is the sample <em>proportion</em>), its expected value is
<span class="math display">\[\boxed{\boxed{E[binom.prop'n] = \frac{n \pi}{n} = \pi; \   V[.] = \frac{n  \pi(1-\pi))}{n^2} = \frac{\pi(1-\pi)}{n}; \ SD[.] = \frac{\sqrt{\pi(1-\pi)}}{ \sqrt{n}} = \frac{\sigma_{0,1}}{\sqrt{n}} } }  \]</span></p>
<p>Note here the generic way we write the SD of the sampling distribution of a sample proportion, in the same way that we write the SD of the sampling distribution of a sample mean, as <span class="math inline">\(\sigma_u/\sqrt{n},\)</span> where <span class="math inline">\(\sigma_u\)</span> is the ‘unit’ SD, the standard deviation of the values of <em>individuals</em>. The individual values in the case of a Bernoulli randomn variable are just 0s and 1s, and their SD is <span class="math inline">\(\sqrt{\pi(1-\pi)}.\)</span> We call this SD the SD of the 0’1 and 1’s, or <span class="math inline">\(\sigma_{0,1}\)</span> for short.</p>
<p>Notice how, even though it might look nicer and simpler to compute, and involves just 1 square root calculation, we did not write the SD of a binomial proportion as<br><span class="math display">\[SD[binom.proportion] = \sqrt{\frac{\pi(1-\pi)}{ n} }.\]</span>
We choose instead to use the <span class="math inline">\(\sigma/\sqrt{n}\)</span> version, to show that it has the same <em>form</em> as the SD for the sampling distribution of a sample mean. Now that we no longer need to savw keystrokes on a hand caloculator, we should move away from computational forms and focus instead on the intuitive form. Sadly, many textbooks re-use the same concept in disjoint chapters without telling readers they are cases of the same SD formula.</p>
<p>There is a lot to be gained by thinking of proportions as means, but where the <span class="math inline">\(Y\)</span> values are just 0’s and 1’s. You can use the <code>R code</code> below to simulate a very large number of 0’s and 1’s, and calculate their variance. The <code>sd</code> function in <code>R</code> doesn’t know or care that the values you supply it are limited to just 0s and 1s, or spread along an interval. Better still don’t use the <code>rbinom</code> function; instead use the <code>sample</code> function, with replacement.</p>
<div class="sourceCode" id="cb108"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="va">n</span> <span class="op">=</span> <span class="fl">750</span>

<span class="va">zeros.and.ones</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, <span class="va">n</span> , 
   prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.8</span><span class="op">)</span>,replace<span class="op">=</span><span class="cn">TRUE</span> <span class="op">)</span>

<span class="va">m</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">zeros.and.ones</span>,<span class="va">n</span><span class="op">/</span><span class="fl">75</span>,<span class="fl">75</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/noquote.html">noquote</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">m</span>,<span class="fl">1</span>,<span class="va">paste</span>,collapse<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;  [1] 100111011111100011011111111011111010111011101111111111101001111111111101101</span>
<span class="co">#&gt;  [2] 011111111111111111111111111101111111011100011011111111111111111101111111110</span>
<span class="co">#&gt;  [3] 111110111111111110110110111011111011111010111111111110111110111111101101110</span>
<span class="co">#&gt;  [4] 111000111011111111110111111111111101011111111000010111010111100110111100111</span>
<span class="co">#&gt;  [5] 111111011111001111111011101111101011011011011110111111111111111111101110011</span>
<span class="co">#&gt;  [6] 011101101001011100111110101111111111011010111110101000101011111101111011111</span>
<span class="co">#&gt;  [7] 111001001111101111110111111110001101111111110111111111001111100111011010111</span>
<span class="co">#&gt;  [8] 111111111101101111111011111011011111101111111111110111111111101101011111111</span>
<span class="co">#&gt;  [9] 111111111010110111110010110011110101111110110011111111011011111111110111101</span>
<span class="co">#&gt; [10] 011111100111111011101110011111101111111111101110111111111110011111111111111</span>

<span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">zeros.and.ones</span><span class="op">)</span><span class="op">/</span><span class="va">n</span>
<span class="co">#&gt; [1] 0.7986667</span>
<span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">zeros.and.ones</span><span class="op">)</span>,<span class="fl">4</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.4013</span></code></pre></div>
<p>Try the above code with a larger <span class="math inline">\(n\)</span> and a different <span class="math inline">\(\pi\)</span> and convince yourself that the variance (and thus the SD) of the individual 0 and 1 values (a) have nothing to do with how many there are and everything to do with what proportion of them are of each type and (b) are larger when the proportions are close to each other, and smaller when they are not.</p>
<div class="rmdnote" name="Could we get by without studying the Binomial Distribution?">
<p>The answer is ‘for most applications, yes.’ The reason is that in in most cases, we are able to use a Gaussian (Normal) approximation to the binomial distribution. Thus, all we need are its expectation and variance (standard deviation): we don’t need the <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code> probability mass function, or the <code><a href="https://rdrr.io/r/stats/Binomial.html">pbinom()</a></code> that gives the cumulative distribution function and thus the tail areas, or the <code><a href="https://rdrr.io/r/stats/Binomial.html">qbinom()</a></code> function that gives the quantiles. But sometimes we deal with situations where the binomial distributions are not symmetric and close-enough-to-Gaussian.</p>
</div>
<p>Below we recount how, in 1738, almost 4 decades before Gauss was born, when summing the probabilities of a binomial distribution with a large <span class="math inline">\(n\)</span>, <a href="https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem">deMoivre</a> effectively used the as-yet unrecognized ‘Gaussian’ distribution as a very accurate approximation. Without calling it this, he relied on the standard deviation of the binomial distribution.</p>
</div>
</div>
<div id="binomial" class="section level2">
<h2>
<span class="header-section-number">14.3</span> Binomial<a class="anchor" aria-label="anchor" href="#binomial"><i class="fas fa-link"></i></a>
</h2>
<p><strong>The Binomial Distribution is a model for the (sampling) variability of a proportion or count in a randomly selected sample</strong></p>
<p><strong>The Binomial Distribution: what it is</strong></p>
<ul>
<li><p>The <span class="math inline">\(n+1\)</span> probabilities <span class="math inline">\(p_{0}, p_{1}, ..., p_{y}, ..., p_{n}\)</span> of observing <span class="math inline">\(y\)</span> = <span class="math inline">\(0, 1, 2, \dots , n\)</span> ‘positives’ in <span class="math inline">\(n\)</span> independent realizations of a Bernoulli random variable <span class="math inline">\(Y\)</span>with probability, <span class="math inline">\(\pi,\)</span> that Y=1, and (1-<span class="math inline">\(\pi\)</span>) that it is 0. The number is the sum of <span class="math inline">\(n\)</span> independen Bernoulli random variables with the same probability, such as in s.r.s of <span class="math inline">\(n\)</span> individuals.</p></li>
<li><p>Each of the <span class="math inline">\(n\)</span> observed elements is binary (0 or 1)</p></li>
<li><p>There are <span class="math inline">\(2^{n}\)</span> possible <em>sequences</em> … but only <span class="math inline">\(n+1\)</span> possible <em>values</em>, i.e. <span class="math inline">\(0/n,\;1/n,\;\dots ,\;n/n\)</span> can think of <span class="math inline">\(y\)</span> as sum of <span class="math inline">\(n\)</span> Bernoulli r. v.’s. [Later on, in ptractive, we will work in the same scale as parameter. i.e., (0,1). not the (0,n) ‘count’ scale.]</p></li>
<li>
<p>Apart from <span class="math inline">\(n\)</span>, the probabilities <span class="math inline">\(p_{0}\)</span> to <span class="math inline">\(p_{n}\)</span> depend on only 1 parameter:</p>
<ul>
<li>the probability that a selected individual will be ‘positive’ i.e., have the trait of interest</li>
<li>the proportion of ‘positive’ individuals in the sampled population</li>
</ul>
</li>
<li>
<p>Usually we denote this (un-knowable) proportion by <span class="math inline">\(\pi\)</span> (or sometimes by the more generic <span class="math inline">\(\theta\)</span>)</p>
<ul>
<li>Textbooks are not consistent (see below); we try to use Greek letters for parameters,</li>
<li>Note Miettinen’s use of UPPER-CASE letters, [e.g. <span class="math inline">\(P\)</span>, <span class="math inline">\(M\)</span>] for <em>PARAMETERS</em> and lower-case letters [e.g., <span class="math inline">\(p\)</span>, <span class="math inline">\(m\)</span>] for <em>statistics</em> (<em>estimates} of parameters</em>).</li>
</ul>
</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="left">Author(s)</th>
<th align="center">PARAMETER</th>
<th align="center">Statistic</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Clayton and Hills</td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center"><span class="math inline">\(p = D/n\)</span></td>
</tr>
<tr class="even">
<td align="left">Moore and McCabe, Baldi and Moore</td>
<td align="center"><span class="math inline">\(p\)</span></td>
<td align="center"><span class="math inline">\(\hat{p} = y/n\)</span></td>
</tr>
<tr class="odd">
<td align="left">Miettinen</td>
<td align="center"><span class="math inline">\(P\)</span></td>
<td align="center"><span class="math inline">\(p = y/n\)</span></td>
</tr>
<tr class="even">
<td align="left">This book</td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center"><span class="math inline">\(p = D/n\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li>Shorthand: <span class="math inline">\(Y \sim Binomial(n, \pi)\)</span> or <span class="math inline">\(y \sim Binomial(n, \pi)\)</span>
</li>
</ul>
<p><strong>How it arises</strong></p>
<ul>
<li>Sample Surveys</li>
<li>Clinical Trials</li>
<li>Pilot studies</li>
<li>Genetics</li>
<li>Epidemiology</li>
</ul>
<p><strong>Uses</strong></p>
<ul>
<li><p>to make inferences about <span class="math inline">\(\pi\)</span> from observed proportion <span class="math inline">\(p = y/n\)</span>.</p></li>
<li>
<p>to make inferences in more complex situations, e.g. …</p>
<ul>
<li><p>Prevalence Difference: <span class="math inline">\(\pi_{index.category}\)</span> - <span class="math inline">\(\pi_{reference.category}\)</span></p></li>
<li><p>Risk Difference (RD): <span class="math inline">\(\pi_{index.category}\)</span> - <span class="math inline">\(\pi_{reference.category}\)</span></p></li>
<li><p>Risk Ratio, or its synonym Relative Risk (RR): <span class="math inline">\(\frac{\pi_{index.category}}{\pi_{reference.category}}\)</span></p></li>
<li><p>Odds Ratio (OR): <span class="math inline">\(\frac{\pi_{index.category}/(1-\pi_{index.category})}{  \pi_{reference.category}/(1-\pi_{reference.category}) }\)</span></p></li>
<li><p>Trend in several <span class="math inline">\(\pi\)</span>’s</p></li>
</ul>
</li>
</ul>
<p><strong>Requirements for <span class="math inline">\(Y\)</span> to have a Binomial<span class="math inline">\((n, \pi)\)</span> distribution</strong></p>
<ul>
<li><p>Each element in the ‘population’ is 0 or 1, but we are only interested in estimating the proportion (<span class="math inline">\(\pi\)</span>) of 1’s; we are not interested in individuals.</p></li>
<li><p>Fixed sample size <span class="math inline">\(n\)</span>.</p></li>
<li><p>Elements selected at random and independent of each other; each element in population has the same probability of being sampled: i.e., we have <span class="math inline">\(n\)</span> independent Bernoulli random variables with the same expectation (statisticians say ‘<em>i.i.d</em>’ or ‘<em>independent and identically distributed</em>’).</p></li>
<li><p>It helps to distinguish the population values, say <span class="math inline">\(Y_1\)</span> to <span class="math inline">\(Y_N\)</span>, from the <span class="math inline">\(n\)</span> sampled values <span class="math inline">\(y_1\)</span> to <span class="math inline">\(y_n\)</span>.
Denote by <span class="math inline">\(y_i\)</span> the value of the <span class="math inline">\(i\)</span>-th sampled element. Prob[<span class="math inline">\(y_i\)</span> = 1] is constant (it is <span class="math inline">\(\pi\)</span>) across <span class="math inline">\(i\)</span>.
In the <a href="http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Mean-Quantile/inside_outside.pdf">What proportion of our time do we spend indoors?</a> example, it is the <strong>random/blind sampling</strong> of the temporal and spatial patterns of 0s and 1s that <strong>makes <span class="math inline">\(y_1\)</span> to <span class="math inline">\(y_n\)</span> independent of each other</strong>. The <span class="math inline">\(Y\)</span>’s, the elements in the population can be related to each other [e.g. there can be a peculiar spatial distribution of persons] but if elements are chosen at random, the chance that the value of the <span class="math inline">\(i\)</span>-th element chosen is a 1 cannot depend on the value of <span class="math inline">\(y_{i−1}\)</span> or any other <span class="math inline">\(y\)</span>: the sampling is ‘blind’ to the spatial location of the 1’s and 0s.</p></li>
</ul>
<div id="binomial-probabilities-illustrated-using-a-binomial-tree" class="section level3">
<h3>
<span class="header-section-number">14.3.1</span> Binomial probabilities, illustrated using a Binomial Tree<a class="anchor" aria-label="anchor" href="#binomial-probabilities-illustrated-using-a-binomial-tree"><i class="fas fa-link"></i></a>
</h3>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-3-1.png" alt="From 5 (independent and identically distributed) Bernoulli observations to Binomial(n=5), with the Bernoulli probability left unspecified. There are 2 to the power n possible (distinct) sequences of 0's and 1's, each with its probability. We are not interested in these 2 to the power n probabilities, but in the probability that the sample  contains y 1's and (n-y) 0's. There are only (n+1) possibilities for y, namely 0 to n. Fortunately, each of the n.choose.y sequences that lead to the same sum or count y, has the same probability. So we group the 2.to.power.n sequences into (n+1) sets, according to the sum or count. Each sequence in the set with  y 1's and (n-y) 0's has the same probability, namely  the prob.to.the.power.y times (1-prob).to.the.power.(n-y). Thus, in lieu of adding all such probabilities, we simply multiply this  probability by the number, n.choose-y -- shown in black -- of unique sequences in the set. Check: the frequencies in black add to 2.to.power.n. Nowadays, the (n+1) probabilities are easily obtained by supplying a value for the 'prob' argument in the R function dbinom(), instead of  computing the binomial coefficient n.choose-y by hand." width="672"><p class="caption">
Figure 14.1: From 5 (independent and identically distributed) Bernoulli observations to Binomial(n=5), with the Bernoulli probability left unspecified. There are 2 to the power n possible (distinct) sequences of 0’s and 1’s, each with its probability. We are not interested in these 2 to the power n probabilities, but in the probability that the sample contains y 1’s and (n-y) 0’s. There are only (n+1) possibilities for y, namely 0 to n. Fortunately, each of the n.choose.y sequences that lead to the same sum or count y, has the same probability. So we group the 2.to.power.n sequences into (n+1) sets, according to the sum or count. Each sequence in the set with y 1’s and (n-y) 0’s has the same probability, namely the prob.to.the.power.y times (1-prob).to.the.power.(n-y). Thus, in lieu of adding all such probabilities, we simply multiply this probability by the number, n.choose-y – shown in black – of unique sequences in the set. Check: the frequencies in black add to 2.to.power.n. Nowadays, the (n+1) probabilities are easily obtained by supplying a value for the ‘prob’ argument in the R function dbinom(), instead of computing the binomial coefficient n.choose-y by hand.
</p>
</div>
<p>If you rotate the binomial tree to the right by 90 degrees, and use your imagination, you can see how it resembles the <a href="https://en.wikipedia.org/wiki/Quincunx">quincunx</a> constructed by <a href="https://en.wikipedia.org/wiki/Bean_machine">Francis Galton</a>. He used it to show how the Central Linit Theorem, applied to the sum of several ‘Bernoulli deflections to the right and left’, makes a Binomial distribution approach a Gaussian one. Several <a href="https://en.wikipedia.org/wiki/Bean_machine#Games">games</a> and game shows are built on this pinball machine, for example, <a href="https://fivethirtyeight.com/features/what-if-god-were-a-giant-game-of-plinko/">Plinko</a> and, more recently, <a href="https://www.nbc.com/the-wall?nbc=1">The Wall</a>.
Galton’s quincunx has its own cottage industry, and versions of it often displayed in Science Museums. The present authors inherited a low tech version of the <a href="http://www.galtonboard.com">Galton Board</a>, where the ‘shot’ are turnip seeds, from former McGill <a href="https://www.mcgill.ca/medicine/staff-resources/inmemoriam/2003">Professor – and early teacher of course 607 – FDK Liddell</a>.</p>
</div>
<div id="calculating-binomial-probabilities" class="section level3">
<h3>
<span class="header-section-number">14.3.2</span> Calculating Binomial probabilities<a class="anchor" aria-label="anchor" href="#calculating-binomial-probabilities"><i class="fas fa-link"></i></a>
</h3>
<p><em>Exactly</em></p>
<ul>
<li>
<p>probability mass function (p.m.f.) :</p>
<ul>
<li><p>formula: <span class="math inline">\(Prob[y] = \  ^n C _y \ \pi^y \  (1 − \pi)^{n−y}\)</span>.</p></li>
<li><p>recursively: <span class="math inline">\(Prob[0] = (1−\pi)^n\)</span>;    <span class="math inline">\(Prob[y] = \frac{n−y+1}{y} \times \frac{\pi}{1-\pi} \times Prob[y−1]\)</span>.</p></li>
</ul>
</li>
<li>
<p>Statistical Packages:</p>
<ul>
<li><p>R functions <code><a href="https://rdrr.io/r/stats/Binomial.html">dbinom()</a></code>, <code><a href="https://rdrr.io/r/stats/Binomial.html">pbinom()</a></code>, <code><a href="https://rdrr.io/r/stats/Binomial.html">qbinom()</a></code><br>
probability mass, distribution/cdf, and quantile functions.</p></li>
<li><p>Stata function <code>Binomial(n,k,p)</code></p></li>
<li><p>SAS <code>PROBBNML(p, n, y)</code> function</p></li>
</ul>
</li>
<li><p>Spreadsheet — Excel function <code>BINOMDIST(y, n, π, cumulative)</code></p></li>
<li><p>Tables: CRC; Fisher and Yates; Biometrika Tables; Documenta Geigy</p></li>
</ul>
<p><em>Using an approximation</em></p>
<ul>
<li><p>Poisson Distribution (<span class="math inline">\(n\)</span> large; small <span class="math inline">\(\pi\)</span>)</p></li>
<li><p><strong>Normal (Gaussian) Distribution</strong> (<span class="math inline">\(n\)</span> large or midrange <span class="math inline">\(\pi\)</span>, so that the expected value, <span class="math inline">\(n \times \pi\)</span>, is sufficiently far ‘in from’ the ‘edges’ of the scale, i.e., sufficiently far in from 0 and from <span class="math inline">\(n\)</span>, so that a Gaussian distribution doesn’t flow past one of the edges. The Normal approximation is good for when you don’t have access to software or Tables, e.g, on a plane, or when the internet is down, or the battery on your phone or laptop had run out, or it takes too long to boot up Windows!).<br>
To use the Normal approximatiom, be aware of the <strong>scale you are working in</strong>, .e.g., if say <span class="math inline">\(n = 10\)</span>, whether the summary is a <strong>count</strong> or a <strong>proportion</strong> or a <strong>percentage</strong>.</p></li>
</ul>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="right"></th>
<th align="left">r.v.</th>
<th align="right">e.g.</th>
<th align="center">E</th>
<th align="center">S.D.</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">
<strong>count</strong>:</td>
<td align="left"><span class="math inline">\(y\)</span></td>
<td align="right">2</td>
<td align="center"><span class="math inline">\(n \times \pi\)</span></td>
<td align="center"><span class="math inline">\(\{n \times \pi \times (1-\pi) \}^{1/2}\)</span></td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center">i.e., <span class="math inline">\(n^{1/2} \times \sigma_{Bernoulli}\)</span>
</td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="right">
<strong>proportion</strong>:</td>
<td align="left"><span class="math inline">\(p=y/n\)</span></td>
<td align="right">0.2</td>
<td align="center"><span class="math inline">\(\pi\)</span></td>
<td align="center"><span class="math inline">\(\{\pi \times (1-\pi) / n \}^{1/2}\)</span></td>
</tr>
<tr class="odd">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center">i.e., <span class="math inline">\(\sigma_{Bernoulli} / n^{1/2}\)</span>
</td>
</tr>
<tr class="even">
<td align="right"></td>
<td align="left"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="right">
<strong>percentage</strong>:</td>
<td align="left"><span class="math inline">\(100p\%\)</span></td>
<td align="right">20%</td>
<td align="center"><span class="math inline">\(100 \times \pi\)</span></td>
<td align="center"><span class="math inline">\(100 \times SD[p]\)</span></td>
</tr>
</tbody>
</table></div>
<p>The first person to suggest an approximation, using what we now call the ‘Normal’ or ‘Gaussian’ of ‘Laplace-Gaussian’ distribution, was
<a href="http://www.biostat.mcgill.ca/hanley/statbook/TheDoctrineOfChancesAnnotated.pdf">deMoivre, in 1738</a>. There is a debate among historians as to whether this marks the first description of the Normal distribution: the piece does not explicitly point to the probability density function <span class="math inline">\(\frac{1}{\sigma \sqrt{2 \pi}} \times exp[-z^2/2\sigma^2],\)</span> but it does highlight the role of the quantity <span class="math inline">\((1/2) \times \sqrt{n}\)</span>, the standard deviation of the sum of <span class="math inline">\(n\)</span> independent Bernoulli random variables, each with expectation 1/2 and thus a ‘unit’ standard deviation of 1/2, and also the SD quantity <span class="math inline">\(\sqrt{\pi(1-\pi)}\)</span> <span class="math inline">\(\times\)</span> <span class="math inline">\(\sqrt{n}\)</span> in the more general case. DeMoivre arrived at the familiar ‘68-95-99.7 rule’ : the percentages of a normal distribution that lie within 1, 2 and 3 SD’s of its mean.</p>
<p><strong>Factors that modulate the shapes of Binomial distributions</strong></p>
<ul>
<li><p>size of <span class="math inline">\(n\)</span>: the larger the n, the more symmetric</p></li>
<li><p>value of <span class="math inline">\(\pi\)</span>: the closer to 1/2, the more symmetric</p></li>
</ul>
<p>In these small-<span class="math inline">\(n\)</span> contexts, only those distribtions where <span class="math inline">\(\pi\)</span> is close to 0.5 are reasonably symmetric.</p>
<p>In larger-<span class="math inline">\(n\)</span> contexts (see below), as long as there is ‘room’ for them to be, binomial distribtions where the expected value <span class="math inline">\(E = n \times \pi\)</span> is at least 5-10 ‘in from the edges’ (i.e. to the right of 0, or the left of <span class="math inline">\(n\)</span>, are reasonably symmetric.</p>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-5-1.png" alt="Binomial random variables/distributions, where n = 5, and the Bernoulli expectation (probability) is smaller (left panels) or larger (right panels)." width="672"><p class="caption">
Figure 14.2: Binomial random variables/distributions, where n = 5, and the Bernoulli expectation (probability) is smaller (left panels) or larger (right panels).
</p>
</div>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-6-1.png" alt="Various Binomial random variables/distributions, where n = 20. The dotted horizontal lines in light blue are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units from the (0,n) boundaries are shown in blue." width="672"><p class="caption">
Figure 8.5: Various Binomial random variables/distributions, where n = 20. The dotted horizontal lines in light blue are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units from the (0,n) boundaries are shown in blue.
</p>
</div>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-7-1.png" alt="Various Binomial random variables/distributions, where n = 50. The blue dotted lines are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units in from the (0,n) boundaries are shown in blue" width="672"><p class="caption">
Figure 8.6: Various Binomial random variables/distributions, where n = 50. The blue dotted lines are 5 and 10 units in from the (0,n) boundaries. The distributions where the expected value E or mean, mu ( = n * Bernoulli Probability) is at least 5 units in from the (0,n) boundaries are shown in blue
</p>
</div>
<p>Back when binomial computations were difficult, and the normal approximation was not accurate, there was another approximation that saved labour, and in particular, avoided having to deal with the very large numbers involved in the <span class="math inline">\(^nC_y\)</span> binomial coefficients (even if built into a hand calculator, these can be problematic when <span class="math inline">\(n\)</span> is large).</p>
<p><a href="http://www.biostat.mcgill.ca/hanley/statbook/Poisson1837Excerpts.pdf">Poisson</a>, in 1837, having shown how to use the Normal distribution for binomial (and the closely-related negative binomial) calculations, devoted 1 small section (81) of less than 2 pages, to the case where (in our notation) <strong>one of the two chances <span class="math inline">\(\pi\)</span> or <span class="math inline">\((1-\pi)\)</span> <em>‘est très petite’</em></strong> [Poisson’s <span class="math inline">\(q\)</span> is our <span class="math inline">\(\pi\)</span>, his <span class="math inline">\(\mu\)</span> is our <span class="math inline">\(n\)</span>, his <span class="math inline">\(\omega\)</span> is our <span class="math inline">\(\mu\)</span>, and his <span class="math inline">\(n\)</span> is our <span class="math inline">\(y\)</span>]. In our notation, he arrived at
<span class="math display">\[Prob[y \ or \ fewer \ events] = \bigg(1 + \frac{\mu}{1!} + \frac{\mu^2}{2!} + \dots + \frac{\mu^y}{y!} \bigg) \  e^{-\mu}.\]</span></p>
<p>In the last of his just three paragraphs on this digression, he notes the probability <span class="math inline">\(e^{-\mu}\)</span> of no event, and <span class="math inline">\(1 - e^{-\mu}\)</span> of at least 1.
He also calculates that, when <span class="math inline">\(\mu\)</span> = 1, the chance is very close to 1 in 100 million that more than <span class="math inline">\(y\)</span> = 10 events would occur in a very large series of trials (of length <span class="math inline">\(n\)</span>), where the probability is 1/<span class="math inline">\(n\)</span> in each trial. Although he give little emphasis to his formula, and no real application, it is Poisson’s name that is now undividely associated with this formula.</p>
<p>Below are some examples of how well it approximates a Binomial in the ‘corner’ where <span class="math inline">\(\pi\)</span> is very small and <span class="math inline">\(n\)</span> is very large. If, of course, the product, <span class="math inline">\(\mu = n \times \pi\)</span> , reaches double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions, and – if one has ready acccess to the tail areas of a Normal distribution – with less effort. Today, of course, unless you are limited to a hand calculator when the internet and R and paper tables are nor available, you would not need to use any approximation, Normal or Poisson, to a binomial.</p>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-9-1.png" alt="Various Binomial random variables/distributions, with large n's and small Bernoulli probabilities, together with the Poisson distributions with the corresponding means. The Poisson distrubution provides a good approximation in the 'lowee corner of the Bimonial distribution with large n's and small probabilities. And, when the product, mu = n * probability, is in the double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions." width="864"><p class="caption">
Figure 14.3: Various Binomial random variables/distributions, with large n’s and small Bernoulli probabilities, together with the Poisson distributions with the corresponding means. The Poisson distrubution provides a good approximation in the ’lowee corner of the Bimonial distribution with large n’s and small probabilities. And, when the product, mu = n * probability, is in the double digits, the Normal approximation distribution provides and accurate approximation for both the Binomial and the Poisson distributions.
</p>
</div>
</div>
</div>
<div id="when-the-binomial-does-not-apply" class="section level2">
<h2>
<span class="header-section-number">14.4</span> When the Binomial does not apply<a class="anchor" aria-label="anchor" href="#when-the-binomial-does-not-apply"><i class="fas fa-link"></i></a>
</h2>
<p>It does not apply if one (or both) of the ‘<strong>i.i.d.</strong>’ (<strong>independent</strong> and <strong>identical</strong>) conditions do not hold. The first of these conditions is often the one that is absent.</p>
<p>Two very nice <a href="http://www.biostat.mcgill.ca/hanley/statbook/CochranTable.png">examples</a> can be found in Cochran’s (old but still excellent) <a href="http://www.biostat.mcgill.ca/hanley/statbook/Cochran3rdEdition.png">textbook on sampling</a>. They were re-visited a few decades years ago in connection with what was (then) a new technique for dealing with
<a href="http://www.medicine.mcgill.ca/epidemiology/hanley/Reprints/sim.pdf#page=2">correlated responses.</a></p>
<p>The table shows, for each of the 30 randomly sampled households, the number of household members had visited a physician in the previous year. Can we base the precision of the observed proportion, 30/104 or 28%, on a binomial distribution where <span class="math inline">\(n\)</span> = 104?</p>
<p>True, the ‘<em>n</em>’ in each household is not the same from household to household, but we can segregate the households by size, and carry out separate binomial calculations for each, all the time assuming a common binomial proportion across houdeholds.</p>
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="va">XLAB</span> <span class="op">=</span> <span class="st">"y: number in household who has visited an MD in last year"</span>
<span class="va">YLAB</span> <span class="op">=</span> <span class="st">"Number of Households\nfor which y persons visited an MD"</span>

<span class="fu">show.household.survey.data</span><span class="op">(</span><span class="va">n.who.visited.md</span>,<span class="va">XLAB</span>,<span class="va">YLAB</span><span class="op">)</span>
<span class="co">#&gt; [1] No. persons with history/trait of interest: 30 / 104 ; Prop'n: 0.29</span>
<span class="co">#&gt; [1]  </span>
<span class="co">#&gt; Below, the row labels are the household sizes.</span>
<span class="co">#&gt; Each column label is the number in a household with history/trait of interest.</span>
<span class="co">#&gt; The entries are how many households had the indicated configuration.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;           0    1    2    3    4    5    6    7    Total</span>
<span class="co">#&gt; 1         1    .    .    .    .    .    .    .        1</span>
<span class="co">#&gt; 2         3    .    1    .    .    .    .    .        4</span>
<span class="co">#&gt; 3         7    2    1    2    .    .    .    .       12</span>
<span class="co">#&gt; 4         4    1    3    .    1    .    .    .        9</span>
<span class="co">#&gt; 5         .    .    1    .    .    1    .    .        2</span>
<span class="co">#&gt; 6         1    .    .    .    .    .    .    .        1</span>
<span class="co">#&gt; 7         1    .    .    .    .    .    .    .        1</span>
<span class="co">#&gt; Total    17    3    6    2    1    1    0    0       30</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Same row/col meanings, but theoretical, binomial-based,</span>
<span class="co">#&gt; EXPECTED frequencies of households having these many persons</span>
<span class="co">#&gt;  who would have -- if all 128 persons were independently sampled</span>
<span class="co">#&gt; from a population where the proportion with </span>
<span class="co">#&gt; the history/trait of interest was as above.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;           0     1     2     3     4     5     6     7 Total</span>
<span class="co">#&gt; 1     0.712 0.288     .     .     .     .     .     .     1</span>
<span class="co">#&gt; 2     1.013 0.821 0.166     .     .     .     .     .     4</span>
<span class="co">#&gt; 3     1.081 1.314 0.533 0.072     .     .     .     .    12</span>
<span class="co">#&gt; 4     1.025 1.663 1.011 0.273 0.028     .     .     .     9</span>
<span class="co">#&gt; 5     0.912 1.849 1.499 0.608 0.123 0.010     .     .     2</span>
<span class="co">#&gt; 6     0.779 1.894 1.920 1.038 0.315 0.051 0.003     .     1</span>
<span class="co">#&gt; 7     0.646 1.834 2.231 1.507 0.611 0.149 0.020 0.001     1</span>
<span class="co">#&gt; Total 6.167 9.663 7.360 3.498 1.077 0.210 0.024 0.001    30</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; We will just assess the fit on the 'totals'row:</span>
<span class="co">#&gt;  the numbers in the individual rows are too small to judge.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;         0   1   2   3   4   5 6 7 Total</span>
<span class="co">#&gt; [1,] 17.0 3.0 6.0 2.0 1.0 1.0 0 0    30</span>
<span class="co">#&gt; [2,]  6.2 9.7 7.4 3.5 1.1 0.2 0 0    30</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Better still, here is a picture:</span></code></pre></div>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-11"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-11-1.png" alt="Of 30 randomly sampled households, (O)bserved  numbers of households, shown in grey, where 0, 1, .. in household had visited  a physician in the previous year. The 30 households contained 104 persons, 28 of whom had visited a physician in the previous year. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same  probability 28/104. The observed variance is considerably LARGER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the 'non-identical probabilities' aspect. The other possibility, the 'non-independence' aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage." width="672"><p class="caption">
Figure 14.4: Of 30 randomly sampled households, (O)bserved numbers of households, shown in grey, where 0, 1, .. in household had visited a physician in the previous year. The 30 households contained 104 persons, 28 of whom had visited a physician in the previous year. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same probability 28/104. The observed variance is considerably LARGER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the ‘non-identical probabilities’ aspect. The other possibility, the ‘non-independence’ aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage.
</p>
</div>
<p>Cochran provides <a href="http://www.biostat.mcgill.ca/hanley/statbook/CochranHouseholdSurvey.png">this explanation</a> for the <strong>greater than binomial variation</strong> in the proportions</p>
<blockquote>
<p>The variance given by the ratio method, 0.00520, is much larger than that given by the binomial formula, 0.00197. For various reasons, families differ in the frequency with which their members consult a doctor. For the sample as a whole, the proportion who consult a doctor is only a little more than one in four, but there are several families in which every member has seen a doctor. Similar results would be obtained for any characteristic in which the members of the same family tend to act in the same way.</p>
</blockquote>
<p>Clearly, if we are dealing with an <strong>infectious disease</strong>, we would need to be very <strong>careful about our statistical model</strong> for the numbers in a family or household or care facility that are affected.</p>
<div class="sourceCode" id="cb110"><pre class="downlit sourceCode r">
<code class="sourceCode R">

<span class="va">XLAB</span> <span class="op">=</span> <span class="st">"y: number of males in household"</span>
<span class="va">YLAB</span><span class="op">=</span>  <span class="st">"Number of Households\n where y persons were male"</span>
<span class="fu">show.household.survey.data</span><span class="op">(</span><span class="va">n.males</span>,<span class="va">XLAB</span>,<span class="va">YLAB</span><span class="op">)</span>
<span class="co">#&gt; [1] No. persons with history/trait of interest: 53 / 104 ; Prop'n: 0.51</span>
<span class="co">#&gt; [1]  </span>
<span class="co">#&gt; Below, the row labels are the household sizes.</span>
<span class="co">#&gt; Each column label is the number in a household with history/trait of interest.</span>
<span class="co">#&gt; The entries are how many households had the indicated configuration.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;          0     1    2    3    4    5    6    7    Total</span>
<span class="co">#&gt; 1        1     .    .    .    .    .    .    .        1</span>
<span class="co">#&gt; 2        .     4    .    .    .    .    .    .        4</span>
<span class="co">#&gt; 3        .     7    5    .    .    .    .    .       12</span>
<span class="co">#&gt; 4        .     1    3    5    .    .    .    .        9</span>
<span class="co">#&gt; 5        .     1    .    1    .    .    .    .        2</span>
<span class="co">#&gt; 6        .     .    .    1    .    .    .    .        1</span>
<span class="co">#&gt; 7        .     .    .    1    .    .    .    .        1</span>
<span class="co">#&gt; Total    1    13    8    8    0    0    0    0       30</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Same row/col meanings, but theoretical, binomial-based,</span>
<span class="co">#&gt; EXPECTED frequencies of households having these many persons</span>
<span class="co">#&gt;  who would have -- if all 128 persons were independently sampled</span>
<span class="co">#&gt; from a population where the proportion with </span>
<span class="co">#&gt; the history/trait of interest was as above.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;           0     1     2     3     4     5     6     7 Total</span>
<span class="co">#&gt; 1     0.490 0.510     .     .     .     .     .     .     1</span>
<span class="co">#&gt; 2     0.481 1.000 0.519     .     .     .     .     .     4</span>
<span class="co">#&gt; 3     0.354 1.103 1.146 0.397     .     .     .     .    12</span>
<span class="co">#&gt; 4     0.231 0.962 1.499 1.038 0.270     .     .     .     9</span>
<span class="co">#&gt; 5     0.142 0.737 1.531 1.591 0.827 0.172     .     .     2</span>
<span class="co">#&gt; 6     0.083 0.520 1.352 1.873 1.460 0.607 0.105     .     1</span>
<span class="co">#&gt; 7     0.048 0.347 1.083 1.875 1.949 1.215 0.421 0.062     1</span>
<span class="co">#&gt; Total 1.829 5.178 7.130 6.775 4.505 1.994 0.526 0.062    30</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; We will just assess the fit on the 'totals'row:</span>
<span class="co">#&gt;  the numbers in the individual rows are too small to judge.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;        0    1   2   3   4 5   6   7 Total</span>
<span class="co">#&gt; [1,] 1.0 13.0 8.0 8.0 0.0 0 0.0 0.0    30</span>
<span class="co">#&gt; [2,] 1.8  5.2 7.1 6.8 4.5 2 0.5 0.1    30</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Better still, here is a picture:</span></code></pre></div>
<div class="figure" style="text-align: left">
<span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="15-binomial_files/figure-html/unnamed-chunk-12-1.png" alt="Of 30 randomly sampled households, (O)bserved  numbers of households, shown in grey, where 0, 1, .. in household were male. The 30 households contained 104 persons, 53 of whom were male. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same  probability 53/104. The observed variance is considerably SMALLER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the 'non-identical probabilities' aspect. The other possibility, the 'non-independence' aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage." width="672"><p class="caption">
Figure 8.9: Of 30 randomly sampled households, (O)bserved numbers of households, shown in grey, where 0, 1, .. in household were male. The 30 households contained 104 persons, 53 of whom were male. Also shown, in blue are the (E)xpected numbers of households, assuming the data were generated from 104 independent Bernoulli random variables, each with the same probability 53/104. The observed variance is considerably SMALLER than that predicted by a binomial distribution. You would see this even if the individuals in the house were not from the same family. For example, if the occupants were students, the proportion of them with such a history would be different (?lower) than if the occupants were older: this is the ‘non-identical probabilities’ aspect. The other possibility, the ‘non-independence’ aspect, is that health status and the seeking medical care are affected by shared family factors, such as behaviours, attitudes, lifestyle, and insurance coverage.
</p>
</div>
<p>Cochran provides <a href="http://www.biostat.mcgill.ca/hanley/statbook/CochranHouseholdSurvey.png">this explanation</a> for the <strong>(unsually) less than binomial variation</strong> seen in this response variable.</p>
<blockquote>
<p>In estimating the proportion of males in the population, the results are different. By the same type of calculation, we find binomial formula: v(p) = 0.00240; ratio formula v(p) = 0.00114<br>
Here the <strong>binomial formula overestimates the variance</strong>. The reason is interesting. Most households are set up as a result of a marriage, hence contain at least one male and one female. Consequently the proportion of males per family varies less from one half than would be expected from the binomial formula. None of the 30 families, except one with only one member, is composed entirely of males, or entirely of females. <strong>If the binomial distribution were applicable, with a true P of approximately one half, households with all members of the same sex would constitute one quarter of the households of size 3 and one eighth of the households of size 4. This property of the sex ratio has been discussed by Hansen and Hurwitz (1942). Other illustrations of the error committed by improper use of the binomial formula in sociological investigations have been given by <a href="http://www.biostat.mcgill.ca/hanley/statbook/Kish1957.pdf">Kish (1957)</a>.</strong></p>
</blockquote>
<p><strong>Does the Binomial Distribution apply if… ?</strong></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="65%">
<col width="17%">
<col width="17%">
</colgroup>
<tbody>
<tr class="odd">
<td align="left">Interested in:</td>
<td align="right"><span class="math inline">\(\pi\)</span></td>
<td align="left">the proportion of 16 year old girls in Quebec protected against rubella</td>
</tr>
<tr class="even">
<td align="left">Choose:</td>
<td align="right">
<span class="math inline">\(n\)</span> = 100</td>
<td align="left">girls: 20 at random from each of 5 randomly selected schools [‘cluster’ sample]</td>
</tr>
<tr class="odd">
<td align="left">Count</td>
<td align="right"><span class="math inline">\(y\)</span></td>
<td align="left">how many of the <span class="math inline">\(n\)</span> = 100 are protected</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left">
<span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n=100, \ \pi)\)</span> ?</td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="even">
<td align="left">‘SMAC’:</td>
<td align="right"><span class="math inline">\(\pi\)</span></td>
<td align="left">Chemistry Auto-analyzer with n = 18 channels. Critertion for ‘positivity’ set so that Prob[‘abnormal’ result in Healthy person] = 0.03 for each of 18 chemistries tested</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Count</td>
<td align="right"><span class="math inline">\(y\)</span></td>
<td align="left">(In 1 patient) how many of <span class="math inline">\(n\)</span> = 18 give abnormal result.</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left">
<span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n=18, \ \pi=0.03)\)</span> ? <a href="https://www.amazon.ca/Biostatistics-Clinical-Medicine-Joseph-Ingelfinger/dp/0023597216">credit: Ingelfinger textbook</a>
</td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="even">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="odd">
<td align="left">Sex Ratio:</td>
<td align="right"><span class="math inline">\(n=4\)</span></td>
<td align="left">children in each family</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"><span class="math inline">\(y\)</span></td>
<td align="left">number of girls in family</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="left">
<span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n=4, \ \pi=0.49)\)</span> ?</td>
</tr>
<tr class="even">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="odd">
<td align="left">Interested in:</td>
<td align="right"><span class="math inline">\(\pi_u\)</span></td>
<td align="left">proportion in ‘usual’ exercise classes and in</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"><span class="math inline">\(\pi_e\)</span></td>
<td align="left">expt’l. exercise classes who ‘stay the course’</td>
</tr>
<tr class="odd">
<td align="left">Randomly</td>
<td align="right">4</td>
<td align="left">classes of</td>
</tr>
<tr class="even">
<td align="left">Allocate</td>
<td align="right">25</td>
<td align="left">students each to usual course</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"><span class="math inline">\(n_u = 4 \times 25 = 100\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right">
<span class="math inline">\(n_e\)</span> = 4</td>
<td align="left">classes of</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">25</td>
<td align="left">students each to experimental course</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"><span class="math inline">\(n_e = 4 \times 25 =100\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Count</td>
<td align="right"><span class="math inline">\(y_u\)</span></td>
<td align="left">how many of the <span class="math inline">\(n_u\)</span> = 100 complete course</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"><span class="math inline">\(y_e\)</span></td>
<td align="left">how many of the <span class="math inline">\(n_e\)</span> = 100 complete course</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right"></td>
<td align="left">
<span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y_u ~ \sim Binomial(n = 100, \ \pi_u)\)</span> ?</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left">
<span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y_e ~ \sim Binomial(n = 100, \ \pi_e)\)</span> ?</td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
<tr class="even">
<td align="left">Pilot Study:</td>
<td align="right"></td>
<td align="left">To estimate proportion <span class="math inline">\(\pi\)</span> of population that is eligible and willing to participate in long-term research study, keep recruiting until obtain</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right">
<span class="math inline">\(y\)</span> = 5</td>
<td align="left">who are. Have to approach <span class="math inline">\(n\)</span> to get <span class="math inline">\(y\)</span>.</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right"></td>
<td align="left">
<span class="math inline">\(\bullet\)</span> Is <span class="math inline">\(y ~ \sim Binomial(n, \ \pi)\)</span> ?</td>
</tr>
<tr class="odd">
<td align="left">………….</td>
<td align="right">………..</td>
<td align="left">………………………………………………………</td>
</tr>
</tbody>
</table></div>
</div>
<div id="more-on-the-approximation-of-the-binomial-distribution" class="section level2">
<h2>
<span class="header-section-number">14.5</span> More on the Approximation of the Binomial Distribution<a class="anchor" aria-label="anchor" href="#more-on-the-approximation-of-the-binomial-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The Normal distribution emerges frequently as an approximation of the
distribution of data characteristics. The probability theory that
mathematically establishes such approximation is called the Central
Limit Theorem. In this section we
demonstrate the Normal approximation in the context of the Binomial
distribution.</p>
<div id="approximate-binomial-probabilities-and-percentiles-1" class="section level3">
<h3>
<span class="header-section-number">14.5.1</span> Approximate Binomial Probabilities and Percentiles<a class="anchor" aria-label="anchor" href="#approximate-binomial-probabilities-and-percentiles-1"><i class="fas fa-link"></i></a>
</h3>
<p>Consider, for example, the probability of obtaining between 1940 and
2060 heads when tossing 4,000 fair coins. Let <span class="math inline">\(Y\)</span> be the total number of
heads. The tossing of a coin is a trial with two possible outcomes:
“Head” and “Tail.” The probability of a “Head” is 0.5 and there are
4,000 trials. Let us call obtaining a “Head” in a trial a “Success”.
Observe that the random variable <span class="math inline">\(Y\)</span> counts the total number of
successes. Hence, <span class="math inline">\(Y \sim \mathrm{Binomial}(4000,0.5)\)</span>.</p>
<p>The probability <span class="math inline">\(\operatorname{P}(1940 \leq Y \leq 2060)\)</span> can be computed as the
difference between the probability <span class="math inline">\(\operatorname{P}(Y \leq 2060)\)</span> of being less or
equal to 2060 and the probability <span class="math inline">\(\operatorname{P}(Y &lt; 1940)\)</span> of being strictly
less than 1940. However, 1939 is the largest integer that is still
strictly less than the integer 1940. As a result we get that
<span class="math inline">\(\operatorname{P}(Y &lt; 1940) = \operatorname{P}(Y \leq 1939)\)</span>. Consequently,
<span class="math inline">\(\operatorname{P}(1940 \leq Y \leq 2060) = \operatorname{P}(Y \leq 2060) - \operatorname{P}(Y \leq 1939)\)</span>.</p>
<p>Applying the function “<code>pbinom</code>” for the computation of the Binomial
cumulative probability, namely the probability of being less or equal to
a given value, we get that the probability in the range between 1940 and
2060 is equal to</p>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">pbinom</a></span><span class="op">(</span><span class="fl">2060</span>,<span class="fl">4000</span>,<span class="fl">0.5</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">pbinom</a></span><span class="op">(</span><span class="fl">1939</span>,<span class="fl">4000</span>,<span class="fl">0.5</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.9442883</span></code></pre></div>
<p>This is an exact computation. The Normal approximation produces an
approximate evaluation, not an exact computation. The Normal
approximation replaces Binomial computations by computations carried out
for the Normal distribution. The computation of a probability for a
Binomial random variable is replaced by computation of probability for a
Normal random variable that has the same expectation and standard
deviation as the Binomial random variable.</p>
<p>Notice that if <span class="math inline">\(Y \sim \mathrm{Binomial}(4000,0.5)\)</span> then the expectation
is <span class="math inline">\(\operatorname{E}(Y) = 4,000 \times 0.5 = 2,000\)</span> and the variance is
<span class="math inline">\(\operatorname{Var}(Y) = 4,000 \times 0.5 \times 0.5 = 1,000\)</span>, with the standard
deviation being the square root of the variance. Repeating the same
computation that we conducted for the Binomial random variable, but this
time with the function “<code>pnorm</code>” that is used for the computation of the
Normal cumulative probability, we get:</p>
<div class="sourceCode" id="cb112"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">mu</span> <span class="op">&lt;-</span> <span class="fl">4000</span><span class="op">*</span><span class="fl">0.5</span>
<span class="va">sig</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">4000</span><span class="op">*</span><span class="fl">0.5</span><span class="op">*</span><span class="fl">0.5</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">2060</span>,<span class="va">mu</span>,<span class="va">sig</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">1939</span>,<span class="va">mu</span>,<span class="va">sig</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.9442441</span></code></pre></div>
<p>Observe that in this example the Normal approximation of the probability
(0.9442441) agrees with the Binomial computation of the probability
(0.9442883) up to 3 significant digits.</p>
<p>Normal computations may also be applied in order to find approximate
percentiles of the Binomial distribution. For example, let us identify
the central region that contains for a <span class="math inline">\(\mathrm{Binomial}(4000,0.5)\)</span>
random variable (approximately) 95% of the distribution. Towards that
end we can identify the boundaries of the region for the Normal
distribution with the same expectation and standard deviation as that of
the target Binomial distribution:</p>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.975</span>,<span class="va">mu</span>,<span class="va">sig</span><span class="op">)</span>
<span class="co">#&gt; [1] 2061.98</span>
<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.025</span>,<span class="va">mu</span>,<span class="va">sig</span><span class="op">)</span>
<span class="co">#&gt; [1] 1938.02</span></code></pre></div>
<p>After rounding to the nearest integer we get the interval <span class="math inline">\([1938,2062]\)</span>
as a proposed central region.</p>
<p>In order to validate the proposed region we may repeat the computation
under the actual Binomial distribution:</p>
<div class="sourceCode" id="cb114"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">qbinom</a></span><span class="op">(</span><span class="fl">0.975</span>,<span class="fl">4000</span>,<span class="fl">0.5</span><span class="op">)</span>
<span class="co">#&gt; [1] 2062</span>
<span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">qbinom</a></span><span class="op">(</span><span class="fl">0.025</span>,<span class="fl">4000</span>,<span class="fl">0.5</span><span class="op">)</span>
<span class="co">#&gt; [1] 1938</span></code></pre></div>
<p>Again, we get the interval <span class="math inline">\([1938,2062]\)</span> as the central region, in
agreement with the one proposed by the Normal approximation. Notice that
the function “<code>qbinom</code>” produces the percentiles of the Binomial
distribution.</p>
<div class="important">
<p>The ability to approximate one distribution by the other, when
computation tools for both distributions are handy, seems to be of
questionable importance. Indeed, the significance of the Normal
approximation is not so much in its ability to approximate the Binomial
distribution as such. Rather, the important point is that the Normal
distribution may serve as an approximation to a wide class of
distributions, with the Binomial distribution being only one example.
Computations that are based on the Normal approximation will be valid
for all members in the class of distributions, including cases where we
don’t have the computational tools at our disposal or even in cases
where we do not know what the exact distribution of the member is (aka the CLT).</p>
</div>
<p>On the other hand, one need not assume that any distribution is well
approximated by the Normal distribution. For example, the distribution
of wealth in the population tends to be skewed, with more than 50% of
the people possessing less than 50% of the wealth and small percentage
of the people possessing the majority of the wealth. The Normal
distribution is not a good model for such distribution. The Exponential
distribution, or distributions similar to it, may be more appropriate.</p>
</div>
<div id="continuity-corrections-1" class="section level3">
<h3>
<span class="header-section-number">14.5.2</span> Continuity Corrections<a class="anchor" aria-label="anchor" href="#continuity-corrections-1"><i class="fas fa-link"></i></a>
</h3>
<p>In order to complete this section let us look more carefully at the
Normal approximations of the Binomial distribution.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:Normal6"></span>
<img src="15-binomial_files/figure-html/Normal6-1.png" alt="Normal Approximation of the Binomial Distribution" width="60%"><p class="caption">
Figure 9.7: Normal Approximation of the Binomial Distribution
</p>
</div>
<p>In principle, the Normal approximation is valid when <span class="math inline">\(n\)</span>, the number of
independent trials in the Binomial distribution, is large. When <span class="math inline">\(n\)</span> is
relatively small the approximation may not be so good. Indeed, take
<span class="math inline">\(Y \sim \mathrm{Binomial}(30,0.3)\)</span> and consider the probability
<span class="math inline">\(\operatorname{P}(Y \leq 6)\)</span>. Compare the actual probability to the Normal
approximation:</p>
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">pbinom</a></span><span class="op">(</span><span class="fl">6</span>,<span class="fl">30</span>,<span class="fl">0.3</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.159523</span>
<span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">6</span>,<span class="fl">30</span><span class="op">*</span><span class="fl">0.3</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">30</span><span class="op">*</span><span class="fl">0.3</span><span class="op">*</span><span class="fl">0.7</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.1159989</span></code></pre></div>
<p>The Normal approximation, which is equal to 0.1159989, is not too close
to the actual probability, which is equal to 0.1595230.</p>
<p>A naïve application of the Normal approximation for the
<span class="math inline">\(\mathrm{Binomial}(n,p)\)</span> distribution may not be so good when the number
of trials <span class="math inline">\(n\)</span> is small. Yet, a small modification of the approximation
may produce much better results. In order to explain the modification
consult Figure <a href="ChapNormal.html#fig:Normal6">9.7</a> where you will find the bar plot of the
Binomial distribution with the density of the approximating Normal
distribution superimposed on top of it. The target probability is the
sum of heights of the bars that are painted in <em>red</em>. In the naïve
application of the Normal approximation we used the area under the
normal density which is to the left of the bar associated with the value
<span class="math inline">\(y=6\)</span>.</p>
<p>Alternatively, you may associate with each bar located at <span class="math inline">\(y\)</span> the area
under the normal density over the interval <span class="math inline">\([y-0.5, y+0.5]\)</span>. The
resulting correction to the approximation will use the Normal
probability of the event <span class="math inline">\(\{Y \leq 6.5\}\)</span>, which is the area shaded in
<em>red</em>. The application of this approximation, which is called
<em>continuity correction</em> produces:</p>
<div class="sourceCode" id="cb116"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">6.5</span>,<span class="fl">30</span><span class="op">*</span><span class="fl">0.3</span>,<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">30</span><span class="op">*</span><span class="fl">0.3</span><span class="op">*</span><span class="fl">0.7</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.1596193</span></code></pre></div>
<p>Observe that the corrected approximation is much closer to the target
probability, which is 0.1595230, and is substantially better that the
uncorrected approximation which was 0.1159989. Generally, it is
recommended to apply the continuity correction to the Normal
approximation of a discrete distribution.</p>
<p>Consider the <span class="math inline">\(\mathrm{Binomial}(n,p)\)</span> distribution. Another situation
where the Normal approximation may fail is when <span class="math inline">\(p\)</span>, the probability of
“Success” in the Binomial distribution, is too close to 0 (or too close
to 1).</p>
</div>
</div>
<div id="sec:RVarExercises" class="section level2">
<h2>
<span class="header-section-number">14.6</span> Exercises<a class="anchor" aria-label="anchor" href="#sec:RVarExercises"><i class="fas fa-link"></i></a>
</h2>
<p>A particular measles vaccine produces a reaction (a
fever higher that 102 Fahrenheit) in each vaccinee with probability of
0.09. A clinic vaccinates 500 people each day.</p>
<ol style="list-style-type: decimal">
<li><p>What is the expected number of people that will develop a reaction
each day?</p></li>
<li><p>What is the standard deviation of the number of people that will
develop a reaction each day?</p></li>
<li><p>In a given day, what is the probability that more than 40 people
will develop a reaction?</p></li>
<li><p>In a given day, what is the probability that the number of people
that will develop a reaction is between 50 and 45 (inclusive)?</p></li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:RVar10"></span>
<img src="15-binomial_files/figure-html/RVar10-1.png" alt="Bar Plots of the Negative-Binomial Distribution" width="60%"><p class="caption">
Figure 14.5: Bar Plots of the Negative-Binomial Distribution
</p>
</div>
</div>
<div id="summary-6" class="section level2">
<h2>
<span class="header-section-number">14.7</span> Summary<a class="anchor" aria-label="anchor" href="#summary-6"><i class="fas fa-link"></i></a>
</h2>
<div id="glossary" class="section level3 unnumbered">
<h3>Glossary<a class="anchor" aria-label="anchor" href="#glossary"><i class="fas fa-link"></i></a>
</h3>
<dl>
<dt>Binomial Random Variable:</dt>
<dd>
<p>The number of successes among <span class="math inline">\(n\)</span> repeats of independent trials with
a probability <span class="math inline">\(p\)</span> of success in each trial. The distribution is
marked as <span class="math inline">\(\mathrm{Binomial}(n,p)\)</span>.</p>
</dd>
<dt>Density:</dt>
<dd>
<p>Histogram that describes the distribution of a continuous random
variable. The area under the curve corresponds to probability.</p>
</dd>
</dl>
</div>
<div id="summary-of-formulas" class="section level3 unnumbered">
<h3>Summary of Formulas<a class="anchor" aria-label="anchor" href="#summary-of-formulas"><i class="fas fa-link"></i></a>
</h3>
<dl>
<dt>Discrete Random Variable:</dt>
<dd>
</dd>
</dl>
<p><span class="math display">\[\begin{aligned}
        \operatorname{E}(Y) &amp;= \sum_y \big(y \times \operatorname{P}(y)\big) \\
        \operatorname{Var}(Y) &amp;= \sum_y\big( (y-\operatorname{E}(Y))^2 \times \operatorname{P}(y)\big) \end{aligned}\]</span></p>
<dl>
<dt>Binomial:</dt>
<dd>
<p><span class="math inline">\(\operatorname{E}(Y) = n p \;, \quad \operatorname{Var}(Y) = n p(1-p)\)</span></p>
</dd>
</dl>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="inference-one-mean.html"><span class="header-section-number">13</span> Inference for a single mean</a></div>
<div class="next"><a href="inference-one-prop.html"><span class="header-section-number">15</span> Inference for a single proportion</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ChapBinom"><span class="header-section-number">14</span> Binomial Random Variable</a></li>
<li><a class="nav-link" href="#objectives-2"><span class="header-section-number">14.1</span> Objectives</a></li>
<li>
<a class="nav-link" href="#bernoulli"><span class="header-section-number">14.2</span> Bernoulli</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#expectation-and-variance-of-a-bernoulli-random-variable"><span class="header-section-number">14.2.1</span> Expectation and Variance of a Bernoulli random variable</a></li></ul>
</li>
<li>
<a class="nav-link" href="#binomial"><span class="header-section-number">14.3</span> Binomial</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#binomial-probabilities-illustrated-using-a-binomial-tree"><span class="header-section-number">14.3.1</span> Binomial probabilities, illustrated using a Binomial Tree</a></li>
<li><a class="nav-link" href="#calculating-binomial-probabilities"><span class="header-section-number">14.3.2</span> Calculating Binomial probabilities</a></li>
</ul>
</li>
<li><a class="nav-link" href="#when-the-binomial-does-not-apply"><span class="header-section-number">14.4</span> When the Binomial does not apply</a></li>
<li>
<a class="nav-link" href="#more-on-the-approximation-of-the-binomial-distribution"><span class="header-section-number">14.5</span> More on the Approximation of the Binomial Distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#approximate-binomial-probabilities-and-percentiles-1"><span class="header-section-number">14.5.1</span> Approximate Binomial Probabilities and Percentiles</a></li>
<li><a class="nav-link" href="#continuity-corrections-1"><span class="header-section-number">14.5.2</span> Continuity Corrections</a></li>
</ul>
</li>
<li><a class="nav-link" href="#sec:RVarExercises"><span class="header-section-number">14.6</span> Exercises</a></li>
<li>
<a class="nav-link" href="#summary-6"><span class="header-section-number">14.7</span> Summary</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#glossary">Glossary</a></li>
<li><a class="nav-link" href="#summary-of-formulas">Summary of Formulas</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>EPIB607</strong>" was written by Sahir Bhatnagar and James A Hanley. It was last built on 2021-12-07.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
