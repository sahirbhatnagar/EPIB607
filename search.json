[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Welcome course website EPIB 607 FALL 2021: Inferential Statistics McGill University.","code":""},{"path":"index.html","id":"objectives","chapter":"Preface","heading":"Objectives","text":"aim course provide students basic principles statistical inference can:Visualize/Analyze/Interpret data using statistical methods R statistical software program.Understand statistical results scientific paper.Learn tools creating reproducible analysesApply statistical methods research.Use methods learned course foundation advanced biostatistics courses.","code":""},{"path":"index.html","id":"audience","chapter":"Preface","heading":"Audience","text":"principal audience researchers natural social sciences haven’t introductory course statistics (one long time ago). audience accepts statistics penetrated life sciences pervasively required knowledge research understanding scientific papers.course tailored graduate students population health sciences first year. Concurrently, take first courses epidemiologic methods. department known emphasis quantitative methods, students’ ability carry quantitative work. Since data deal non-experimental, strong emphasis multivariable regression. students statistical courses undergraduates, courses start beginning, pitched Master’s level.last decade, incoming classes become diverse, backgrounds, career plans. recently begun MScPH program plan consumers rather producers research; previously, majority students pursued thesis-based Masters involved considerable statistical analyses produce new statistical evidence.","code":""},{"path":"index.html","id":"about-these-notes","chapter":"Preface","heading":"About these notes","text":"notes collection useful links, videos, online resources papers introductory course statistics. instructors found single book sufficiently teaches topics covered course. Part due advancements computing far outpaced publication modern textbooks. Indeed, computer replaced many calculations traditionally taught done hand. direct readers think good learning resource given topic (following Flipped Classroom strategy). also provide commentary notes think useful.","code":""},{"path":"index.html","id":"topicstextbooks","chapter":"Preface","heading":"Topics/textbooks","text":"first term course 607, recent choices Practice Statistics Life Sciences Baldi Moore, Stats de Veaux, Velleman Bock. Others recommended older texts Pagano Gauvreau, Rosner. us also drawn material Statistics Freedman, Pisani, Purves Adkikari, \nStatistical Methods Medical Research, 4th Edition_ \nArmitage,Berry, Matthews.newer books tried teach topic engagingly, starting data come , (descriptively) displaying single distributions, relationships variables. many others typically move Probability; Random Variables; Sampling Distributions; Confidence intervals Tests Hypotheses; Inference /single Mean/Proportion/Rate difference two Means/Proportions/Rates; Chi-square Tests 2 way frequency tables; Simple Correlation Regression. include (point online) chapter Non-Parametric Tests. typically end tables probability tail areas, critical values.Bradford Hill’s Principles Medical Statistics followed sequence 80 years ago, black type book measured 6 inches 9 inches 1 inch, weighed less pound. Today’s multi-colour texts 50% longer, 50% wider, twice thick, weigh 5 pounds .topics covered second term course include multiple regression involving\nGaussian, Binomial, Poisson variation, well\n(possibly censored) time-durations – reciprocals, event rates. difficult point one modern comprehensive textbook.\npressure add even topics, correlated data, missing data, measurement error etc. top second statistics course.","code":""},{"path":"index.html","id":"regression-from-the-outset","chapter":"Preface","heading":"Regression from the outset","text":"important balance desire cover regression-based topics good grounding, first term, basic concepts underlie statistical analyses.first term epidemiology course deals proportions rates (risks hazards) – core epidemiology – comparisons involving . Control confounding typically via odds/risk/rate differences/ratios obtained standardization Mantel-Haenszel-type summary measures. Teachers reluctant spend time teach classical confidence intervals , intuitive – students covered multiple regression – superceded model-based intervals.One way synchronize epidemiology, teach six separate topics Mean/Proportion/Rate differences two Means/Proportions/Rates unified way embedding 6 regression format right outset, use generalized linear models, focus --none contrasts, represented binary ‘X’ values.benefits. now, lot time 607 spent 1-sample 2-sample methods (chi-square tests) don’t lead anywhere (generalize). Ironically, first-term concerns equal unequal variance tests longer raised, obsessed , multiple regression framework second term.teaching/learning statistical concepts/techiques greatly enriched real-world applications published reports public health epidemiology research. 1980, first course statistics provided access 80% articles NEJM articles. large dividend longer case – even less journals report non-experimental research. 1-sample 2-sample methods, chi-square tests core first statistics courses longer techniques underlie reported summaries abstracts full text. statistical analysis sections \nmany articles still start descriptive statistics perfunctory list parametric non-parametric 1 2 sample tests, describe multivariable techniques used produce reported summaries. [Laboratory sciences can still get t-tests ‘anova’s – occasional ancova’; studies involving intact human beings free-living populations can .] Thus, first statistical course get ‘understanding’ dividend research articles introductory epidemiology course , first statistical course needs teach techniques produce results abstracts. Even can go far, approach can promote regression approach right week one, build week, rather introduce first time week 9 10, course already beginning wind , assignments courses piling .","code":""},{"path":"index.html","id":"parameters-first-data-later","chapter":"Preface","heading":"Parameters first, data later","text":"many teachers students think regression, imagine cloud points x-y space, least squares fitting regression line. start thinking data.teachers, introduce regression, describing/teaching equation connects parameters, constructed way parameter-constrast interest easily directly visible. Three teachers Clayton Hills 1995, Miettinen1985, Rothman 2012. case, first chapter regression limited parameters undersatnding mean; data appear next chapter.lot commend approach. reminds epidemiologists – even statisticians – statistical inference parameters. addressing data data-summaries, need specify estimands – .e, parameter(s) () pursuing.Fisher, introducing Likelihood 1922, one earliest statisticians distinguish parameters statistics. decried ‘obscurity envelops theoretical bases statistical methods’ ascribed two considerations. (emphasis )first place, appears widely thought, rather felt, subject results liable greater smaller errors, precise definition ideas concepts , impossible, least practical necessity.second place, happened statistics purely verbal confusion hindered distinct formulation statistical problems; customary apply name, mean, standard deviation, correlation coefficient, etc., true value like know, can estimate, particular value happen arrive methods estimation. [R. . Fisher. Mathematical Foundations Theoretical Statistics. Philosophical Transactions Royal Society London. Series , Containing Papers Mathematical Physical Character, Vol. 222 (1922), pp. 309-368]easy tempting start data, since form summary statistic usually easy write directly. can also used motivate definition: example, define odds ratio empirical computational form ad/bc. However, ‘give answer first, question later’ approach comes short soon one asks statistically stable estimate . derive standard error confidence interval, one appeal sampling distribution. , one needs identify random variables involvced, parameters determine/modulate statistical distributions.students master big picture (parameter(s) pursued), task estimating fitting equations data considerably simplified, becomes generic. approach upfront thinking devoted parameters – Miettinen calls design study object – focus pre-specified ‘deliverable.’","code":""},{"path":"index.html","id":"lets-switch-to-y-bar-and-drop-x-bar","chapter":"Preface","heading":"Let’s switch to “y-bar”, and drop “x-bar”","text":"prevailing practice, introducing descriptive statistics, even 1 two sample procedures, use term x-bar (\\(\\bar{x}\\)) arithmetic mean (one notable execption de Veaux al.) misses chance prepare students regression, E[Y|X] object interest, X-conditional Y’s random variables. Technically speaking, X’s even considered random variables. Elevating status Y’s explaining role X’s, impact X distributions precision might also cut practice checking normality X’s, even though X’s random variables. merely X locations/profiles Y measured/recorded. possible, X distribution determined investigators, give precise less correlated estimates parameters pursued. Switching \\(\\bar{x}\\) \\(\\bar{y}\\) simple yet meaningful step direction. JH made switch 10 years ago.","code":""},{"path":"index.html","id":"computing-from-the-outset","chapter":"Preface","heading":"Computing from the outset","text":"1980, calculations first part 607 hand calculator. Computing summary statistics hand seen way help students understand concepts involved, absence automated rapid computation considered drawback. However,\nalways help students understand concept standard deviation regression slope, since formulae designed minimize number keystrokes, rather illuminate construct involved. example, common rescale relocate data cut numbers digits entered, group values bins, use midpoints frequencies. also common use computationally-economical 1-pass---data formula sample variance\n\\[s^2 =   \\frac{ \\sum y^2 - \\frac{(\\sum y)^2}{n}}{n-1},\\]\neven though definitional formula \\[s^2 = \\frac{\\sum(y - \\bar{y})^2}{n-1}.\\]latter (definitional) one considered long, even though first compute \\(\\bar{y}\\) go back compute (square) \\(y - \\bar{y}\\) helped students internalize sample variance .spreadsheets arrived early 1980s, students use built-mean formula compute display \\(\\bar{y}\\), another formula compute display new column deviations \\(\\bar{y}\\), another compute display new column squares deviations, another count number deviations, final formula arrives \\(s^2.\\) understanding comes coding definitional formula, spreadsheet simply speedily carries , allowing user see relevant components, noticing one looks reasonable. Ultimately, students master concept, move built-formulae hide (avoid) intermediate quantities.teachers actually encouraged use spreadsheets, instead promoted commercial statistical packages SAS, SPSS Stata. Thus, opportunity learn `walk first, run later’ afforded spreadsheets fully exploited.RStudio integrated environment R, free software environment statistical computing graphics runs wide variety platforms. Just like spreadsheet software, one can use R just calculator, programmable calculator, programming , learn concepts moving built-functions. large user-community tradition sharing information ways things. graphics language contains primitive functions allow customization, well higher-level functions, tightly integrated statistical routines data frame functions. R Markdown helps foster reproducible research. Shiny apps allow interactivity visualization, bit like ‘-ifs’ spreadsheet.","code":""},{"path":"index.html","id":"the-link-between-epib607-and-epib613","chapter":"Preface","heading":"The link between EPIB607 and EPIB613","text":"takes practice become comfortable R. less mathematical, somewhat cryptic , quite intuitive , packages. last several years, department offered 13 hour course introduction R first term. Initially aim prepare students using course 621 second term, Fall 2018, 2019 2020 offerings EPIB607, computing R use R Studio became mandatory. Just epidemiology material Fall shared 2 courses (601 602), aim also spread statistics material 607 613, integrate two tightly. example, material ‘descriptive’ (.e., model-based) statistics graphical displays covered 613, 607 begin parameters models. Rather treat computing separate activity, exercises based 607 material carried part 613 classes/tutorials. statistical material used motivate computer tasks.Classic introductory statistics textbooks written time computers still infancy. , even newer editions heavily rely -hand computations looking tables tail probabilities. take modern approach introduce computational methods statistics statistical software program R.","code":""},{"path":"index.html","id":"teaching-strategy","chapter":"Preface","heading":"Teaching strategy","text":"course follow Flipped Classroom model. , students expected engaged material coming class. allows instructor delegate delivery basic content definitions textbooks videos, enforces idea students simply passive recipients information. approach allows professor focus valuable class time nurturing efficient discussions surrounding ideas within content, guiding interactive\nexploration typical misconceptions, promoting collaborative problem solving peers.","code":""},{"path":"index.html","id":"datacamp","chapter":"Preface","heading":"DataCamp","text":"class supported DataCamp, intuitive learning platform data science analytics. Learn time, anywhere become expert R, Python, SQL, . DataCamp’s learn--methodology combines short expert videos hands---keyboard exercises help learners retain knowledge. DataCamp offers 350+ courses expert instructors topics importing data, data visualization, machine learning. ’re constantly expanding curriculum keep latest technology trends provide best learning experience skill levels. Join 6 million learners around world close skills gap.asked complete courses DataCamp background reading assignments. can sign free account link. Note: required sign @mail.mcgill.ca @mcgill.ca email address.","code":""},{"path":"index.html","id":"r-code-conventions","chapter":"Preface","heading":"R Code Conventions","text":"use R code throughout notes. R code displayed1 typeset using monospace font syntax highlighting enabled ensure differentiation functions, variables, . example, following adds 1 1Each code segment may contain actual output R. output appear grey font prefixed #>. example, output code segment look like :","code":"\na = 1L + 1L\na#> [1] 2"},{"path":"index.html","id":"development","chapter":"Preface","heading":"Development","text":"book built bookdown open source freely available. approach encourages contributions, ensures reproducibility provides access material worldwide. online version book hosted sahirbhatnagar.com/EPIB607. entire source code available https://github.com/sahirbhatnagar/EPIB607.notice errors, grateful let us know filing issue \nmaking pull request clicking Edit page link right hand side page.","code":""},{"path":"index.html","id":"about-the-authors","chapter":"Preface","heading":"About the authors","text":"Sahir R. Bhatnagar: Assistant Professor Biostatistics - McGill University, Montreal, Canada.\nWebsite: https://sahirbhatnagar.com/\nGitHub: https://github.com/sahirbhatnagar\nWebsite: https://sahirbhatnagar.com/GitHub: https://github.com/sahirbhatnagarJames . Hanley: Professor Biostatistics - McGill University, Montreal, Canada.\nWebsite: http://www.medicine.mcgill.ca/epidemiology/hanley/\nWebsite: http://www.medicine.mcgill.ca/epidemiology/hanley/","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":"\nEPIB607: Inferential Statistics Sahir Bhatnagar James Hanley licensed Attribution-NonCommercial 4.0 International\n","code":""},{"path":"schedule.html","id":"schedule","chapter":"Schedule","heading":"Schedule","text":"","code":""},{"path":"syllabus.html","id":"syllabus","chapter":"Syllabus","heading":"Syllabus","text":"","code":""},{"path":"syllabus.html","id":"general-information","chapter":"Syllabus","heading":"General Information","text":"Course #: EPIB 607Course #: EPIB 607Term year: Fall 2021Term year: Fall 2021Course pre-requisite(s): first year course undergraduate\ndifferential integral calculus. Basic understanding \nexponentials, logs, histograms, graphs, mean, median, mode, standard\ndeviation. Enrollment Epidemiology Public Health program\nMcGill University, permission instructor.Course pre-requisite(s): first year course undergraduate\ndifferential integral calculus. Basic understanding \nexponentials, logs, histograms, graphs, mean, median, mode, standard\ndeviation. Enrollment Epidemiology Public Health program\nMcGill University, permission instructor.Course co-requisite: EPIB 613Course co-requisite: EPIB 613Course schedule: Tuesdays Thursdays 11:35am - 1:25pm \nEducation 129.Course schedule: Tuesdays Thursdays 11:35am - 1:25pm \nEducation 129.Number credits: 4Number credits: 4","code":""},{"path":"syllabus.html","id":"instructor-ta-information","chapter":"Syllabus","heading":"Instructor & TA Information","text":"","code":""},{"path":"syllabus.html","id":"instructor-information","chapter":"Syllabus","heading":"Instructor Information","text":"Name Title: Sahir Rai Bhatnagar, PhD, Assistant ProfessorName Title: Sahir Rai Bhatnagar, PhD, Assistant ProfessorE-mail:\nsahir.bhatnagar@mcgill.caE-mail:\nsahir.bhatnagar@mcgill.caOffice hours: Fridays 8am - 9am (via Zoom)Office hours: Fridays 8am - 9am (via Zoom)Communication plan: office hours held virtually via Zoom\n(link posted myCourses).\nlarge class many students struggle topics.\nreason, often inefficient instructor teaching\nassistants respond individual questions via email. strongly encouraged attend virtual office hours ask questions person. easiest \nefficient way ensure respond timely manner. also\ncreated Slack workspace entire class, monitored \nTAs . may post questions , potentially get\nresponses peers//TAs. must send email, please include\ninformative subject title beginning [Fall 2021 - EPIB-607-001 - Inferential Statistics].\nneed time troubleshoot problem topic, feel free\nsetup appointment TAs. best help.\ninstructor teaching assistants make every effort respond \nemails/Slack messages within 48 hours receipt. Emails/Slack messages\nreceived 5:00pm EST weekends treated though received\nfollowing business day.Communication plan: office hours held virtually via Zoom\n(link posted myCourses).\nlarge class many students struggle topics.\nreason, often inefficient instructor teaching\nassistants respond individual questions via email. strongly encouraged attend virtual office hours ask questions person. easiest \nefficient way ensure respond timely manner. also\ncreated Slack workspace entire class, monitored \nTAs . may post questions , potentially get\nresponses peers//TAs. must send email, please include\ninformative subject title beginning [Fall 2021 - EPIB-607-001 - Inferential Statistics].\nneed time troubleshoot problem topic, feel free\nsetup appointment TAs. best help.\ninstructor teaching assistants make every effort respond \nemails/Slack messages within 48 hours receipt. Emails/Slack messages\nreceived 5:00pm EST weekends treated though received\nfollowing business day.","code":""},{"path":"syllabus.html","id":"ta-information","chapter":"Syllabus","heading":"TA Information","text":"assigned specific TA based last name. Throughout \nterm, assigned TA one marking assignments. Issues marks assignments\ndiscussed TA first. required attend office hours TA. exceptional circumstances , can attend office hours another TA. \nTA office hours held virtually (link posted myCourses).Chinchin Wang\nchinchin.wang@mail.mcgill.ca\nOffice hours: Tuesdays 9:00am - 10:00am\nStudents last names beginning : Aa - En\nChinchin Wangchinchin.wang@mail.mcgill.cachinchin.wang@mail.mcgill.caOffice hours: Tuesdays 9:00am - 10:00amOffice hours: Tuesdays 9:00am - 10:00amStudents last names beginning : Aa - EnStudents last names beginning : Aa - EnMariam El Sheikh\nmariam.elsheikh@mail.mcgill.ca\nOffice hours: Mondays 12:00pm - 1:00pm\nStudents last names beginning : Er - Lu\nMariam El Sheikhmariam.elsheikh@mail.mcgill.camariam.elsheikh@mail.mcgill.caOffice hours: Mondays 12:00pm - 1:00pmOffice hours: Mondays 12:00pm - 1:00pmStudents last names beginning : Er - LuStudents last names beginning : Er - LuTing Zhang\nting.zhang3@mail.mcgill.ca\nOffice hours: Fridays 10:00am - 11:00am\nStudents last names beginning : Ma - Sam\nTing Zhangting.zhang3@mail.mcgill.cating.zhang3@mail.mcgill.caOffice hours: Fridays 10:00am - 11:00amOffice hours: Fridays 10:00am - 11:00amStudents last names beginning : Ma - SamStudents last names beginning : Ma - SamJingyan Fu\njingyan.fu@mail.mcgill.ca\nOffice hours: Thursdays 1:30pm - 2:30pm\nStudents last names beginning : Sar - Zh\nJingyan Fujingyan.fu@mail.mcgill.cajingyan.fu@mail.mcgill.caOffice hours: Thursdays 1:30pm - 2:30pmOffice hours: Thursdays 1:30pm - 2:30pmStudents last names beginning : Sar - ZhStudents last names beginning : Sar - Zh","code":""},{"path":"syllabus.html","id":"course-overview","chapter":"Syllabus","heading":"Course Overview","text":"Introduction basic principles statistical inference used \nclinical epidemiologic research. Topics include variability; methods\nprocessing describing data; sampling sampling distributions;\ninferences regarding means proportions, non-parametric methods,\nregression correlation.principal audience researchers natural social sciences\nhaven’t introductory course statistics (one \nlong time ago). audience accepts statistics penetrated \nlife sciences pervasively required knowledge \nresearch understanding scientific papers.","code":""},{"path":"syllabus.html","id":"instructor-message-regarding-course-delivery","chapter":"Syllabus","heading":"Instructor Message Regarding Course Delivery","text":"Lectures delivered person Tuesdays Thursdays 11:35am – 1:25pm. classroom crowded. August 25, 2021, 90 students registered class room assigned seats 96 full capacity, allowance distancing. McGill’s guidelines require distancing, require vaccination, state neither notified someone class contracts Coronavirus disease, thus unnecessarily putting community greater risk Covid infection. decision challenged dozens legal experts McGill Faculty Law stress McGill legal obligation require vaccination protect community. decision implement vaccine mandate also highly criticized several experts epidemiology infectious disease McGill School Population Global Health strongly advocating community health risks requiring vaccination. McGill Association University Teachers also wants vaccine mandate. SSMU students’ union also organizing protest Wednesday demand vaccine mandate better protect students.McGill students fall regularly attending packed (often overcapacity) classes distancing, inadequate ventilation, faculty removing masks. learning situation occurring within broader Montreal context full vaccination rate among 18-24 year-olds low (63% present), children 12 vaccinated, masking regularly enforced public transit. response, additionally multiple public letters petitions signed thousands asking McGill implement vaccine requirement including faculty students McGill across Quebec. reason sharing information ensure sufficiently aware ongoing challenges surrounding McGill’s failure mandate vaccine help facilitate informed decision-making attending classes fall.sorry provide safer environment us class. McGill University clearly communicated instructors required teach person face potential punishment failure complete job duties.Lectures recorded McGill’s Lecture Recording Service Zoom, posted myCourses. feel comfortable classroom need quarantine exposure may choose participate course virtually watching recorded lectures. However, please note midterm final exams delivered person different classroom allows distancing online option provided. course, pandemic conditions change plans may also change.useful links Teaching Learning Services: -\nGuidelines Students Teaching, Learning, \nAssessment\n- Learning\nResources","code":""},{"path":"syllabus.html","id":"learning-outcomes","chapter":"Syllabus","heading":"Learning Outcomes","text":"aim course provide students basic principles \nstatistical inference can:Visualize/Analyze/Interpret data using statistical methods \nR statistical software program.Visualize/Analyze/Interpret data using statistical methods \nR statistical software program.Understand statistical results scientific paper.Understand statistical results scientific paper.Apply statistical methods research.Apply statistical methods research.Use methods learned course foundation \nadvanced biostatistics courses.Use methods learned course foundation \nadvanced biostatistics courses.","code":""},{"path":"syllabus.html","id":"instructional-method","chapter":"Syllabus","heading":"Instructional Method","text":"course follow Partially Flipped Classroom model: , students\nexpected engaged material coming class.\nallows instructor delegate delivery basic content \ndefinitions textbooks videos, enforces idea students\nsimply passive recipients information. approach \nallows professor focus valuable class time nurturing efficient\ndiscussions surrounding ideas within content, guiding\ninteractive exploration typical misconceptions, promoting\ncollaborative problem solving peers. Refer class schedule\nassigned readings.focus computation: Classic introductory statistics textbooks\nwritten time computers still infancy.\n, even newer editions heavily rely -hand computations\nlooking tables tail probabilities. take modern\napproach introduce computational methods statistics \nstatistical software program R. Assignments must submitted \nR Markdown format ensure reproducible results.Reliance EPIB613: course relies heavily material presented \nEPIB613. constant contact EPIB613 instructors ensure requisite\ncomputing knowledge needed complete assignments course covered timely\nmanner. present material covered EPIB613. assume\nfollowing material course. Students external departments responsible learning material (seek help classmate) case \nregistered EPIB613.","code":""},{"path":"syllabus.html","id":"required-course-materials","chapter":"Syllabus","heading":"Required Course Materials","text":"","code":""},{"path":"syllabus.html","id":"course-notes","chapter":"Syllabus","heading":"Course notes","text":"course website https://sahirbhatnagar.com/EPIB607/ contain required material.\nrequired textbook course. However, several references \nposted myCourses.","code":""},{"path":"syllabus.html","id":"equipment","chapter":"Syllabus","heading":"Equipment","text":"Hand calculators (square root, log, exponential function) \nrequired. Laptops -class exercises can useful \nrequired.","code":""},{"path":"syllabus.html","id":"software","chapter":"Syllabus","heading":"Software","text":"R RStudio. installation instructions available https://sahirbhatnagar.com/EPIB607/install.html","code":""},{"path":"syllabus.html","id":"tutorials-from-datacamp","chapter":"Syllabus","heading":"Tutorials from DataCamp","text":"class supported DataCamp, allow learn R\ncombination short expert videos hands---keyboard\nexercises. asked complete courses DataCamp\nbackground reading assignments. can sign free\naccount \nlink.\nNote: required sign @mail.mcgill.ca @mcgill.ca\nemail address.","code":""},{"path":"syllabus.html","id":"course-content","chapter":"Syllabus","heading":"Course Content","text":"","code":""},{"path":"syllabus.html","id":"descriptive-statistics","chapter":"Syllabus","heading":"Descriptive Statistics","text":"Histograms, density plots, measures center, boxplots, standard\ndeviationHistograms, density plots, measures center, boxplots, standard\ndeviationData visualization (aesthetics, visual cues, coordinate systems,\nscales, facets layers)Data visualization (aesthetics, visual cues, coordinate systems,\nscales, facets layers)Choosing color palettes: Cynthia Brewer palettes, perceptually\nuniform palettes, color blind friendly palettes.Choosing color palettes: Cynthia Brewer palettes, perceptually\nuniform palettes, color blind friendly palettes.Tidy dataTidy data","code":""},{"path":"syllabus.html","id":"sampling-distributions","chapter":"Syllabus","heading":"Sampling Distributions","text":"Parameters statisticsParameters statisticsStandard error meanStandard error meanNormal (Gaussian) distributionNormal (Gaussian) distributionCentral Limit TheoremCentral Limit TheoremConfidence intervalsConfidence intervalsBootstrap sampling distributions confidence intervalsBootstrap sampling distributions confidence intervals","code":""},{"path":"syllabus.html","id":"one-sample-inference","chapter":"Syllabus","heading":"One Sample Inference","text":"Inference population meanInference population meanP values, power, sample size considerationsP values, power, sample size considerationsInference population proportionInference population proportionInference population rateInference population rate","code":""},{"path":"syllabus.html","id":"regression","chapter":"Syllabus","heading":"Regression","text":"Linear regression means, difference means, ratio meansLinear regression means, difference means, ratio meansPoisson regression rates, rate differences, rate ratiosPoisson regression rates, rate differences, rate ratiosLogistic regression odds ratios risk ratiosLogistic regression odds ratios risk ratios","code":""},{"path":"syllabus.html","id":"evaluation","chapter":"Syllabus","heading":"Evaluation","text":"final grade maximum :","code":""},{"path":"syllabus.html","id":"assignments","chapter":"Syllabus","heading":"Assignments","text":"assignments completed RMarkdown, submitted via Crowdmark. FAQ submit assignment via Crowdmark available https://crowdmark.com/help/completing--submitting--assessment/. sure submit one pdf per question. can compile .Rmd file HTML print pdf, can compile pdf directly (LaTeX installed).\nassignments consist completing DataCamp course. instances, required submit anything. able see completed assignment DataCamp instructor dashboard.\nExtensions assignments may granted upon request . Assignments submitted solutions posted accepted given grade 0.","code":""},{"path":"syllabus.html","id":"group-project","chapter":"Syllabus","heading":"Group Project","text":"objective group project construct exercise solutions suitable testing demonstrating understanding basic principles biostatistics discussed course.Exercises must based () one two articles scientific journal perhaps lay press\n(ii)  dataset. data must taken RA project, must \nfreely available web another public source. article data concern health\nproblem amenable statistical investigation. narrative exercise clear concise.\nexercise comprise 5-7 questions requiring altogether three hours completion. \nquestions may cover part course. must also produce separate set model answers;\nequally short point.group project evaluated using following criteria (total 10 points):choice subject ingenuity (2.0 points)Testing important biostatistical principles (2.5 points)Exercises clear, concise, creative. ’s better one question tests several concepts together, vs. several questions link (2.5 points)Quality solutions (2.0 points)report reproducible (1.0 points)Projects done groups 2 4 people.\nExamples final projects prepared students previous years posted MyCourses.\nprojects must uploaded myCourses. One submission per group.upload consist following:One .Rmd file containing questions solutions. must fully reproducible using techniques discussed class, .e., able download submission, open .Rmd file, compile without error. aware file paths hard coded solutions.One compiled .pdf .html file Rmarkdown documentAny article(s) questions basedAny data-sets used questions, text CSV format. dataset publicly available link dataset R package sufficient.","code":""},{"path":"syllabus.html","id":"midterm-and-final-exams","chapter":"Syllabus","heading":"Midterm and Final Exams","text":"possibility take midterm final exam alternative dates. \nattend midterm, final exam account 70% grade. Also note midterm final exams delivered person different classroom allows distancing online option provided. final grade consist letter grade. Students may request special accommodations Office Students Disabilities.event extraordinary circumstances beyond University’s control, content /evaluation scheme course subject change.","code":""},{"path":"syllabus.html","id":"mcgill-policy-statments","chapter":"Syllabus","heading":"McGill Policy Statments","text":"","code":""},{"path":"syllabus.html","id":"land-acknowledgement","chapter":"Syllabus","heading":"Land Acknowledgement","text":"McGill University located land long served site meeting exchange amongst Indigenous peoples, including Haudenosaunee (h oh - D EE - n oh - SH oh - n ee) Anishinabeg (Ah-nish-ih-nah’-bey) nations. McGill honours, recognizes respects nations traditional stewards lands waters meet today.connection land inextricably linked Indigenous identity. Historically, cultural protocol acknowledging traditional territory symbolizes importance place identity Indigenous peoples. Within many Indigenous communities, protocol requires individuals situate , relationships people land. many Indigenous peoples Canada, increasingly broader Canadian society, traditional territory acknowledgements important cultural protocol practised ceremonial events way acknowledge honour Indigenous peoples’ connections ancestral lands.Acknowledging traditional territory ensures:Recognition given land’s history order strengthen cultivate relationships local Indigenous communities;institution’s community exposed educated Indigenous histories, cultures, identities;welcoming space Indigenous students, staff faculty.","code":""},{"path":"syllabus.html","id":"language-of-submission","chapter":"Syllabus","heading":"Language of Submission","text":"accord McGill University’s Charter Students’ Rights, students\ncourse right submit English French \nwritten work graded. apply courses \nacquiring proficiency language one objectives.Conformément à la Charte des droits de l’étudiant de l’Université\nMcGill, chaque étudiant le droit de soumettre en français ou en\nanglais tout travail écrit devant être noté (sauf dans le cas des cours\ndont l’un des objets est la maîtrise d’une langue).","code":""},{"path":"syllabus.html","id":"academic-integrity","chapter":"Syllabus","heading":"Academic Integrity","text":"McGill University values academic integrity. Therefore, students\nmust understand meaning consequences cheating, plagiarism \nacademic offences Code Student Conduct \nDisciplinary Procedures (see\nwww.mcgill.ca/students/srr/honest/\ninformation).L’université McGill attache une haute importance à l’honnêteté\nacadémique. Il incombe par conséquent à tous les étudiants de comprendre\nce que l’entend par tricherie, plagiat et autres infractions\nacadémiques, ainsi que les conséquences que peuvent avoir de telles\nactions, selon le Code de conduite de l’étudiant et des procédures\ndisciplinaires (pour de plus amples renseignements, veuillez consulter\nle site\nwww.mcgill.ca/students/srr/honest/).","code":""},{"path":"syllabus.html","id":"text-matching-software","chapter":"Syllabus","heading":"Text Matching Software","text":"Work submitted evaluation part course may checked text matching software within myCourses.","code":""},{"path":"syllabus.html","id":"copyright","chapter":"Syllabus","heading":"Copyright","text":"Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) protected law may copied distributed form medium without explicit permission instructor. Note infringements copyright (https://www.mcgill.ca/secretariat/files/secretariat/policy_on_copyright.pdf) can subject follow University Code Student Conduct Disciplinary Procedures.","code":""},{"path":"syllabus.html","id":"zoom-recordings-of-class-lectures","chapter":"Syllabus","heading":"Zoom Recordings of Class Lectures","text":"Please read Guidelines Instructors Students Teaching, Learning, Assessment course outline. notified “pop-” box Zoom part class recorded (see image ). remaining sessions recorded, agree recording, understand image, voice, name may disclosed classmates. also understand recordings made available myCourses students registered course.University committed maintaining teaching learning spaces respectful inclusive . end, offensive, violent, harmful language arising contexts following may cause disciplinary action:Zoom sessions, including Username (use legal preferred name), virtual backgrounds, “chat” boxes, whiteboard annotations, breakout roomsmyCourses discussion foraSlack channels messages","code":""},{"path":"syllabus.html","id":"student-wellness","chapter":"Syllabus","heading":"Student Wellness","text":"instructor course endeavor provide inclusive learning environment. However, experience barriers learning course, hesitate discuss Office Students Disabilities, 514-398-6009.Many students may face mental health challenges can impact academic success also ability thrive campus community. Please reach support need ; many resources available -campus, -campus online.difficulty affording food lack safe stable place live believe may affect performance course, encourage contact Dean Students can connect support services. feel comfortable , please let know well, can discuss can best support learning.","code":""},{"path":"syllabus.html","id":"course-evaluations","chapter":"Syllabus","heading":"Course Evaluations","text":"End--course evaluations one ways McGill works towards maintaining improving quality courses student’s learning experience. notified e-mail evaluations available. Please note minimum number responses must received results available students.","code":""},{"path":"syllabus.html","id":"sustainability","chapter":"Syllabus","heading":"Sustainability","text":"McGill entering new exciting chapter sustainability campus. research operations, sustainable solutions local global challenges innovated levels University. centre initiatives, guiding way, McGill University Climate & Sustainability Strategy 2020-2025.Strategy commits McGill three ambitious, yet realistic long-term targets; attaining Platinum sustainability rating 2030, becoming zero-waste 2035, achieving carbon neutrality 2040. responsibility members global community can achieve goals reduce impact climate change contribute positively sustainability planet.support achievement three long-term targets, content Strategy examines University’s activities across eight categories: Research & Education, Buildings & Utilities, Waste Management, Travel & Commuting, Food Systems, Procurement, Landscapes & Ecosystems, Community Building. category headlined one flagship action, another 54 complimentary actions distributed throughout Strategy. Additionally, category highlights ways students, staff, faculty can get involved contribute sustainable McGill.Everyone McGill role play. changing personal habit leading large-scale project, can take part addressing urgency climate change contributing sustainable future.","code":""},{"path":"introdata.html","id":"introdata","chapter":"1 Introduction to Data","heading":"1 Introduction to Data","text":"section adapted Introduction Statistics Life Biomedical Sciences2Packages used Section","code":"\npacman::p_load(\n  openintro,\n  oibiostat, # devtools::install_github(\"OI-Biostat/oi_biostat_data\")\n  ggplot2,\n  ggpubr,\n  DT,\n  kableExtra\n)"},{"path":"introdata.html","id":"leapCaseStudy","chapter":"1 Introduction to Data","heading":"1.1 Case Study: preventing peanut allergies","text":"proportion young children Western countries peanut allergies doubled last 10 years. Previous research suggests exposing infants peanut-based foods, rather excluding foods diets, may effective strategy preventing development peanut allergies. Learning Early Peanut Allergy (LEAP) study conducted investigate whether early exposure peanut products reduces probability child develop peanut allergies.3The study team enrolled children United Kingdom 2006 2009, selecting 640 infants eczema, egg allergy, . child randomly assigned either peanut consumption (treatment) group peanut avoidance (control) group. Children treatment group fed least 6 grams peanut protein daily 5 years age, children control group avoided consuming peanut protein 5 years age.5 years age, child tested peanut allergy using oral food challenge (OFC): 5 grams peanut protein single dose. child recorded passing oral food challenge allergic reaction detected, failing oral food challenge allergic reaction occurred. children previously tested peanut allergy skin test, conducted time study entry; main analysis presented paper based data 530 children earlier negative skin test. Although total 542 children earlier negative skin test, data collection occur 12 children.Individual-level data study shown Figure 1.1. row represents participant shows participant’s study ID number, treatment group assignment, OFC outcome. data available oibiostat R package.\nFigure 1.1: Individual-level LEAP results\ndata can organized form two-way summary table; Table 1.1 shows results categorized treatment group OFC outcome.\nTable 1.1: Summary LEAP results, organized treatment group (either peanut avoidance consumption) result oral food challenge 5 years age (either pass fail)\nsummary Table 1.1 makes easier identify patterns data. Recall question interest whether children peanut consumption group less likely develop peanut allergies peanut avoidance group. avoidance group, proportion children failing OFC \\(36/263 = 0.137\\) (13.7%); consumption group, proportion children failing OFC \\(5/267 = 0.019\\) (1.9%). Figure 1.2 shows graphical method displaying study results, using either number individuals per category Table 1.1 proportion individuals specific OFC outcome group.\nFigure 1.2: (top) bar plot displaying number individuals failed passed OFC treatment group. (bottom) bar plot displaying proportions individuals group failed passed OFC.\nproportion participants failing OFC 11.8% higher peanut avoidance group peanut consumption group. Another way summarize data compute ratio two proportions (0.137/0.019 = 7.31), conclude proportion participants failing OFC avoidance group 7 times large consumption group; .e., risk failing OFC 7 times great participants avoidance group relative consumption group.Based results study, seems early exposure peanut products may effective strategy reducing chances developing peanut allergies later life. important note study conducted United Kingdom single site pediatric care; clear results can generalized countries cultures.results also raise important statistical issue: study provide definitive evidence peanut consumption beneficial? words, 11.8% difference two groups larger one expect chance variation alone? material inference later chapters provide statistical tools evaluate question.","code":"\ndata(LEAP)\nstats::addmargins(base::table(LEAP$treatment.group, LEAP$overall.V60.outcome)) \nggplot(data = LEAP, aes(x = treatment.group, fill = overall.V60.outcome)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +\n theme(legend.position = \"top\") + ggpubr::theme_pubr() + \n  scale_fill_openintro()\n\nggplot(data = LEAP, aes(x = treatment.group, fill = overall.V60.outcome)) + \n  geom_bar(position = position_fill(reverse = TRUE)) +\n theme(legend.position = \"top\") + ggpubr::theme_pubr() + \n  scale_fill_openintro()"},{"path":"introdata.html","id":"dataBasics","chapter":"1 Introduction to Data","heading":"1.2 Data basics","text":"Effective organization description data first step \nanalyses. section introduces structure organizing data \nbasic terminology used describe data.","code":""},{"path":"introdata.html","id":"frogDataExample","chapter":"1 Introduction to Data","heading":"1.2.1 Observations, variables, and data matrices","text":"evolutionary biology, parental investment refers amount time, energy, resources devoted towards raising offspring. section introduces frog dataset, originates 2013 study maternal investment frog species.4 Reproduction costly process female frogs, necessitating trade-individual egg size total number eggs produced. Researchers interested investigating maternal investment varies altitude collected measurements egg clutches found breeding ponds across 11 study sites; 5 sites, body size individual female frogs also recorded.\nFigure 1.3: Data matrix frog dataset.\nFigure 1.3 displays rows 1, 2, 3, 150 data 431 clutches\nobserved part study. frog dataset available oibiostat R package. row table corresponds single clutch, indicating clutch collected (altitude latitude), egg.size, clutch.size, clutch.volume, body.size mother available. empty cell corresponds missing value, indicating information individual female collected particular clutch. recorded characteristics referred variables; table, column represents variable.important check definitions variables, always obvious. example, clutch.size recorded whole numbers? given clutch, researchers counted approximately 5 grams’ worth eggs estimated total number eggs based mass entire clutch. Definitions variables given Table .data Figure 1.3 form data frame , example, organized tidy format. row tidy data frame corresponds observational unit, column corresponds variable. piece data frame LEAP study introduced Section 1.1 shown Figure 1.1; rows study participants three variables shown participant. Tidy data frames convenient way record store data. data collected another individual, another row can easily added; similarly, another column can added new variable.","code":""},{"path":"introdata.html","id":"variableTypes","chapter":"1 Introduction to Data","heading":"1.2.2 Types of variables","text":"Functional polymorphisms Associated human Muscle Size Strength study (FAMuSS) measured variety demographic, phenotypic, genetic characteristics 1,300 participants.5 Data study used number subsequent studies, one examining relationship muscle strength genotype location ACTN3 gene.6The famuss dataset subset data 595 participants.7 famuss dataset oibiostat package shown Figure 1.4, variables described Table .\nFigure 1.4: Data matrix famuss dataset.\nvariables age, height, weight, ndrm.ch numerical variables. take numerical values, reasonable add, subtract, take averages values. contrast, variable reporting telephone numbers classified numerical, since sums, differences, averages context meaning. Age measured years said discrete, since can take numerical values jumps; .e., positive integer values. Percent change strength non-dominant arm (ndrm.ch) continuous, can take value within specified range.variables sex, race, actn3.r577x categorical variables, take values names labels. possible values categorical variable called variable’s levels. example, levels actn3.r577x three possible genotypes particular locus: CC, CT, TT. Categorical variables without natural ordering called nominal categorical variables; sex, race, actn3.r577x nominal categorical variables. Categorical variables levels natural ordering referred ordinal categorical variables. example, age participants grouped 5-year intervals (15-20, 21-25, 26-30, etc.) ordinal categorical variable.Categorical variables sometimes called factor variables.\nFigure 1.5: Breakdown variables respective types.\n","code":"\ndata(\"famuss\")"},{"path":"introdata.html","id":"variableRelations","chapter":"1 Introduction to Data","heading":"1.2.3 Relationships between variables","text":"Many studies motivated researcher examining two variables related. example, values one variable increase values another decrease? values one variable tend differ levels another variable?One study used famuss data investigate whether ACTN3 genotype particular location (residue 577) associated change muscle strength. ACTN3 gene codes protein involved muscle function. common mutation gene specific location changes cytosine (C) nucleotide thymine (T) nucleotide; individuals TT genotype unable produce ACTN3 protein.Researchers hypothesized genotype location might influence muscle function. measure muscle function, recorded percent change non-dominant arm strength strength training; variable, ndrm.ch, response variable study. response variable defined particular research question study seeks address, measures outcome interest study. study typically examine whether values response variable differ values explanatory variable change, , two variables related. given study may examine several explanatory variables single response variable. explanatory variable examined relation ndrm.ch study actn3.r557x, ACTN3 genotype location 577.Response variables sometimes called dependent variables explanatory variables often called independent variables predictors.","code":""},{"path":"introdata.html","id":"data-collection-principles","chapter":"1 Introduction to Data","heading":"1.3 Data collection principles","text":"first step research identify questions investigate. clearly articulated research question essential selecting subjects studied, identifying relevant variables, determining data collected.","code":""},{"path":"introdata.html","id":"populations-and-samples","chapter":"1 Introduction to Data","heading":"1.3.1 Populations and samples","text":"Consider following research questions:bluefin tuna Atlantic Ocean particularly high levels mercury, unsafe human consumption?infants predisposed developing peanut allergy, evidence introducing peanut products early life effective strategy reducing risk developing peanut allergy?recently developed drug designed treat glioblastoma, form brain cancer, appear effective inducing tumor shrinkage drug currently market?questions refers specific target population. example, first question, target population consists bluefin tuna Atlantic Ocean; individual bluefin tuna represents case. almost always either expensive logistically impossible collect data every case population. result, nearly research based information obtained sample population. sample represents small fraction population. Researchers interested evaluating mercury content bluefin tuna Atlantic Ocean collect sample 500 bluefin tuna (quantity), measure mercury content, use observed information formulate answer research question.","code":""},{"path":"introdata.html","id":"anecdotal-evidence","chapter":"1 Introduction to Data","heading":"1.3.2 Anecdotal evidence","text":"Anecdotal evidence typically refers unusual observations easily recalled striking characteristics. Physicians may likely remember characteristics single patient unusually good response drug instead many patients respond. dangers drawing general conclusions anecdotal information obvious; single observation used draw conclusions population.incorrect generalize individual observations, unusual observations can sometimes valuable. E.C. Heyde general practitioner Vancouver noticed elderly patients aortic-valve stenosis (abnormal narrowing) caused accumulation calcium also suffered massive gastrointestinal bleeding. 1958, published observation.8 research led identification underlying cause association, now called Heyde’s Syndrome.9An anecdotal observation can never basis conclusion, may well inspire design systematic study definitive.","code":""},{"path":"introdata.html","id":"sampling-from-a-population","chapter":"1 Introduction to Data","heading":"1.3.3 Sampling from a population","text":"Sampling population, done correctly, provides reliable information characteristics large population. US Centers Disease Control (US CDC) conducts several surveys obtain information US population, including Behavior Risk Factor Surveillance System (BRFSS) (https://www.cdc.gov/brfss/index.html). BRFSS established 1984 collect data health-related risk behaviors, now collects data 400,000 telephone interviews conducted year. CDC conducts similar surveys diabetes, health care access, immunization. Likewise, World Health Organization () conducts World Health Survey partnership approximately 70 countries learn health adult populations health systems countries (http://www..int/healthinfo/survey/en/).general principle sampling straightforward: sample population useful learning population sample representative population. words, characteristics sample correspond characteristics population.Suppose quality improvement team integrated health care system, Harvard Pilgrim Health Care, interested learning members health plan perceive quality services offered plan. common pitfall conducting survey use convenience sample, individuals easily accessible likely included sample individuals. sample collected approaching plan members visiting outpatient clinic particular week, sample fail enroll generally healthy members typically use outpatient services schedule routine physical examinations; method produce unrepresentative sample (Figure 1.6).\nFigure 1.6: Instead sampling members equally, approaching members visiting clinic particular week disproportionately selects members frequently use outpatient services.\nRandom sampling best way ensure sample reflects population. simple random sample (SRS), member population chance sampled. One way achieve simple random sample health plan members randomly select certain number names complete membership roster, contact individuals interview (Figure 1.7).\nFigure 1.7: Five members randomly selected population interviewed.\nEven simple random sample taken, guaranteed sample representative population. non-response rate survey high, may indicative biased sample. Perhaps majority participants respond survey certain group within population reached; example, questions assume participants fluent English, high non-response rate expected population largely consists individuals fluent English\n(Figure 1.8). non-response bias can skew results; generalizing unrepresentative sample may likely lead incorrect conclusions population.\nFigure 1.8: Surveys may reach certain group within population, leads non-response bias. example, survey written English may result responses health plan members fluent English.\n","code":""},{"path":"introdata.html","id":"sampling-methods","chapter":"1 Introduction to Data","heading":"1.3.4 Sampling methods","text":"Almost statistical methods based notion implied randomness. data sampled population random, statistical methods – calculating estimates errors associated estimates – reliable. Four random sampling methods discussed section: simple, stratified, cluster, multistage sampling.simple random sample, case population equal chance included sample (Figure 1.9). simple random sampling, case sampled independently cases; .e., knowing certain case included sample provides information cases also sampled.stratified sampling, population first divided groups called strata cases selected within stratum (typically simple random sampling) (Figure 1.9). strata chosen similar cases grouped together. Stratified sampling especially useful cases stratum similar respect outcome interest, cases strata might quite different.Suppose health care provider facilities different cities. range services offered differ city, locations given city offer similar services, effective quality improvement team use stratified sampling identify participants study, city represents stratum plan members randomly sampled city.\nFigure 1.9: Examples simple random stratified sampling. top panel, simple random sampling used randomly select 18 cases (circled orange dots) total population (dots). bottom panel illustrates stratified sampling: cases grouped six strata, simple random sampling employed within stratum.\ncluster sample, population first divided many groups, called clusters. , fixed number clusters sampled observations clusters included sample (Figure 1.10, top panel). multistage sample similar cluster sample, rather keeping observations cluster, random sample collected within selected cluster (Figure 1.10, bottom panel).\nFigure 1.10: Examples cluster cluster sampling multistage sampling. top panel illustrates cluster sampling: data binned nine clusters, three sampled, observations within clusters sampled. bottom panel illustrates multistage sampling, differs cluster sampling subset three selected clusters sampled.\nUnlike stratified sampling, cluster multistage sampling helpful high case--case variability within cluster, clusters similar one another. example, neighborhoods city represent clusters, cluster multistage sampling work best population within neighborhood diverse, neighborhoods relatively similar.Applying stratified, cluster, multistage sampling can often economical drawing random samples. However, analysis data collected using methods complicated using data simple random sample; text discuss analysis methods simple random samples.","code":""},{"path":"introdata.html","id":"introducing-experiments-and-observational-studies","chapter":"1 Introduction to Data","heading":"1.3.5 Introducing experiments and observational studies","text":"two primary types study designs used collect data experiments observational studies.experiment, researchers directly influence data arise, assigning groups individuals different treatments assessing outcome varies across treatment groups. LEAP study example experiment two groups, experimental group received intervention (peanut consumption) control group received standard approach (peanut avoidance). studies assessing effectiveness new drug, individuals control group typically receive placebo, inert substance appearance experimental intervention. study designed average, difference individuals treatment groups whether consumed peanut protein. allows observed differences experimental outcome directly attributed intervention constitute evidence causal relationship intervention outcome.observational study, researchers merely observe record data, without interfering data arise. example, investigate certain diseases develop, researchers might collect data conducting surveys, reviewing medical records, following cohort many similar individuals. Observational studies can provide evidence association variables, show causal connection. However, many instances randomized experiments unethical, explore whether lead exposure young children associated cognitive impairment.","code":""},{"path":"introdata.html","id":"experiments","chapter":"1 Introduction to Data","heading":"1.3.6 Experiments","text":"Experimental design based three principles: control, randomization, replication.Control: selecting participants study, researchers work control extraneous variables choose sample participants representative population interest. example, participation study might restricted individuals condition suggests may benefit intervention tested. Infants enrolled LEAP study required 4 11 months age, severe eczema /allergies eggs.Randomization: Randomly assigning patients treatment groups ensures groups balanced respect variables can controlled. example, randomization LEAP study ensures proportion males females approximately groups. Additionally, perhaps infants susceptible peanut allergy undetected genetic condition; randomization, reasonable assume infants present equal numbers groups. Randomization allows differences outcome groups reasonably attributed treatment rather inherent variability patient characteristics, since treatment represents systematic difference two groups.situations researchers suspect variables intervention may influence response, individuals can first grouped blocks according certain attribute randomized treatment group within block; technique referred blocking stratification. team behind LEAP study stratified infants two cohorts based whether child developed red, swollen mark (wheal) skin test time enrollment; afterwards, infants randomized peanut consumption avoidance groups. Figure 1.11 illustrates blocking scheme used study.Replication: results study conducted larger number cases generally reliable smaller studies; observations made large sample likely representative population interest. single study, replication accomplished collecting sufficiently large sample. LEAP study randomized total 640 infants.Randomized experiments essential tool research. US Food Drug Administration typically requires new drug can marketed two independently conducted randomized trials confirm safety efficacy; European Medicines Agency similar policy. Large randomized experiments medicine provided basis major public health initiatives. 1954, approximately 750,000 children participated randomized study comparing polio vaccine placebo.10 United States, results study quickly led widespread successful use vaccine polio prevention.\nFigure 1.11: simplified schematic blocking scheme used LEAP study, depicting 640 patients underwent randomization. Patients first divided blocks based response initial skin test, block randomized avoidance consumption groups. strategy ensures even representation patients group positive negative skin tests.\n","code":""},{"path":"introdata.html","id":"observational-studies","chapter":"1 Introduction to Data","heading":"1.3.7 Observational studies","text":"observational studies, researchers simply observe selected potential explanatory response variables. Participants differ important explanatory variables may also differ ways influence response; result, advisable make causal conclusions relationship explanatory response variables based observational data. example, observational studies obesity shown obese individuals tend die sooner individuals normal weight, misleading conclude obesity causes shorter life expectancy. Instead, underlying factors probably involved; obese individuals typically exhibit health behaviors influence life expectancy, reduced exercise unhealthy diet.Suppose observational study tracked sunscreen use incidence skin cancer, found sunscreen person uses, likely skin cancer. results mean sunscreen causes skin cancer. One important piece missing information sun exposure – someone often exposed sun, likely use sunscreen contract skin cancer (Figure 1.12). Sun exposure confounding variable: variable associated explanatory response variables (also called lurking variable, confounding factor, confounder). guarantee confounding variables can examined measured; result, advisable draw causal conclusions observational studies.\nFigure 1.12: Confounding triangle example.\nConfounding limited observational studies. example, consider randomized study comparing two treatments (varenicline buproprion) placebo therapies aiding smoking cessation.11 beginning study, participants randomized groups: 352 varenicline, 329 buproprion, 344 placebo. participants successfully completed assigned therapy: 259, 225, 215 patients group , respectively. analysis based participants completed therapy, introduce confounding; possible underlying differences individuals complete therapy . Including randomized participants final analysis maintains original randomization scheme controls differences groups. strategy, commonly used analyzing clinical trial data, referred intention--treat analysis.Observational studies may reveal interesting patterns associations can investigated follow-experiments. Several observational studies based dietary data different countries showed strong association dietary fat breast cancer women. observations led launch Women’s Health Initiative (WHI), large randomized trial sponsored US National Institutes Health (NIH). WHI, women randomized standard versus low fat diets, previously observed association confirmed.Observational studies can either prospective retrospective. prospective study identifies participants collects information scheduled times events unfold. example, Nurses’ Health Study, researchers recruited registered nurses beginning 1976 collected data administering biennial surveys; data study used investigate risk factors major chronic diseases women. Retrospective studies collect data events taken place, medical records. datasets may contain retrospectively- prospectively-collected variables. Cancer Care Outcomes Research Surveillance Consortium (CanCORS) enrolled participants lung colorectal cancer, collected information diagnosis, treatment, previous health behavior, also maintained contact participants gather data long-term outcomes.12","code":""},{"path":"introdata.html","id":"numerical-data","chapter":"1 Introduction to Data","heading":"1.4 Numerical data","text":"section discusses techniques exploring summarizing numerical variables, using frog data parental investment study introduced Section 1.2.","code":""},{"path":"introdata.html","id":"measures-of-center-mean-and-median","chapter":"1 Introduction to Data","heading":"1.4.1 Measures of center: mean and median","text":"mean, sometimes called average, measure center distribution data. find average clutch volume observed egg clutches, add clutch volumes divide total number clutches. computational convenience, volumes rounded first~decimal.\\[\n\\overline{y} = \\frac{177.8 + 257.0 + \\cdots + 933.3}{431} = 882.5\\ \\textrm{mm}^{3}.\n\\]\nsample mean often labeled \\(\\overline{y}\\), distinguish \\(\\mu\\), mean entire population sample drawn. letter \\(y\\) used generic placeholder variable interest, clutch.volume.Definition 1.1  (Mean) sample mean numerical variable sum values observations divided number observations:\n\\[\\overline{y} = \\frac{y_1+y_2+\\cdots+y_n}{n},\\]\n\\(y_1, y_2, \\dots, y_n\\) represent \\(n\\) observed values.median another measure center; middle number distribution values ordered smallest largest. distribution contains even number observations, median average middle two observations. 431 clutches dataset, median clutch volume \\(216^{th}\\) observation sorted values clutch.volume: \\(831.8\\ \\textrm{mm}^{3}\\).","code":""},{"path":"introdata.html","id":"measures-of-spread-standard-deviation-and-interquartile-range","chapter":"1 Introduction to Data","heading":"1.4.2 Measures of spread: standard deviation and interquartile range","text":"spread distribution refers similar varied values distribution ; .e., whether values tightly clustered spread wide range.standard deviation set data describes typical distance observation mean. distance single observation mean deviation. deviations \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), \\(431^{st}\\) observations clutch.volume variable.\\[\\begin{align*}\ny_1-\\overline{y} &= 177.8 - 882.5 = -704.7 \\hspace{5mm}\\text{ } \\\\\ny_2-\\overline{y} &= 257.0 - 882.5 = -625.5 \\\\\ny_3-\\overline{y} &= 151.4 - 882.5 = -731.1 \\\\\n&\\ \\vdots \\\\\ny_{431}-\\overline{y} &= 933.2 - 882.5 = 50.7\n\\end{align*}\\]sample variance, average squares deviations, denoted \\(s^2\\):\\[\\begin{align*}\ns^2 &= \\frac{(-704.7)^2 + (-625.5)^2 + (-731.1)^2 + \\cdots + (50.7)^2}{431-1} \\\\\n&= \\frac{496,602.09 + 391,250.25 + 534,507.21 + \\cdots + 2570.49}{430} \\\\\n&= 143,680.9.\n\\end{align*}\\]denominator \\(n-1\\) rather \\(n\\); mathematical nuance accounts fact sample mean used estimate population mean calculation. Details statistical theory can found advanced texts.sample standard deviation \\(s\\) square root variance:\n\\[s=\\sqrt{143,680.9} = 379.05 \\textrm{mm}^{3}.\\]Like mean, population values variance standard deviation denoted Greek letters:\n\\(\\sigma_{}^2\\) variance \\(\\sigma\\) standard deviation.Definition 1.2  (Standard Deviation) sample standard deviation numerical variable computed square root variance, sum squared deviations divided number observations minus 1.\n\\[\\begin{eqnarray}\ns = \\sqrt{\\frac{({y_1 - \\overline{y})}^{2}+({y_2 - \\overline{y})}^{2}+\\cdots+({y_n - \\overline{y})}^{2}}{n-1}},\n\\label{SDEquation}\n\\end{eqnarray}\\]\n\\(y_1, y_2, \\dots, y_n\\) represent \\(n\\) observed values.Variability can also measured using interquartile range (IQR). IQR distribution difference first third quartiles: \\(Q_3 - Q_1\\). first quartile (\\(Q_1\\)) equivalent 25\\(^{th}\\) percentile; .e., 25% data fall value. third quartile (\\(Q_3\\)) equivalent 75\\(^{th}\\) percentile. definition, median represents second quartile, half values falling half falling . IQR clutch.volume \\(1096.0 - 609.6 = 486.4\\ \\textrm{mm}^{3}\\).Measures center spread ways summarize distribution numerically. Using numerical summaries allows distribution efficiently described numbers. Numerical summaries also known summary statistics. example, calculations clutch.volume indicate typical egg clutch volume 880 mm\\(^3\\), middle \\(50\\%\\) egg clutches volumes approximately \\(600\\ \\textrm{mm}^{3}\\) \\(1100.0\\ \\textrm{mm}^{3}\\).","code":""},{"path":"introdata.html","id":"robust-estimates","chapter":"1 Introduction to Data","heading":"1.4.3 Robust estimates","text":"Figure 1.13 shows values clutch.volume points single axis. values seem extreme relative observations: four largest values, appear distinct rest distribution. extreme values affect value numerical summaries?\nFigure 1.13: Dot plot clutch volumes frog oibiostat package data.\nFigure 1.14 shows summary statistics calculated two scenarios, one one without four largest observations. data, median change, IQR differs 6 \\(\\textrm{mm}^{3}\\). contrast, mean standard deviation much affected, particularly standard deviation.\nFigure 1.14: comparison median, IQR, mean (\\(\\overline{y}\\)), standard deviation (\\(s\\)) change extreme observations present.\nmedian IQR referred robust estimates extreme observations little effect values. distributions contain extreme values, median IQR provide accurate sense center spread mean standard deviation.","code":""},{"path":"introdata.html","id":"visualizing-distributions-of-data-histograms-and-boxplots","chapter":"1 Introduction to Data","heading":"1.4.4 Visualizing distributions of data: histograms and boxplots","text":"Graphs show important features distribution evident numerical summaries, asymmetry extreme values. dot plots show exact value observation, histograms boxplots graphically summarize distributions.histogram, observations grouped bins plotted bars. Figure 1.15 shows number clutches volume 0 200 \\(\\textrm{mm}^{3}\\), 200 400 \\(\\textrm{mm}^{3}\\), etc. 2,600 2,800 \\(\\textrm{mm}^{3}\\). Note: default R, bins left-open right-closed; .e., intervals form (, b]. Thus, observation value 200 fall 0-200 bin instead 200-400 bin.} binned counts plotted Figure 1.16.\nFigure 1.15: counts binned ar{clutch.volume} data.\n\nFigure 1.16: histogram clutch.volume.\nHistograms provide view data density. Higher bars indicate frequent observations, lower bars represent relatively rare observations. Figure 1.16 shows egg clutches volumes 500-1,000 mm\\(^3\\), many clutches volumes smaller 1,000 mm\\(^{3}\\) clutches larger volumes.Histograms show shape distribution. tails symmetric distribution roughly equal, data trailing center roughly equally directions. Asymmetry arises one tail distribution longer . distribution said right skewed data trail right, left skewed data trail left. ways describe data skewed right/left: skewed right/left skewed positive/negative end. Figure 1.16 shows distribution clutch volume right skewed; clutches relatively small volumes, clutches high volumes.mode represented prominent peak distribution. Another definition mode, typically used statistics, value occurrences. common dataset contains observations value, makes definition impractical many datasets. Figure 1.17 shows histograms one, two, three major peaks. distributions called unimodal, bimodal, multimodal, respectively. distribution two prominent peaks called multimodal. Note less prominent peak unimodal distribution counted since differs neighboring bins observations. Prominent subjective term, usually clear histogram major peaks .\nFigure 1.17: left right: unimodal, bimodal, multimodal distributions.\nboxplot indicates positions first, second, third quartiles distribution addition extreme observations. Boxplots also known box--whisker plots. Figure 1.18 shows boxplot clutch.volume alongside vertical dot plot.\nFigure 1.18: boxplot dot plot clutch.volume. horizontal dashes indicate bottom 50% data open circles represent top 50%.\nboxplot, interquartile range represented rectangle extending first quartile third quartile, rectangle split median (second quartile). Extending outwards box, whiskers capture data fall \\(Q_1 - 1.5\\times IQR\\) \\(Q_3 + 1.5\\times IQR\\). whiskers must end data points; values given adding subtracting \\(1.5\\times IQR\\) define maximum reach whiskers. example, clutch.volume variable, \\(Q_3 + 1.5 \\times IQR = 1,096.5 + 1.5\\times 486.4 = 1,826.1\\ \\textrm {mm}^{3}\\). However, clutch volume 1,826.1 \\(\\textrm {mm}^{3}\\); thus, upper whisker extends 1,819.7 \\(\\textrm {mm}^{3}\\), largest observation smaller \\(Q_3 + 1.5\\times IQR\\).observation lies beyond whiskers shown dot; observations called outliers. outlier value appears extreme relative rest data. clutch.volume variable, several large outliers small outliers, indicating presence unusually large egg clutches.high outliers Figure 1.18 reflect right-skewed nature data. right skew also observable position median relative first third quartiles; median slightly closer first quartile. symmetric distribution, median halfway first third quartiles.","code":""},{"path":"introdata.html","id":"transforming-data","chapter":"1 Introduction to Data","heading":"1.4.5 Transforming data","text":"working strongly skewed data, can useful apply transformation, rescale data using function. natural log transformation commonly used clarify features variable many values clustered near zero observations positive.\nFigure 1.19: Histogram per capita income.\n\nFigure 1.20: Histogram log-transformed per capita income.\nexample, income data often skewed right; typically large clusters low moderate income, large incomes outliers. Figure 1.19 shows histogram average yearly per capita income measured US dollars 165 countries 2011. data available wdi.2011 R package oibiostat. data heavily right skewed, majority countries average yearly per capita income lower $10,000. data log-transformed, distribution becomes roughly symmetric (Figure 1.20). statistics, natural logarithm usually written \\(\\log\\). settings sometimes written \\(\\ln\\).symmetric distributions, mean standard deviation particularly informative summaries. distribution symmetric, approximately 70% data within one standard deviation mean 95% data within two standard deviations mean; guideline known empirical rule.Example 1.1  log-transformed scale, mean \\(\\log\\) income 8.50, standard deviation 1.54. Apply empirical rule describe distribution average yearly per capita income among 165 countries.According empirical rule, middle 70% data within one standard deviation mean, range (8.50 - 1.54, 8.50 + 1.54) = (6.96, 10.04) log(USD). 95% data within two standard deviations mean, range (8.50 - 2(1.54), 8.50 + 2(1.54)) = (5.42, 11.58) log(USD).Undo log transformation. middle 70% data within range\n\\[ (e^{6.96}, e^{10.04}) = (1054, 22925)\\]. middle 95% data within range\n\\[(e^{5.42}, e^{11.58}) = (226, 106937).\\]Functions natural log can also used transform data, square root inverse.","code":""},{"path":"introdata.html","id":"categorical-data","chapter":"1 Introduction to Data","heading":"1.5 Categorical data","text":"section introduces tables plots summarizing categorical data, using famuss dataset oibiostat package.table single variable called frequency table. Table 1.2 frequency table  variable, showing distribution genotype location r577x ACTN3 gene FAMuSS study participants.relative frequency table like Table @ref(tab:famussRelFrequencyTable}, proportions per category shown instead counts.\nTable 1.2: frequency table actn3.r577x variable.\n\nTable 1.3: relative frequency table actn3.r577x variable.\nbar plot common way display single categorical variable. left panel Figure 1.21 shows bar plot counts per genotype actn3.r577x variable. plot right panel shows proportion observations level (.e. genotype).\nFigure 1.21: Two bar plots actn3.r577x. left panel shows counts, right panel shows proportions genotype.\n","code":""},{"path":"introdata.html","id":"relationships-between-two-variables","chapter":"1 Introduction to Data","heading":"1.6 Relationships between two variables","text":"section introduces numerical graphical methods exploring summarizing relationships two variables. Approaches vary depending whether two variables numerical, categorical, whether one numerical one categorical.","code":""},{"path":"introdata.html","id":"two-numerical-variables","chapter":"1 Introduction to Data","heading":"1.6.1 Two numerical variables","text":"","code":""},{"path":"introdata.html","id":"scatterplots","chapter":"1 Introduction to Data","heading":"1.6.1.1 Scatterplots","text":"frog parental investment study, researchers used clutch volume primary variable interest rather egg size clutch volume represents eggs protective gelatinous matrix surrounding eggs. larger clutch volume, higher energy required produce ; thus, higher clutch volume indicative increased maternal investment. Previous research reported larger body size allows females produce larger clutches; idea supported frog data?scatterplot provides case--case view relationship two numerical variables. Figure 1.22 shows clutch volume plotted body size, clutch volume \\(y\\)-axis body size \\(x\\)-axis. point represents single case. example, case one egg clutch volume body size (female produced clutch) recorded.\nFigure 1.22: scatterplot showing clutch.volume (vertical axis) vs. body.size (horizontal axis).\nplot shows discernible pattern, suggests association, relationship, clutch volume body size; points tend lie straight line, indicative linear association. Two variables positively associated increasing values one tend occur increasing values ; two variables negatively associated increasing values one variable occurs decreasing values . evident relationship two variables, said uncorrelated independent.expected, clutch volume body size positively associated; larger frogs tend produce egg clutches larger volumes. observations suggest larger females capable investing energy offspring production relative smaller females.National Health Nutrition Examination Survey (NHANES) consists set surveys measurements conducted US CDC assess health nutritional status adults children United States. following example uses data sample 500 adults (individuals ages 21 older) NHANES dataset. sample available nhanes.samp.adult.500 oibiostat package.Example 1.2  Body mass index (BMI) measure weight commonly used health agencies assess whether someone overweight, calculated height weight. Describe relationships shown Figures 1.23 1.24. helpful use BMI measure obesity, rather weight?\n\\[BMI = \\dfrac{weight_{kg}}{height^{2}_m} = \\dfrac{weight_{lb}}{height^{2}_{}} \\times 703\\]Figure 1.23 shows positive association height weight; taller individuals tend heavier. Figure 1.24 shows height BMI seem associated; range BMI values observed roughly consistent across height.Weight good measure whether someone overweight; instead, reasonable consider whether someone’s weight unusual relative individuals comparable height. individual weighing 200 pounds 6 ft tall necessarily unhealthy weight; however, someone weighs 200 pounds 5 ft tall likely overweight. reasonable classify individuals overweight obese based weight.BMI acts relative measure weight accounts height. Specifically, BMI used estimate body fat. According US National Institutes Health (US NIH) World Health Organization (), BMI 25.0 - 29.9 considered overweight BMI 30 considered obese.\nFigure 1.23: scatterplot showing height versus weight 500 individuals sample NHANES. One participant 163.9 cm tall (5 ft, 4 ) weighing 144.6 kg (319 lb) highlighted.\n\nFigure 1.24: scatterplot showing height versus BMI 500 individuals sample NHANES. individual highlighted Figure 1.23 marked , BMI 53.83.\nExample 1.3  Figure 1.25 scatterplot life expectancy versus annual per capita income 165 countries 2011. Life expectancy measured expected lifespan children born 2011 income adjusted purchasing power country. Describe relationship life expectancy annual per capita income; seem linearly associated?Life expectancy annual per capita income positively associated; higher per capita income associated longer life expectancy. However, two variables linearly associated. income low, small increases per capita income associated relatively large increases life expectancy. However, per capita income exceeds approximately $20,000 per year, increases income associated smaller gains life expectancy.linear association, change \\(y\\)-variable every unit \\(x\\)-variable consistent across range \\(x\\)-variable; example, linear association present increase income $10,000 corresponded increase life expectancy 5 years, across range income.\nFigure 1.25: scatterplot life expectancy (years) versus annual per capita income (US dollars) wdi.2011 dataset oibiostat package.\n","code":""},{"path":"introdata.html","id":"correlation","chapter":"1 Introduction to Data","heading":"1.6.1.2 Correlation","text":"Correlation numerical summary statistic measures strength linear relationship two variables. denoted \\(r\\), takes values -1 1.paired values two variables lie exactly line, \\(r = \\pm 1\\); closer correlation coefficient \\(\\pm 1\\), stronger linear association. two variables positively associated, paired values tend lie line positive slope, \\(r > 0\\). two variables negatively associated, \\(r < 0\\). value \\(r\\) 0 approximately 0 indicates apparent association two variables.paired values lie perfectly either horizontal vertical line, association \\(r\\) mathematically undefined.correlation coefficient quantifies strength linear trend. Prior calculating correlation, advisable confirm data exhibit linear relationship. Although mathematically possible calculate correlation set paired observations, life expectancy versus income data Figure 1.25, correlation used assess strength nonlinear relationship.\nFigure 1.26: Scatterplots correlation coefficients. first row shows positive associations second row shows negative associations. left right, strength linear association \\(x\\) \\(y\\) increases.\nDefinition 1.3  (Correlation) correlation two variables \\(x\\) \\(y\\) given :\n\\[r =  \\frac{1}{n-1}\\sum^{n}_{=1}\n\\left(\\frac{x_{}-\\overline{x}}\n{s_{x}}\\right)\\left(\\frac{y_{}-\\overline{y}}{s_{y}}\\right),\\]\n\\((x_1,y_1), (x_2,y_2), \\ldots, (x_n, y_n)\\) \\(n\\) paired values \\(x\\) \\(y\\), \\(s_x\\) \\(s_y\\) sample standard deviations \\(x\\) \\(y\\) variables, respectively.Example 1.4  Calculate correlation coefficient \\(x\\) \\(y\\), plotted Figure 1.27. Calculate mean standard deviation \\(x\\) \\(y\\): \\(\\overline{x} = 2\\), \\(\\overline{y} = 3\\), \\(s_x = 1\\), \\(s_y = 2.65\\).\\[\\begin{align*}\nr &=  \\frac{1}{n-1}\\sum^{n}_{=1}\n\\left(\\frac{x_{}-\\overline{x}}\n{s_{x}}\\right)\\left(\\frac{y_{}-\\overline{y}}{s_{y}}\\right) \\\\\n&= \\frac{1}{3 - 1} \\left[\\left(\\frac{1 - 2}\n{1}\\right)\\left(\\frac{5 - 3}{2.65}\\right) + \\left(\\frac{2 - 2}\n{1}\\right)\\left(\\frac{4 - 3}{2.65}\\right) + \\left(\\frac{3 - 2}\n{1}\\right)\\left(\\frac{0 - 3}{2.65}\\right)  \\right] \\\\\n&= -0.94.\n\\end{align*}\\]correlation -0.94, reflects negative association visible scatterplot Figure 1.27.\nFigure 1.27: scatterplot showing three points: (1, 5), (2, 4), (3, 0)\nExample 1.5  appropriate use correlation numerical summary relationship life expectancy income log transformation applied variables? Refer Figure 1.28.Figure 1.28 shows approximately linear relationship; correlation coefficient reasonable numerical summary relationship. calculated statistical software, \\(r = 0.79\\), indicative strong linear relationship.\nFigure 1.28: scatterplot showing log(income) (horizontal axis) vs. log(life.expectancy) (vertical axis).\n","code":""},{"path":"introdata.html","id":"two-categorical-variables","chapter":"1 Introduction to Data","heading":"1.6.2 Two categorical variables","text":"","code":""},{"path":"introdata.html","id":"contingency-tables","chapter":"1 Introduction to Data","heading":"1.6.2.1 Contingency tables","text":"contingency table summarizes data two categorical variables, value table representing number times particular combination outcomes occurs. Contingency tables also known two-way tables. Table 1.4 summarizes relationship race genotype famuss data.row totals provide total counts across row column totals total counts column; collectively, marginal totals.\nTable 1.4: contingency table race actn3.r577x.\nLike relative frequency tables distribution one categorical variable, contingency tables can also converted show proportions. Since two variables, necessary specify whether proportions calculated according row variable column variable.Table 1.5 shows row proportions Table 1.4; proportions indicate genotypes distributed within race. example, value 0.593 upper left corner indicates African Americans study, 59.3% CC genotype.\nTable 1.5: contingency table row proportions race actn3.r577x variables.\nTable 1.6 shows column proportions Table 1.4; proportions indicate distribution races within genotype category. example, value 0.092 indicates CC individuals study, 9.2% African American.\nTable 1.6: contingency table column proportions race actn3.r577x variables.\nExample 1.6  African Americans study, CC common genotype TT least common genotype. pattern hold races study? observations study suggest distribution genotypes r577x vary populations?pattern holds Asians, races. Caucasian individuals sampled study, CT common genotype 46.3%. CC common genotype Asians, population, genotypes evenly distributed: 38.2% Asians sampled CC, 32.7% CT, 29.1% TT. distribution genotypes r577x seems vary population.Exercise 1.1  shown Table 1.6, 72.3% CC individuals study Caucasian. data suggest general population, people CC genotype highly likely Caucasian?, reasonable conclusion draw data. high proportion Caucasians among CC individuals primarily reflects large number Caucasians sampled study – 78.5% people sampled Caucasian. uneven representation different races one limitation famuss data.","code":""},{"path":"introdata.html","id":"segmented-bar-plots","chapter":"1 Introduction to Data","heading":"1.6.2.2 Segmented bar plots","text":"segmented bar plot way visualizing information contingency table. Figures 1.29 1.30 graphically displays data Table 1.4; bar represents level actn3.r577x divided levels race. Figure 1.30 uses row proportions create standardized segmented bar plot.\nFigure 1.29: Segmented bar plot individuals genotype, bars divided race\n\nFigure 1.30: Standardized version Figure 1.29\nAlternatively, data can organized shown Figures 1.31 1.32, bar representing level race. standardized plot particularly useful case, presenting distribution genotypes within race clearly Figure 1.31.\nFigure 1.31: Segmented bar plot individuals race, bars divided genotype\n\nFigure 1.32: Standardized version Figure 1.31\n","code":""},{"path":"aesthetic-mapping.html","id":"aesthetic-mapping","chapter":"2 Visualizing data: Mapping data onto aesthetics","heading":"2 Visualizing data: Mapping data onto aesthetics","text":"section reproduced book Fundamentals Data Visualization13Packages used SectionWhenever visualize data, take data values convert systematic logical way visual elements make final graphic. Even though many different types data visualizations, first glance scatter plot, pie chart, heatmap don’t seem much common, visualizations can described common language captures data values turned blobs ink paper colored pixels screen. key insight following: data visualizations map data values quantifiable features resulting graphic. refer features aesthetics.","code":"\npacman::p_load(\n  cowplot,\n  colorspace,\n  colorblindr, # devtools::install_github(\"clauswilke/colorblindr\")\n  dviz.supp, #   devtools::install_github(\"clauswilke/dviz.supp\")\n  forcats,\n  patchwork,\n  lubridate\n)"},{"path":"aesthetic-mapping.html","id":"aesthetics-and-types-of-data","chapter":"2 Visualizing data: Mapping data onto aesthetics","heading":"2.1 Aesthetics and types of data","text":"Aesthetics describe every aspect given graphical element. examples provided Figure 2.1. critical component every graphical element course position, describes element located. standard 2d graphics, describe positions x y value, coordinate systems one- three-dimensional visualizations possible. Next, graphical elements shape, size, color. Even preparing black--white drawing, graphical elements need color visible, example black background white white background black. Finally, extent using lines visualize data, lines may different widths dash–dot patterns. Beyond examples shown Figure 2.1, many aesthetics may encounter data visualization. example, want display text, may specify font family, font face, font size, graphical objects overlap, may specify whether partially transparent.\nFigure 2.1: Commonly used aesthetics data visualization: position, shape, size, color, line width, line type. aesthetics can represent continuous discrete data (position, size, line width, color) others can usually represent discrete data (shape, line type).\naesthetics fall one two groups: can represent continuous data can . Continuous data values values arbitrarily fine intermediates exist. example, time duration continuous value. two durations, say 50 seconds 51 seconds, arbitrarily many intermediates, 50.5 seconds, 50.51 seconds, 50.50001 seconds, . contrast, number persons room discrete value. room can hold 5 persons 6, 5.5. examples Figure 2.1, position, size, color, line width can represent continuous data, shape line type can usually represent discrete data.Next ’ll consider types data may want represent visualization. may think data numbers, numerical values two several types data may encounter. addition continuous discrete numerical values, data can come form discrete categories, form dates times, text (Table 2.1). data numerical also call quantitative categorical call qualitative. Variables holding qualitative data factors, different categories called levels. levels factor commonly without order (example “dog”, “cat”, “fish” Table 2.1), factors can also ordered, intrinsic order among levels factor (example “good”, “fair”, “poor” Table 2.1).Table 2.1:  Types variables encountered typical data visualization scenarios.examine concrete example various types data, take look Table 2.2. shows first rows dataset providing daily temperature normals (average daily temperatures 30-year window) four U.S. locations. table contains five variables: month, day, location, station ID, temperature (degrees Fahrenheit). Month ordered factor, day discrete numerical value, location unordered factor, station ID similarly unordered factor, temperature continuous numerical value.Table 2.2:  First 12 rows dataset listing daily temperature normals four weather stations. Data source: NOAA.","code":""},{"path":"aesthetic-mapping.html","id":"scales-map-data-values-onto-aesthetics","chapter":"2 Visualizing data: Mapping data onto aesthetics","heading":"2.2 Scales map data values onto aesthetics","text":"map data values onto aesthetics, need specify data values correspond specific aesthetics values. example, graphic x axis, need specify data values fall onto particular positions along axis. Similarly, may need specify data values represented particular shapes colors. mapping data values aesthetics values created via scales. scale defines unique mapping data aesthetics (Figure 2.2). Importantly, scale must one--one, specific data value exactly one aesthetics value vice versa. scale isn’t one--one, data visualization becomes ambiguous.\nFigure 2.2: Scales link data values aesthetics. , numbers 1 4 mapped onto position scale, shape scale, color scale. scale, number corresponds unique position, shape, color vice versa.\nLet’s put things practice. can take dataset shown Table 2.2, map temperature onto y axis, day year onto x axis, location onto color, visualize aesthetics solid lines. result standard line plot showing temperature normals four locations change year (Figure\n2.3).\nFigure 2.3: Daily temperature normals four selected locations U.S. Temperature mapped y axis, day year x axis, location line color. Data source: NOAA.\nFigure 2.3 fairly standard visualization temperature curve likely visualization data scientists intuitively choose first. However, us variables map onto scales. example, instead mapping temperature onto y axis location onto color, can opposite. now key variable interest (temperature) shown color, need show sufficiently large colored areas color convey useful information.14 Therefore, visualization chosen squares instead lines, one month location, colored average temperature normal month (Figure 2.4).\nFigure 2.4: Monthly normal mean temperatures four locations U.S. Data source: NOAA\nlike emphasize Figure 2.4 uses two position scales (month along x axis location along y axis) neither continuous scale. Month ordered factor 12 levels location unordered factor four levels. Therefore, two position scales discrete. discrete position scales, generally place different levels factor equal spacing along axis. factor ordered (case month), levels need placed appropriate order. factor unordered (case location), order arbitrary, can choose order want. ordered locations overall coldest (Chicago) overall hottest (Death Valley) generate pleasant staggering colors. However, chosen order figure equally valid.Figures 2.3 2.4 used three scales total, two position scales one color scale. typical number scales basic visualization, can use three scales . Figure 2.5 uses five scales, two position scales, one color scale, one size scale, one shape scale, scales represent different variable dataset.\nFigure 2.5: Fuel efficiency versus displacement, 32 cars (1973–74 models). figure uses five separate scales represent data: () x axis (displacement); (ii) y axis (fuel efficiency); (iii) color data points (power); (iv) size data points (weight); (v) shape data points (number cylinders). Four five variables displayed (displacement, fuel efficiency, power, weight) numerical continuous. remaining one (number cylinders) can considered either numerical discrete qualitative ordered. Data source: Motor Trend, 1974.\n","code":""},{"path":"coordinate-systems-axes.html","id":"coordinate-systems-axes","chapter":"3 Coordinate systems and axes","heading":"3 Coordinate systems and axes","text":"section reproduced book Fundamentals Data Visualization15Packages used SectionTo make sort data visualization, need define position scales, determine graphic different data values located. visualize data without placing different data points different locations, even just arrange next along line. regular 2d visualizations, two numbers required uniquely specify point, therefore need two position scales. two scales usually necessarily x y axis plot. also specify relative geometric arrangement scales. Conventionally, x axis runs horizontally y axis vertically, choose arrangements. example, y axis run acute angle relative x axis, one axis run circle run radially. combination set position scales relative geometric arrangement called coordinate system.","code":"\npacman::p_load(\n  cowplot,\n  colorspace,\n  colorblindr, # devtools::install_github(\"clauswilke/colorblindr\")\n  dviz.supp, #   devtools::install_github(\"clauswilke/dviz.supp\")\n  forcats,\n  patchwork,\n  lubridate,\n  tidyr, \n  ggrepel\n)"},{"path":"coordinate-systems-axes.html","id":"cartesian-coordinates","chapter":"3 Coordinate systems and axes","heading":"3.1 Cartesian coordinates","text":"widely used coordinate system data visualization 2d Cartesian coordinate system, location uniquely specified x y value. x y axes run orthogonally , data values placed even spacing along axes (Figure 3.1). two axes continuous position scales, can represent positive negative real numbers. fully specify coordinate system, need specify range numbers axis covers. Figure 3.1, x axis runs -2.2 3.2 y axis runs -2.2 2.2. data values axis limits placed respective location plot. data values outside axis limits discarded.\nFigure 3.1: Standard cartesian coordinate system. horizontal axis conventionally called x vertical axis y. two axes form grid equidistant spacing. , x y grid lines separated units one. point (2, 1) located two x units right one y unit origin (0, 0). point (-1, -1) located one x unit left one y unit origin.\nData values usually aren’t just numbers, however. come units. example, ’re measuring temperature, values may measured degrees Celsius Fahrenheit. Similarly, ’re measuring distance, values may measured kilometers miles, ’re measuring duration, values may measured minutes, hours, days. Cartesian coordinate system, spacing grid lines along axis corresponds discrete steps data units. temperature scale, example, may grid line every 10 degrees Fahrenheit, distance scale, may grid line every 5 kilometers.Cartesian coordinate system can two axes representing two different units. situation arises quite commonly whenever ’re mapping two different types variables x y. example, Figure 2.3, plotted temperature vs. days year. y axis Figure 2.3 measured degrees Fahrenheit, grid line every 20 degrees, x axis measured months, grid line first every third month. Whenever two axes measured different units, can stretch compress one relative maintain valid visualization data (Figure 3.2). version preferable may depend story want convey. tall narrow figure emphasizes change along y axis short wide figure opposite. Ideally, want choose aspect ratio ensures important differences position noticeable.\nFigure 3.2: Daily temperature normals Houston, TX. Temperature mapped y axis day year x axis. Parts (), (b), (c) show figure different aspect ratios. three parts valid visualizations temperature data. Data source: NOAA.\nhand, x y axes measured units, grid spacings two axes equal, distance along x y axis corresponds number data units. example, can plot temperature Houston, TX temperature San Diego, CA, every day year (Figure 3.3a). Since quantity plotted along axes, need make sure grid lines form perfect squares, case Figure 3.3.\nFigure 3.3: Daily temperature normals Houston, TX, plotted versus respective temperature normals San Diego, CA. first days months January, April, July, October highlighted provide temporal reference. () Temperatures shown degrees Fahrenheit. (b) Temperatures shown degrees Celsius. Data source: NOAA.\nmay wonder happens change units data. , units arbitrary, preferences might different somebody else’s. change units linear transformation, add subtract number data values /multiply data values another number. Fortunately, Cartesian coordinate systems invariant linear transformations. Therefore, can change units data resulting figure change long change axes accordingly. example, compare Figures 3.3a 3.3b. show data, part () temperature units degrees Fahrenheit part (b) degrees Celsius. Even though grid lines different locations numbers along axes different, two data visualizations look exactly .","code":""},{"path":"coordinate-systems-axes.html","id":"nonlinear-axes","chapter":"3 Coordinate systems and axes","heading":"3.2 Nonlinear axes","text":"Cartesian coordinate system, grid lines along axis spaced evenly data units resulting visualization. refer position scales coordinate systems linear. linear scales generally provide accurate representation data, scenarios nonlinear scales preferred. nonlinear scale, even spacing data units corresponds uneven spacing visualization, conversely even spacing visualization corresponds uneven spacing data units.commonly used nonlinear scale logarithmic scale log scale short. Log scales linear multiplication, unit step scale corresponds multiplication fixed value. create log scale, need log-transform data values exponentiating numbers shown along axis grid lines. process demonstrated Figure 3.4, shows numbers 1, 3.16, 10, 31.6, 100 placed linear log scales. numbers 3.16 31.6 may seem strange choice, chosen exactly half-way 1 10 10 100 log scale. can see observing \\(10^{0.5} = \\sqrt{10} \\approx 3.16\\) equivalently \\(3.16 \\times 3.16 \\approx 10\\). Similarly, \\(10^{1.5} = 10\\times10^{0.5} \\approx 31.6\\).\nFigure 3.4: Relationship linear logarithmic scales. dots correspond data values 1, 3.16, 10, 31.6, 100, evenly-spaced numbers logarithmic scale. can display data points linear scale, can log-transform show linear scale, can show logarithmic scale. Importantly, correct axis title logarithmic scale name variable shown, logarithm variable.\nMathematically, difference plotting log-transformed data linear scale plotting original data logarithmic scale (Figure 3.4). difference lies labeling individual axis ticks axis whole. cases, labeling logarithmic scale preferable, places less mental burden reader interpret numbers shown axis tick labels. also less risk confusion base logarithm. working log-transformed data, can get confused whether data transformed using natural logarithm logarithm base 10. ’s uncommon labeling ambiguous, e.g. “log(x)”, doesn’t specify base . recommend always verify base working log-transformed data. plotting log-transformed data, always specify base labeling axis.multiplication log scale looks like addition linear scale, log scales natural choice data obtained multiplication division. particular, ratios generally shown log scale. example, taken number inhabitants county Texas divided median number inhabitants across Texas counties. resulting ratio number can larger smaller 1. ratio exactly 1 implies corresponding county median number inhabitants. visualizing ratios log scale, can see clearly population numbers Texas counties symmetrically distributed around median, populous counties 100 times inhabitants median least populous counties 100 times fewer inhabitants (Figure 3.5). contrast, data, linear scale obscures differences county median population number county much smaller population number median (Figure 3.6).\nFigure 3.5: Population numbers Texas counties relative median value. Select counties highlighted name. dashed line indicates ratio 1, corresponding county median population number. populous counties approximately 100 times inhabitants median county, least populous counties approximately 100 times fewer inhabitants median county. Data source: 2010 Decennial U.S. Census.\n\nFigure 3.6: Population sizes Texas counties relative median value. displaying ratio linear scale, overemphasized ratios > 1 obscured ratios < 1. general rule, ratios displayed linear scale. Data source: 2010 Decennial U.S. Census.\nlog scale, value 1 natural midpoint, similar value 0 linear scale. can think values greater 1 representing multiplications values less 1 divisions. example, can write \\(10 = 1\\times 10\\) \\(0.1 = 1/10\\). value 0, hand, can never appear log scale. lies infinitely far 1. One way see consider \\(\\log(0) = -\\infty\\). , alternatively, consider go 1 0, takes either infinite number divisions finite value (e.g., \\(1/10/10/10/10/10/10\\dots = 0\\)) alternatively one division infinity (.e., \\(1/\\infty = 0\\)).Log scales frequently used data set contains numbers different magnitudes. Texas counties shown Figures 3.5 3.6, populous one (Harris) 4,092,459 inhabitants 2010 U.S. Census least populous one (Loving) 82. log scale appropriate even hadn’t divided population numbers median turn ratios. county 0 inhabitants? county shown logarithmic scale, lie minus infinity. situation, recommendation sometimes use square-root scale, uses square root transformation instead log transformation (Figure 3.7). Just like log scale, square-root scale compresses larger numbers smaller range, unlike log scale, allows presence 0.\nFigure 3.7: Relationship linear square-root scales. dots correspond data values 0, 1, 4, 9, 16, 25, 36, 49, evenly-spaced numbers square-root scale, since squares integers 0 7. can display data points linear scale, can square-root-transform show linear scale, can show square-root scale.\nsee two problems square-root scales. First, linear scale one unit step corresponds addition subtraction constant value log scale corresponds multiplication division constant value, rule exists square-root scale. meaning unit step square-root scale depends scale value ’re starting. Second, unclear best place axis ticks square-root scale. obtain evenly spaced ticks, place squares, axis ticks , example, positions 0, 4, 25, 49, 81 (every second square) highly unintuitive. Alternatively, place linear intervals (10, 20, 30, etc), result either axis ticks near low end scale many near high end. Figure 3.7, placed axis ticks positions 0, 1, 5, 10, 20, 30, 40, 50 square-root scale. values arbitrary provide reasonable covering data range.Despite problems square-root scales, valid position scales discount possibility appropriate applications. example, just like log scale natural scale ratios, one argue square-root scale natural scale data come squares. One scenario data naturally squares context geographic regions. show areas geographic regions square-root scale, highlighting regions’ linear extent East West North South. extents relevant, example, wondering long might take drive across region. Figure 3.8 shows areas states U.S. Northeast linear square-root scale. Even though areas states quite different (Figure 3.8a), time take drive across state closely resemble figure square-root scale (Figure 3.8b) figure linear scale (Figure 3.8a).\nFigure 3.8: Areas Northeastern U.S. states. () Areas shown linear scale. (b) Areas shown square-root scale. Data source: Google.\n","code":""},{"path":"ggplot2-package-for-plots.html","id":"ggplot2-package-for-plots","chapter":"4 ggplot2 package for plots","heading":"4 ggplot2 package for plots","text":"section adapted book Data Visualization: practical introduction16Packages used SectionThis Section teach use ggplot’s core functions produce series scatterplots. one point view, proceed slowly carefully, taking time understand logic behind commands type. reason central activity visualizing data ggplot less always involves sequence steps. worth learning .another point view, though, move fast. basic sequence , understand ggplot assembles pieces plot final image, find analytically aesthetically sophisticated plots come within reach quickly. end Section, example, learned produce small-multiple plot time series data large number countries, smoothed regression line panel. another sense moved quite fast.","code":"\npacman::p_load(\n  gapminder,\n  here,\n  socviz, \n  tidyverse,\n  kableExtra,\n  DT\n)\n\ntheme_set(cowplot::theme_cowplot())"},{"path":"ggplot2-package-for-plots.html","id":"how-ggplot2-works","chapter":"4 ggplot2 package for plots","heading":"4.1 How ggplot2 works","text":"saw Section 2, visualization involves representing data\ndata using lines shapes colors . \nstructured relationship, mapping, variables \ndata representation plot displayed screen \npage. also saw mappings make sense \ntypes variables, (independently ), representations\nharder interpret others. ggplot provides set \ntools map data visual elements plot, specify kind\nplot want, subsquently control fine details \ndisplayed. Figure 4.1 shows \nschematic outline process starting data, top, \nfinished plot bottom.\nFigure 4.1: Overview constructing figure ggplot.\nimportant thing get used ggplot way use\nthink logical structure plot. code \nwrite specifies connections variables data,\ncolors, points, shapes see screen. ggplot,\nlogical connections data plot elements \ncalled aesthetic mappings just aesthetics. begin every plot\ntelling ggplot() function data , \nvariables data logically map onto plot’s aesthetics. \ntake result say general sort plot want, \nscatterplot, boxplot, bar chart. ggplot, overall\ntype plot called geom. geom function creates\n. example, geom_point() makes scatterplots, geom_bar() makes\nbarplots, geom_boxplot() makes boxplots, . combine\ntwo pieces, ggplot() object geom, literally\nadding together expression, using “+” symbol.point, ggplot enough information able draw\nplot . rest just details exactly want\nsee. don’t specify anything , ggplot use set\ndefaults try sensible gets drawn. \noften, want specify exactly want, including\ninformation scales, labels legends axes, \nguides help people read plot. additional\npieces added plot way geom_ function\n. component function, provide arguments \nspecifying , literally add sequence \ninstructions. way systematically build plot piece \npiece.chapter go main steps process. \nproceed example, repeatedly building series plots. \nnoted earlier, strongly encourage go exercise\nmanually, typing (rather copying--pasting) code .\nmay seem bit tedious, far effective way\nget used happening, get feel R’s syntax.\n’ll inevitably make errors, also quickly find\nbecoming able diagnose errors, well \nbetter grasp higher-level structure plots. open\nRMarkdown file notes, remember load tidyverse\nlibrary write code chunks, interspersing notes comments go.","code":""},{"path":"ggplot2-package-for-plots.html","id":"tidy-data","chapter":"4 ggplot2 package for plots","heading":"4.2 Tidy Data","text":"tidyverse tools using want see data \nparticular sort shape, generally referred tidy data.17\nSocial scientists likely familiar \ndistinction wide format long format data. long\nformat table, every variable column, every observation \nrow. wide format table, variables spread across\ncolumns. example, Table 4.1 shows part table\nlife expectancy time series countries.\nTable 4.1: Life Expectancy data gapminder dataset wide\nformat first 10 countries.\nwide format, one variables, year, spread across\ncolumns table.contrast, Table 4.2 shows beginning \ndata long format. tidy data ggplot wants long\nform.\nTable 4.2: Life Expectancy data gapminder dataset long format random sample countries.\nrelated bit terminology, table year\nvariable sometimes called key lifeExp variable \nvalue taken key particular row. terms \nuseful converting tables wide long format. speaking\nfairly loosely . Underneath terms worked-\ntheory forms tabular data can stored , right now\ndon’t need know additional details.compare Tables 4.1 4.2, clear tidy table present data compact form. fact, usually choose present data wanted just show people numbers. Neither untidy data “messy” “wrong” way lay data generic sense. ’s just , even long-form shape makes tables larger, tidy data much straightforward work comes specifying mappings need coherently describe plots.","code":""},{"path":"ggplot2-package-for-plots.html","id":"mappings-link-data-to-things-you-see","chapter":"4 ggplot2 package for plots","heading":"4.3 Mappings link data to things you see","text":"’s useful think recipe template start \ntime want make plot. shown Figure 4.2.\nFigure 4.2: ggplot formula schematic.\nstart just one object , data, shape ggplot understands.\nUsually data frame augmented version ,\nlike tibble. tell core ggplot function data .\nbook, creating object named p \ncontain core information plot. (name p just\nconvenience.) choose plot type, geom add \np. , add features plot needed, \nadditional elements, adjusted scales, title, \nlabels needed.’ll use gapminder data make first plots. Make sure library containing data loaded.Let’s say want plot Life Expectancy per capita GDP country-years data. ’ll creating object necessary information , build . First, must tell ggplot() function data using.point ggplot knows data, mapping. , need tell variables data represented visual elements plot. also doesn’t know sort plot want. ggplot, mappings specified using aes() function. Like :’ve given ggplot() function two arguments instead one: data mapping. data argument tells ggplot find variables use. saves us tediously dig name variable full. Instead, mentions variables looked first.Next, mapping. mapping argument data object, \ncharacter string. Instead, ’s function. (Remember, functions\ncan nested inside functions.) arguments give \naes function sequence definitions ggplot use\nlater. say, “variable x-axis going \ngdpPercap, variable y-axis going lifeExp.”\naes() function say variables names \nfound. ’s ggplot() going assume things \nname object given data argument.mapping = aes(…) argument links variables things \nsee plot. x y values obvious\nones. aesthetic mappings can include, example, color, shape,\nsize, line type (whether line solid dashed, \npattern). ’ll see examples minute. mapping directly\nsay particular, e.g., colors shapes plot.\nRather say variables data represented \nvisual elements like color, shape, point plot area.happens just type p console point hit\nreturn?\nFigure 4.3: empty plot geoms.\np object created ggplot() function, already information mappings want, together lot information added default. (want see just much information p object already, try asking str(p).) However, haven’t given instructions get sort plot draw. need add layer plot. means picking geom_ function. use geom_point(). knows take x y values plot scatterplot.\nFigure 4.4: scatterplot Life Expectancy vs GDP\n","code":"\nlibrary(gapminder)\ndata(gapminder)\np <- ggplot(data = gapminder)\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp))\np\np + geom_point() "},{"path":"ggplot2-package-for-plots.html","id":"build-your-plots-layer-by-layer","chapter":"4 ggplot2 package for plots","heading":"4.4 Build your plots layer by layer","text":"now , much change conceptually\n. question learning \ngreater detail tell ggplot . learn \ndifferent geoms (types plot) available, find \nfunctions control coordinate system, scales,\nguiding elements (like labels tick marks), thematic features\nplots. allow us make much sophisticated plots\nsurprisingly fast. Conceptually, however, always \nthing. start table data tidied,\n:Tell ggplot() function data . data = … step.Tell ggplot() relationships want see. mapping = aes(…) step. convenience put results first two steps object called p.Tell ggplot want see relationships data, .e. choose geom.Layer geoms needed, adding p object one time.Use additional functions adjust scales, labels, tick marks, titles using scale_, family, labs() guides() functions. ’ll learn functions shortly.begin let ggplot use defaults many \nelements. coordinate system used plots often cartesian,\nexample. plane defined x axis y axis. \nggplot assumes, unless tell otherwise. quickly start making adjustments.\nBear mind process adding layers plot really additive. effect create one big object nested list instructions draw piece plot. Usually R, functions simply added objects. Rather, take objects inputs produce objects outputs.\nobjects created ggplot() special. makes easier assemble plots one piece time, inspect look every step. example, let’s try different geom_ function plot.\nFigure 4.5: Life Expectancy vs GDP, using smoother.\ncan see right away geoms lot simply put points grid. geom_smooth() calculated smoothed line us shaded ribbon showing standard error line. want see data points line together, simply add geom_point() back :\nFigure 4.6: Life Expectancy vs GDP, showing points GAM smoother.\nconsole message R tells geom_smooth() function using method called gam, case means fit generalized additive model. suggests maybe methods geom_smooth() understands, might tell use instead. Instructions given functions via arguments, can try adding method = “lm” (“linear model”) argument geom_smooth():\nFigure 4.7: Life Expectancy vs GDP, points ill-advised linear fit.\ntell geom_point() geom_smooth()\ndata coming , mappings use.\ninherit information original p object. ’ll\nsee later, ’s possible give geoms separate instructions \nfollow instead. absence information, \ngeoms look instructions needs ggplot()\nfunction, object created .plot, data quite bunched left side. Gross Domestic Product per capita normally distributed across country years. x-axis scale probably look better transformed linear scale log scale. can use function called scale_x_log10(). might expect function scales x-axis plot log 10 basis. use just add plot:\nFigure 4.8: Life Expectancy vs GDP scatterplot, GAM smoother log scale x-axis.\nx-axis transformation repositions points, also changes shape smoothed line. (switched back gam lm.) ggplot() associated functions made changes underlying data frame, scale transformation applied data smoother layered plot. variety scale transformations can use just way. named transformation want apply, axis want applying . case use scale_x_log10().point, goal just show plot Life Expectancy vs GDP using sensible scales adding smoother, thinking polishing plot nicer axis labels title. Perhaps might also want replace scientific notation x-axis dollar value actually represents. can things quite easily. Let’s take care scale first. labels tick-marks can controlled scale_ functions. ’s possible roll function label axes (just supply labels manually, see later), ’s also handy scales library contains useful pre-made formatting functions. can either load whole library library(scales) , conveniently, just grab specific formatter want library. ’s dollar() function. grab function directly library loaded, use syntax package_name::function_name. , can :\nFigure 4.9: Life Expectancy vs GDP scatterplot, GAM smoother log scale x-axis, better labels tick marks.\nRemember two things scale transformations: First, can directly transform\nx y axis adding something like scale_x_log10() \nscale_y_log10() plot. , x y axis \ntransformed , default, tick marks axis \nlabeled using scientific notation. Second, can give scale_\nfunctions labels argument reformats text printed\nunderneath tick marks axes. Inside scale_x_log10()\nfunction try labels=scales::comma, example. details scale transformations, see Section 3.2.","code":"\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y=lifeExp))\np + geom_smooth()\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y=lifeExp))\np + geom_point() + geom_smooth() \n#> `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y=lifeExp))\np + geom_point() + geom_smooth(method = \"lm\") \n#> `geom_smooth()` using formula 'y ~ x'\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y=lifeExp))\np + geom_point() +\n    geom_smooth(method = \"gam\") +\n    scale_x_log10()\np <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp))\np + geom_point() +\n    geom_smooth(method = \"gam\") +\n    scale_x_log10(labels = scales::dollar)"},{"path":"ggplot2-package-for-plots.html","id":"mapping-aesthetics-vs-setting-them","chapter":"4 ggplot2 package for plots","heading":"4.5 Mapping aesthetics vs setting them","text":"\naesthetic mapping specifies variable expressed one available visual elements, size, color, shape, . ’ve seen, map variables aesthetics like :\n\ncode give direct instruction like “color points purple”. Instead says, “property ‘color’ represent variable continent”, “color map continent”. want turn points figure purple, mapping function. Look happens try:\n\nFigure 4.10: gone wrong ?\nhappened ? legend saying “purple”? points turned pinkish-red instead purple? Remember, aesthetic mapping variables data properties can see graph. aes() function mapping specified, function trying job. wants map variable color aesthetic, assumes giving variable. given one word, though—“purple”. Still, aes() best treat word though variable. variable many observations rows data, aes() falls back R’s recycling rules making vectors different lengths match .\neffect, creates new categorical variable data. string “purple” recycled every row data. Now new column. Every element value, “purple”. ggplot plots results graph ’ve asked , mapping color aesthetic. dutifully makes legend new variable. default, ggplot displays points falling category “purple” () using default first-category hue … red.\n\naes() function mappings . use change properties particular value. want set property, geom_ using, outside mapping = aes(…) step. Try :\n\nFigure 4.11: Setting color attribute points directly.\n\ngeom_point() function can take color argument directly, R knows color “purple” . part aesthetic mapping defines basic structure graphic. point view grammar logic graph, fact points colored purple significance. color purple representing mapping variable feature data relevant way.\n\nFigure 4.12: Setting arguments.\nvarious geom_ functions can take many arguments affect plot looks, involve mapping variables aesthetic elements. Thus, arguments never go inside aes() function. things want set, like color size, name mappable elements. Others, like method se arguments geom_smooth() affect aspects plot. code Figure 4.12, geom_smooth() call sets line color orange sets size (.e., thickness) 8, unreasonably large value. also turn se option switching default value TRUE FALSE. result standard error ribbon shown.Meanwhile geom_smooth() call set alpha argument 0.3. ’s also possible map continuous variable directly alpha property, much like one might map continuous variable single-color gradient. However, generally effective way precisely conveying variation quantity. Like color, size, shape, “alpha” aesthetic property points (plot elements) , variables can mapped. controls transparent object appear drawn. ’s measured scale zero one. object alpha zero completely transparent. Setting zero make mappings object might , color size, invisible well. object alpha one completely opaque. Choosing intermediate value can useful lot overlapping data plot, makes easier see bulk observations located.\nFigure 4.13: polished plot Life Expectancy vs GDP.\ncan now make reasonably polished plot. set alpha points low value, make nicer x- y-axis labels, add title, subtitle, caption. can see code , addition x, y, aesthetic mappings plot (size, fill, color), labs() function can also set text title, subtitle, caption. controls main labels scales. appearance things like axis tick marks responsibility various scale_ functions, scale_x_log10() function used . learn can done scale_ functions soon.variables data can sensibly mapped color aesthetic? Consider continent. Figure 4.14 individual data points colored continent, legend key colors automatically added plot. addition, instead one smoothing line now five. one unique value continent variable. consequence way aesthetic mappings inherited. Along x y, color aesthetic mapping set call ggplot() used create p object. Unless told otherwise, geoms layered top original plot object inherit object’s mappings. case get points smoothers colored continent.\nFigure 4.14: Mapping continent variable color aesthetic.\nwant, might also consider shading standard error ribbon line match dominant color. color standard error ribbon controlled fill aesthetic. Whereas color aesthetic affects appearance lines points, fill filled areas bars, polygons , case, interior smoother’s standard error ribbon.\nFigure 4.15: Mapping continent variable color aesthetic, correcting error bars using fill aesthetic.\n\nMaking sure color fill aesthetics match consistently \nway improves overall look plot. order make \nhappen just need specify mappings \nvariable case.\n","code":"\np <-  ggplot(data = gapminder,\n             mapping = aes(x = gdpPercap,\n                            y = lifeExp,\n                            color = continent))\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp,\n                          color = \"purple\"))\np + geom_point() +\n    geom_smooth(method = \"loess\") +\n    scale_x_log10()\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp))\np + geom_point(color = \"purple\") +\n    geom_smooth(method = \"loess\") +\n    scale_x_log10()\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp)) \np + geom_point(alpha = 0.3) +\n    geom_smooth(color = \"orange\", se = FALSE, size = 8, method = \"lm\") +\n    scale_x_log10()\np <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y=lifeExp))\np + geom_point(alpha = 0.3) + geom_smooth(method = \"gam\") +\n    scale_x_log10(labels = scales::dollar) +\n    labs(x = \"GDP Per Capita\", y = \"Life Expectancy in Years\",\n         title = \"Economic Growth and Life Expectancy\",\n         subtitle = \"Data points are country-years\",\n         caption = \"Source: Gapminder.\")\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp,\n                          color = continent))\np + geom_point() +\n    geom_smooth(method = \"loess\") +\n    scale_x_log10()\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp,\n                          color = continent,\n                          fill = continent))\np + geom_point() +\n    geom_smooth(method = \"loess\") +\n    scale_x_log10()"},{"path":"ggplot2-package-for-plots.html","id":"aesthetics-can-be-mapped-per-geom","chapter":"4 ggplot2 package for plots","heading":"4.6 Aesthetics can be mapped per geom","text":"Perhaps five separate smoothers many, just want one line. still like points color-coded continent. default, geoms inherit mappings ggplot() function. can change mapping aesthetics want geom_ functions want apply . use mapping = aes(…) expression initial call ggplot(), now use geom_ functions well, specifying mappings want apply one. Mappings specified initial ggplot() function—, x y—carry subsequent geoms.\nFigure 4.16: Mapping aesthetics per-geom basis. color mapped continent points smoother.\n’s possible map continuous variables color aesthetic, . example, can map log country-year’s population (pop) color. (can take log population right aes() statement, using log() function. R evaluate us quite happily.) , ggplot produces gradient scale. continuous, marked intervals legend. Depending circumstances, mapping quantities like population continuous color gradient may less effective cutting variable categorical bins running, e.g., low high. general always worth looking data continuous form first rather cutting binning categories.\nFigure 4.17: Mapping continuous variable color.\nFinally, worth paying little attention way ggplot draws scales. every mapped variable scale, can learn lot plot constructed, mappings contains, seeing legends look like. example, take closer look legends produced Figures 4.15 4.16.\nFigure 4.18: Guides legends faithfully reflect mappings represent.\nlegend first figure, (left side Figure 4.18), see several visual elements. key continent shows dot, line, shaded background. key second figure, shown right, dot continent, shaded background line. look code Figures 4.15 4.16, see first case mapped continent variable color fill. drew figure geom_point() fitted line continent geom_smooth(). Points color smoother understands color (line ) fill (shaded standard error ribbon). elements represented legend: point color, line color, ribbon fill. second figure, decided simplify things points colored continent. drew just single smoother whole graph. Thus, legend figure, colored line shaded box absent. see legend mapping color continent geom_point(). Meanwhile graph line drawn geom_smooth() set default bright blue, different anything scale, shaded error ribbon set default gray. Small details like accidents. direct consequence ggplot’s grammatical way thinking relationship data behind plot visual elements represent .","code":"\np <- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp))\np + geom_point(mapping = aes(color = continent)) +\n    geom_smooth(method = \"loess\") +\n    scale_x_log10()\np <- ggplot(data = gapminder,\n            mapping = aes(x = gdpPercap,\n                          y = lifeExp))\np + geom_point(mapping = aes(color = log(pop))) +\n    scale_x_log10()    "},{"path":"ggplot2-package-for-plots.html","id":"save-your-work","chapter":"4 ggplot2 package for plots","heading":"4.7 Save your work","text":"\nNow started make plots, may wondering save , perhaps also control size format. working RMarkdown document plots make embedded , already seen. can set default size plots within .Rmd document setting option first code chunk. one tells R make 8x5 figures:\nknitting RMarkdown file, figures automatically saved individual files figures folder. name file default name code chunk. Hence ’s important label code chunks. Otherwise, file named unnamed-figure-1.\nmaking plots different sizes shapes, sometimes want control size particular plots, without changing default. , can add options particular chunk inside curly braces beginning. Remember, chunk opens three backticks pair braces containing language name (us always r) optional label:\n\n\ncan follow label comma provide series options needed. apply chunk. make figure twelve inches wide nine inches high say e.g. {r example, fig.width = 12, fig.height = 9} braces section.\n\noften need save figures individually, \nend dropped slides published papers \nproduced using RMarkdown. Saving figure file can done \nseveral different ways. working ggplot, easiest way \nuse ggplot2::ggsave function. save recently displayed\nfigure, provide name want save :\n\nRemember , convenience, need write filename = long name file first argument give ggsave(). can also pass plot objects ggsave(). example, can put recent plot object called p_out tell ggave() want save object.\n\nsaving work, sensible subfolder (\none, depending project) save figures. \nalso take care name saved figures sensible way.\nfig_1.pdf my_figure.pdf good names. Figure names \ncompact descriptive, consistent figures within \nproject. addition, although really shouldn’t case \nday age, also wise play safe avoid file names\ncontaining characters likely make code choke future. \ninclude apostrophes, backticks, spaces, forward back slashes, \nquotes.\n\nTreat project folder home base work paper work , put data figures subfolders within project folder. begin , using file manager, create folder named “figures” inside project folder. saving figures, use package make easier work files subfolders type full file paths. Load library setup chunk RMarkdown document. , tells “” current project. see message saying something like , file path user name instead mine:\n\ncan use () function make loading saving \nwork straightforward safer. Assuming folder named “figures” exists project folder, can :\n\n\nsaves p_out file called lifeexp_vs_gdp_gradient.pdf figures directory , .e. current project folder.\n\ncan save figure variety different formats, depending\nneeds (also, lesser extent, particular\ncomputer system). important distinction bear mind \nvector formats raster formats. file vector\nformat, like PDF SVG, stored set instructions \nlines, shapes, colors, relationships. viewing software\n(Adobe Acrobat Apple’s Preview application PDFs) \ninterprets instructions displays figure. Representing\nfigure way allows easily resized without becoming\ndistorted. underlying language PDF format Postscript,\nalso language modern typesetting printing. \nmakes vector-based format like PDF best choice submission \njournals.\n\nraster based format, hand, stores images essentially\ngrid pixels pre-defined size information \nlocation, color, brightness, pixel grid. \nmakes efficient storage, especially used conjunction\ncompression methods take advantage redundancy images\norder save space. Formats like JPG compressed raster\nformats. PNG file raster image format supports lossless\ncompression. graphs containing awful lot data, PNG files\ntend much smaller corresponding PDF. However,\nraster formats easily resized. particular \nexpanded size without becoming pixelated grainy. Formats like\nJPG PNG standard way images displayed web.\nrecent SVG format vector-based format also\nnevertheless supported many web browsers.\n\ngeneral save work several different formats.\nsave different formats different sizes may need\nexperiment scaling plot size fonts\norder get good result. scale argument ggsave() can\nhelp (can try different values, like scale=1.3,\nscale=5, ). can also use ggave() explicitly set\nheight width plot units choose.\n\n\nNow know , let’s get back making graphs.\n","code":"\nknitr::opts_chunk$set(fig.width=6, fig.height=5) ```{r gdp-vs-income}\np + geom_point()\n```\nggplot2::ggsave(filename = \"figures/my_figure.png\") # png output for HTML\nggplot2::ggsave(filename = \"figures/my_figure.pdf\") # pdf output for LaTeX documents\np_out <- p + geom_point() +\n    geom_smooth(method = \"loess\") +\n    scale_x_log10()\n\nggsave(\"my_figure.pdf\", plot = p_out)\nhere::here()\n#> [1] \"/Users/runner/work/EPIB607/EPIB607\"\nggsave(here(\"figures\", \"lifexp_vs_gdp_gradient.pdf\"), plot = p_out)\nggsave(here(\"figures\", \"lifexp_vs_gdp_gradient.pdf\"),\n       plot = p_out, height = 8, width = 10, units = \"in\")"},{"path":"ggplot2-package-for-plots.html","id":"next-steps","chapter":"4 ggplot2 package for plots","heading":"4.8 Next steps","text":"\nStart playing around gapminder data little . can try explorations geom_point() geom_smooth(), together.\n\nhappens put geom_smooth() function geom_point() instead ? tell plot drawn? Think might useful drawing plots.\n\nChange mappings aes() function plot Life Expectancy population (pop) rather per capita GDP. look like? tell unit observation dataset?\n\nTry alternative scale mappings. Besides scale_x_log10() can try scale_x_sqrt() scale_x_reverse(). corresponding functions y-axis transformations. Just write y instead x. Experiment see sort effect plot, whether make sense use.\n\nhappens map color year instead continent? result expected? Think class object year . Remember can get quick look top data, includes shorthand information class variable, typing gapminder.\n\nInstead mapping color = year, happens try color = factor(year)?\n\nlook different scatterplots, think Figure 4.13 little critically. worked point reasonably polished, really best way display country-year data? gaining losing ignoring temporal country-level structure data? better? Sketch alternative visualization might look like.\n\nbegin experiment, remember two things. First, ’s always worth trying something, even ’re sure ’s going happen. Don’t afraid console. nice thing making graphics code won’t break anything can’t reproduce. something doesn’t work, can figure happened, fix things, re-run code make graph .\n\nSecond, remember main flow action ggplot always . start table data, map variables want display aesthetics like position, color, shape, choose one geoms draw graph. code gets accomplished making object basic information data mappings, adding layering additional information needed. get used way thinking plots, especially aesthetic mapping part, drawing becomes easier. Instead think draw particular shapes colors screen, many geom_ functions take care . way, learning new geoms easier think ways display aesthetic mappings specify. learning curve ggplot involves getting used way thinking data representation plot.\n","code":""},{"path":"color-basics.html","id":"color-basics","chapter":"5 Color scales","heading":"5 Color scales","text":"section reproduced book Fundamentals Data Visualization18Packages used SectionThere three fundamental use cases color data visualizations: () can use color distinguish groups data ; (ii) can use color represent data values; (iii) can use color highlight. types colors use way use quite different three cases.","code":"\npacman::p_load(\n  cowplot,\n  colorspace,\n  colorblindr, # devtools::install_github(\"clauswilke/colorblindr\")\n  dviz.supp, # devtools::install_github(\"clauswilke/dviz.supp\")\n  forcats,\n  patchwork,\n  lubridate,\n  tidyr, \n  ggplot2,\n  sf\n)"},{"path":"color-basics.html","id":"color-as-a-tool-to-distinguish","chapter":"5 Color scales","heading":"5.1 Color as a tool to distinguish","text":"frequently use color means distinguish discrete items groups intrinsic order, different countries map different manufacturers certain product. case, use qualitative color scale. scale contains finite set specific colors chosen look clearly distinct also equivalent . second condition requires one color stand relative others. , colors create impression order, case sequence colors get successively lighter. colors create apparent order among items colored, definition order.Many appropriate qualitative color scales readily available. Figure 5.1 shows three representative examples. particular, ColorBrewer project provides nice selection qualitative color scales, including fairly light fairly dark colors.19\nFigure 5.1: Example qualitative color scales. Okabe Ito scale default scale used throughout book.20 ColorBrewer Dark2 scale provided ColorBrewer project.21 ggplot2 hue scale default qualitative scale widely used plotting software ggplot2.\nexample use qualitative color scales, consider Figure 5.2. shows percent population growth 2000 2010 U.S. states. arranged states order population growth, colored geographic region. coloring highlights states regions experienced similar population growth. particular, states West South seen largest population increases whereas states Midwest Northeast grown much less.\nFigure 5.2: Population growth U.S. 2000 2010. States West South seen largest increases, whereas states Midwest Northeast seen much smaller increases even, case Michigan, decrease. Data source: U.S. Census Bureau\n","code":"\n\ndata(\"US_census\") # from dviz.supp package\ndata(\"US_regions\") # from dviz.supp package\n\npopgrowth_df <- left_join(US_census, US_regions) %>%\n    group_by(region, division, state) %>%\n    summarize(pop2000 = sum(pop2000, na.rm = TRUE),\n              pop2010 = sum(pop2010, na.rm = TRUE),\n              popgrowth = (pop2010-pop2000)/pop2000,\n              area = sum(area)) %>%\n    arrange(popgrowth) %>%\n    ungroup() %>%\n    mutate(state = factor(state, levels = state),\n           region = factor(region, levels = c(\"West\", \"South\", \"Midwest\", \"Northeast\")))\n\n# make color vector in order of the state\nregion_colors <- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\")\nregion_colors_dark <- darken(region_colors, 0.4)\nstate_colors <- region_colors_dark[as.numeric(popgrowth_df$region[order(popgrowth_df$state)])]\n\nggplot(popgrowth_df, aes(x = state, y = 100*popgrowth, fill = region)) + \n  geom_col() + \n  scale_y_continuous(\n    limits = c(-.6, 37.5), expand = c(0, 0),\n    labels = scales::percent_format(accuracy = 1, scale = 1),\n    name = \"population growth, 2000 to 2010\"\n  ) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() + \n  theme_dviz_vgrid_2(12, rel_small = 1) +\n  theme(axis.title.y = element_blank(),\n        axis.line.y = element_blank(),\n        axis.ticks.length = unit(0, \"pt\"),\n        axis.text.y = element_text(size = 10, color = state_colors),\n        legend.position = c(.56, .68),\n        legend.background = element_rect(fill = \"#ffffffb0\"))"},{"path":"color-basics.html","id":"color-to-represent-data-values","chapter":"5 Color scales","heading":"5.2 Color to represent data values","text":"Color can also used represent data values, income, temperature, speed. case, use sequential color scale. scale contains sequence colors clearly indicate () values larger smaller ones (ii) distant two specific values . second point implies color scale needs perceived vary uniformly across entire range.Sequential scales can based single hue (e.g., dark blue light blue) multiple hues (e.g., dark red light yellow) (Figure 5.3). Multi-hue scales tend follow color gradients can seen natural world, dark red, green, blue light yellow, dark purple light green. reverse, e.g. dark yellow light blue, looks unnatural doesn’t make useful sequential scale.\nFigure 5.3: Example sequential color scales. ColorBrewer Blues scale monochromatic scale varies dark light blue. Heat Viridis scales multi-hue scales vary dark red light yellow dark blue via green light yellow, respectively.\nRepresenting data values colors particularly useful want show data values vary across geographic regions. case, can draw map geographic regions color data values. maps called choropleths. Figure 5.4 shows example mapped annual median income within county Texas onto map counties.\nFigure 5.4: Median annual income Texas counties. highest median incomes seen major Texas metropolitan areas, particular near Houston Dallas. median income estimate available Loving County West Texas therefore county shown gray. Data source: 2015 Five-Year American Community Survey\ncases, need visualize deviation data values one two directions relative neutral midpoint. One straightforward example dataset containing positive negative numbers. may want show different colors, immediately obvious whether value positive negative well far either direction deviates zero. appropriate color scale situation diverging color scale. can think diverging scale two sequential scales stitched together common midpoint, usually represented light color (Figure 5.5). Diverging scales need balanced, progression light colors center dark colors outside approximately either direction. Otherwise, perceived magnitude data value depend whether fell midpoint value.\nFigure 5.5: Example diverging color scales. Diverging scales can thought two sequential scales stitched together common midpoint color. Common color choices diverging scales include brown greenish blue, pink yellow-green, blue red.\nexample application diverging color scale, consider Figure 5.6, shows percentage people identifying white Texas counties. Even though percentage always positive number, diverging scale justified , 50% meaningful midpoint value. Numbers 50% indicate whites majority numbers 50% indicate opposite. visualization clearly shows counties whites majority, minority, whites non-whites occur approximately equal proportions.\nFigure 5.6: Percentage people identifying white Texas counties. Whites majority North East Texas South West Texas. Data source: 2010 Decennial U.S. Census\n","code":"\n# B19013_001: Median household income in the past 12 months (in 2015 Inflation-adjusted dollars)\n\n# EPSG:3083\n# NAD83 / Texas Centric Albers Equal Area\n# http://spatialreference.org/ref/epsg/3083/\ntexas_crs <- \"+proj=aea +lat_1=27.5 +lat_2=35 +lat_0=18 +lon_0=-100 +x_0=1500000 +y_0=6000000 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n\n# -110, -93.5 transformed using texas_crs\ntexas_xlim <- c(558298.7, 2112587)\n\ndata(\"texas_income\") # from dviz.supp package\n\ntexas_income %>% st_transform(crs = texas_crs) %>%\n  ggplot(aes(fill = estimate)) + \n  geom_sf(color = \"white\") + \n  coord_sf(xlim = texas_xlim, datum = NA) +\n  theme_dviz_map_2() + \n  scale_fill_distiller(\n    palette = \"Blues\", type = 'seq', na.value = \"grey60\", direction = 1,\n    name = \"annual median income (USD)\",\n    limits = c(18000, 90000),\n    breaks = 20000*c(1:4),\n    labels = c(\"$20,000\", \"$40,000\", \"$60,000\", \"$80,000\"),\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      label.position = \"bottom\",\n      title.position = \"top\",\n      ticks = FALSE,\n      barwidth = grid::unit(3.0, \"in\"),\n      barheight = grid::unit(0.2, \"in\")\n    )\n  ) +\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 0.5,\n    legend.justification = c(0, 0),\n    legend.position = c(0.02, 0.1)\n  )\ndata(\"texas_race\") # from dviz.supp package\n\ntexas_race %>% st_sf() %>%\n  st_transform(crs = texas_crs) %>%\n  filter(variable == \"White\") %>%\n  ggplot(aes(fill = pct)) +\n  geom_sf(color = \"white\") +\n  coord_sf(xlim = texas_xlim, datum = NA) + \n  theme_dviz_map_2() + \n  scale_fill_continuous_divergingx(\n    palette = \"Earth\",\n    mid = 50,\n    limits = c(0, 100),\n    breaks = 25*(0:4),\n    labels = c(\"0% \", \"25%\", \"50%\", \"75%\", \" 100%\"),\n    name = \"percent identifying as white\",\n    guide = guide_colorbar(\n      direction = \"horizontal\",\n      label.position = \"bottom\",\n      title.position = \"top\",\n      ticks = FALSE,\n      barwidth = grid::unit(3.0, \"in\"),\n      barheight = grid::unit(0.2, \"in\"))) +\n  theme(\n    legend.title.align = 0.5,\n    legend.text.align = 0.5,\n    legend.justification = c(0, 0),\n    legend.position = c(0.02, 0.1)\n  )"},{"path":"color-basics.html","id":"color-as-a-tool-to-highlight","chapter":"5 Color scales","heading":"5.3 Color as a tool to highlight","text":"Color can also effective tool highlight specific elements data. may specific categories values dataset carry key information story want tell, can strengthen story emphasizing relevant figure elements reader. easy way achieve emphasis color figure elements color set colors vividly stand rest figure. effect can achieved accent color scales, color scales contain set subdued colors matching set stronger, darker, /saturated colors (Figure 5.7).\nFigure 5.7: Example accent color scales, four base colors three accent colors. Accent color scales can derived several different ways: (top) can take existing color scale (e.g., Okabe Ito scale, Fig 5.1) lighten /partially desaturate colors darkening others; (middle) can take gray values pair colors; (bottom) can use existing accent color scale, e.g. one ColorBrewer project.\nexample data can support differing stories different coloring approaches, created variant Figure 5.2 now highlight two specific states, Texas Louisiana (Figure 5.8). states South, immediate neighbors, yet one state (Texas) fifth-fastest growing state within U.S. whereas third slowest growing 2000 2010.\nFigure 5.8: 2000 2010, two neighboring southern states Texas Louisiana experienced among highest lowest population growth across U.S. Data source: U.S. Census Bureau\nworking accent colors, critical baseline colors compete attention. Notice drab baseline colors (Figure 5.8). Yet work well support accent color. easy make mistake using baseline colors colorful, end competing reader’s attention accent colors. easy remedy, however. Just remove color elements figure except highlighted data categories points. example strategy provided Figure 5.9.\nFigure 5.9: Track athletes among shortest leanest male professional athletes participating popular sports. Data source: R. D. Telford R. B. Cunningham22\n","code":"\npopgrowth_hilight <- left_join(US_census, US_regions) %>%\n    group_by(region, division, state) %>%\n    summarize(pop2000 = sum(pop2000, na.rm = TRUE),\n              pop2010 = sum(pop2010, na.rm = TRUE),\n              popgrowth = (pop2010-pop2000)/pop2000,\n              area = sum(area)) %>%\n    arrange(popgrowth) %>%\n    ungroup() %>%\n    mutate(region = ifelse(state %in% c(\"Texas\", \"Louisiana\"), \"highlight\", region)) %>%\n    mutate(state = factor(state, levels = state),\n           region = factor(region, levels = c(\"West\", \"South\", \"Midwest\", \"Northeast\", \"highlight\")))\n\n# make color and fontface vector in order of the states\nregion_colors_bars <- c(desaturate(lighten(c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\"), .4), .8), darken(\"#56B4E9\", .3))\nregion_colors_axis <- c(rep(\"gray30\", 4), darken(\"#56B4E9\", .4))\nregion_fontface <- c(rep(\"plain\", 4), \"bold\")\nstate_colors <- region_colors_axis[as.numeric(popgrowth_hilight$region[order(popgrowth_hilight$state)])]\nstate_fontface <- region_fontface[as.numeric(popgrowth_hilight$region[order(popgrowth_hilight$state)])]\n\nggplot(popgrowth_hilight, aes(x = state, y = 100*popgrowth, fill = region)) + \n  geom_col() + \n  scale_y_continuous(\n    limits = c(-.6, 37.5), expand = c(0, 0),\n    labels = scales::percent_format(accuracy = 1, scale = 1),\n    name = \"population growth, 2000 to 2010\"\n  ) +\n  scale_fill_manual(\n    values = region_colors_bars,\n    breaks = c(\"West\", \"South\", \"Midwest\", \"Northeast\")\n  ) +\n  coord_flip() + \n  theme_dviz_vgrid_2(12, rel_small = 1) +\n  theme(\n    text = element_text(color = \"gray30\"),\n    axis.text.x = element_text(color = \"gray30\"),\n    axis.title.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.length = unit(0, \"pt\"),\n    axis.text.y = element_text(\n      size = 10, color = state_colors,\n      face = state_fontface\n    ),\n    legend.position = c(.56, .68),\n    legend.background = element_rect(fill = \"#ffffffb0\")\n  )\ndata(\"Aus_athletes\") # from dviz.supp package\nmale_Aus <- filter(Aus_athletes, sex==\"m\") %>%\n  filter(sport %in% c(\"basketball\", \"field\", \"swimming\", \"track (400m)\",\n                      \"track (sprint)\", \"water polo\")) %>%\n  mutate(sport = case_when(sport == \"track (400m)\" ~ \"track\",\n                           sport == \"track (sprint)\" ~ \"track\",\n                           TRUE ~ sport))\n\nmale_Aus$sport <- factor(male_Aus$sport,\n                         levels = c(\"track\", \"field\", \"water polo\", \"basketball\", \"swimming\"))\n\ncolors <- c(\"#BD3828\", rep(\"#808080\", 4))\nfills <- c(\"#BD3828D0\", rep(\"#80808080\", 4))\n\nggplot(male_Aus, aes(x=height, y=pcBfat, shape=sport, color = sport, fill = sport)) +\n  geom_point(size = 3) +\n  scale_shape_manual(values = 21:25) +\n  scale_color_manual(values = colors) +\n  scale_fill_manual(values = fills) +\n  xlab(\"height (cm)\") +\n  ylab(\"% body fat\") +\n  theme_dviz_grid_2()"},{"path":"color-basics.html","id":"using-color-scales-with-ggplot2","chapter":"5 Color scales","heading":"5.4 Using color scales with ggplot2","text":"HCL-based color palettes colorspace package23 also provided discrete, continuous, binned color scales use ggplot2 package.24The scales called via schemewhere<aesthetic> name aesthetic (fill, color, colour).<datatype> type variable plotted (discrete, continuous, binned).<colorscale> sets type color scale used (qualitative, sequential, diverging).color palettes can listed visualized follows:examples scales illustrated following sections.","code":"scale_<aesthetic>_<datatype>_<colorscale>()\n# get list of palettes\nhcl_palettes()\n#> HCL palettes\n#> \n#> Type:  Qualitative \n#> Names: Pastel 1, Dark 2, Dark 3, Set 2, Set 3, Warm, Cold,\n#>        Harmonic, Dynamic\n#> \n#> Type:  Sequential (single-hue) \n#> Names: Grays, Light Grays, Blues 2, Blues 3, Purples 2,\n#>        Purples 3, Reds 2, Reds 3, Greens 2, Greens 3,\n#>        Oslo\n#> \n#> Type:  Sequential (multi-hue) \n#> Names: Purple-Blue, Red-Purple, Red-Blue, Purple-Orange,\n#>        Purple-Yellow, Blue-Yellow, Green-Yellow,\n#>        Red-Yellow, Heat, Heat 2, Terrain, Terrain 2,\n#>        Viridis, Plasma, Inferno, Rocket, Mako, Dark\n#>        Mint, Mint, BluGrn, Teal, TealGrn, Emrld,\n#>        BluYl, ag_GrnYl, Peach, PinkYl, Burg, BurgYl,\n#>        RedOr, OrYel, Purp, PurpOr, Sunset, Magenta,\n#>        SunsetDark, ag_Sunset, BrwnYl, YlOrRd, YlOrBr,\n#>        OrRd, Oranges, YlGn, YlGnBu, Reds, RdPu, PuRd,\n#>        Purples, PuBuGn, PuBu, Greens, BuGn, GnBu,\n#>        BuPu, Blues, Lajolla, Turku, Hawaii, Batlow\n#> \n#> Type:  Diverging \n#> Names: Blue-Red, Blue-Red 2, Blue-Red 3, Red-Green,\n#>        Purple-Green, Purple-Brown, Green-Brown,\n#>        Blue-Yellow 2, Blue-Yellow 3, Green-Orange,\n#>        Cyan-Magenta, Tropic, Broc, Cork, Vik, Berlin,\n#>        Lisbon, Tofino\n\n# plot the qualitative palette\nhcl_palettes(type = \"qualitative\") %>% \n  plot"},{"path":"color-basics.html","id":"examples","chapter":"5 Color scales","heading":"5.4.1 Examples","text":"discrete qualitative scale applied fill aesthetic corresponds function colorspace::scale_fill_discrete_qualitative():Similarly, color aesthetic discrete qualitative scale corresponds function colorspace::scale_color_discrete_qualitative():continuous sequential scale applied color aesthetic corresponds function colorspace::scale_color_continuous_sequential():continuous sequential scale applied fill aesthetic corresponds function colorspace::scale_fill_continuous_sequential():binned version sequential scale can obtained function colorspace::scale_fill_binned_sequential():continuous diverging scale applied fill aesthetic corresponds function colorspace::scale_fill_continuous_diverging():","code":"\nggplot(iris, aes(x = Sepal.Length, fill = Species)) + \n  geom_density(alpha = 0.6) +\n  colorspace::scale_fill_discrete_qualitative() + \n  cowplot::theme_minimal_hgrid()\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + \n  geom_point() +\n  colorspace::scale_color_discrete_qualitative(palette = \"Set 2\") + \n  cowplot::theme_minimal_grid()\nggplot(iris, aes(x = Species, y = Sepal.Width, color = Sepal.Length)) + \n  geom_jitter(width = 0.2) +\n  colorspace::scale_color_continuous_sequential(palette = \"Heat\") + \n  cowplot::theme_minimal_hgrid()\ndf <- data.frame(height = c(volcano), \n                 x = c(row(volcano)), \n                 y = c(col(volcano)))\np <- ggplot(df, aes(x, y, fill = height)) + \n  geom_raster() + \n  coord_fixed(expand = FALSE)\n\np + colorspace::scale_fill_continuous_sequential(palette = \"Blues\")\np + colorspace::scale_fill_binned_sequential(palette = \"Blues\")\ncm <- cor(mtcars)\ndf <- data.frame(cor = c(cm), var1 = factor(col(cm)), var2 = factor(row(cm)))\nlevels(df$var1) <- levels(df$var2) <- names(mtcars)\nggplot(df, aes(var1, var2, fill = cor)) + \n  geom_tile() + \n  coord_fixed() +\n  ylab(\"variable\") +\n  scale_x_discrete(position = \"top\", name = \"variable\") +\n  colorspace::scale_fill_continuous_diverging(\"Blue-Red 3\")"},{"path":"paras.html","id":"paras","chapter":"6 Statistical Parameters","heading":"6 Statistical Parameters","text":"objectives chapter :Define parameter statistical contextDefine parameter statistical contextSee examples parametersSee examples parametersUnderstand concept parameter relation parameter equationUnderstand concept parameter relation parameter equationBe able set parameter equations isolate directly pinpoint parameter differences absolute relative scales, using regression equation framework.able set parameter equations isolate directly pinpoint parameter differences absolute relative scales, using regression equation framework.fitting (regression) equations data, can focus research objects without data get way.fitting (regression) equations data, can focus research objects without data get way.See unity (generality) course, seeing big picture, .e., forests, trees.See unity (generality) course, seeing big picture, .e., forests, trees.","code":""},{"path":"paras.html","id":"parameters","chapter":"6 Statistical Parameters","heading":"6.1 Parameters","text":"begin defining meant term parameter statistical contextDefinition 6.1  (Parameter) constant (unknown magnitude) (statistical) model. numerical characteristic population, distinguished statistic obtained sampling.25The term can mean things contexts. example, clinical medicine:quantitative aspect/dimension client’s (patient’s) health, subject measurement (means test). (Example: systolic blood-pressure.)26","code":""},{"path":"paras.html","id":"the-statistical-parameters-we-will-be-concerned-with","chapter":"6 Statistical Parameters","heading":"6.1.1 The statistical parameters we will be concerned with","text":"\\(\\mu\\) mean level quantitative characteristic, e.g. depth earth’s ocean height land, height / BMI / blood pressure levels human population. [One also think mathematical physical constants parameters, even though values effectively ‘known.’ Examples agreement many many decimal places include mathematical constant pi, speed light(c), gravitational constant G. speed sound depends medium travelling , temperature medium. freezing boiling points substances water milk depend altitude barometric pressure]. lower level, might interested personal characters, size person’s vocabulary, person’s mean (minimum, typical) reaction time. target person’s ‘true score’ test – value one get one (, realistically) tested (large) number test items test bank, observed/measured continously period interest.\n    Later address sitautions mean \\(\\mu\\) best ‘centre’ distribution, might want take feature, median, quantile, instead.\\(\\mu\\) mean level quantitative characteristic, e.g. depth earth’s ocean height land, height / BMI / blood pressure levels human population. [One also think mathematical physical constants parameters, even though values effectively ‘known.’ Examples agreement many many decimal places include mathematical constant pi, speed light(c), gravitational constant G. speed sound depends medium travelling , temperature medium. freezing boiling points substances water milk depend altitude barometric pressure]. lower level, might interested personal characters, size person’s vocabulary, person’s mean (minimum, typical) reaction time. target person’s ‘true score’ test – value one get one (, realistically) tested (large) number test items test bank, observed/measured continously period interest.\n    Later address sitautions mean \\(\\mu\\) best ‘centre’ distribution, might want take feature, median, quantile, instead.\\(\\pi\\) Prevalence risk (proportion): e.g., proportion earth’s surface covered water, human population untreated hypertension, lacks internet access, develop new health condition next x years. lower level, might interested personal proportions, proportion calories person consumes come fat, proportion year 2020 person spent internet, indoors, asleep, sedentary.\\(\\pi\\) Prevalence risk (proportion): e.g., proportion earth’s surface covered water, human population untreated hypertension, lacks internet access, develop new health condition next x years. lower level, might interested personal proportions, proportion calories person consumes come fat, proportion year 2020 person spent internet, indoors, asleep, sedentary.\\(\\lambda\\) speed events occur: e.g., earthquakes per earth-day, heart attacks traffic fatalities per (population)-year. lower level, might interested personal intensities, mean number tweets/waking-hour person issued year 2020, mean number times per 100 hours use person’s laptop froze needed re-booted.\\(\\lambda\\) speed events occur: e.g., earthquakes per earth-day, heart attacks traffic fatalities per (population)-year. lower level, might interested personal intensities, mean number tweets/waking-hour person issued year 2020, mean number times per 100 hours use person’s laptop froze needed re-booted.three parameters refers characteristic overall domain, entire surface earth, entire ocean, population. indicators distinguishing among subdomains, refer locations / persons otherwise specified. drill later.Especially epidemiologic research, also generally, one can think \\(\\pi\\) \\(\\lambda\\) parameters occurrence (although word occurrence usually time element, can also timeless: frequently word occurs static text, mineral rock). Prevalence proportion current state, 5-year risk expected proportion probability new state 5 years now. parameter \\(\\lambda\\) measures speed elements question move original state.Even though depths ocean, blood pressures, measured quantitative (rather none) scale, one can divide scale finite number bins/caterories, speak prevalence (proportion) category. Conversely, one can use set descriptive parameters called quantiles, .e, landmarks selected proportions, e.g., 0.05 5%, 25%, 50%, 75%, 95% distribution left (‘’) quantiles.","code":""},{"path":"paras.html","id":"occurence-parameters-are-not-constants-of-nature","chapter":"6 Statistical Parameters","heading":"6.1.2 Occurence Parameters are not constants of nature","text":"noted philosophy science science concerned functional relations objects (Friend Feibleman, 1937). proposition quite evidently tenable epidemiologic objects research. Parameters occurrence, incidence rate particular illness, constants nature. Rather, magnitudes generally depend — functions — variety characteristics individuals — constitutional, behavioral, /environmental. relations, even remotely credible, generally objects medical occurrence research. example, one quite usually interested learning whether rate occurrence particular illness depends (related function ) gender — regardless whether express reason surmise might Miettinen.27Example 6.1  prevalence given blood type based ABO antigen system,, constant gender essentially constant age, constant nature. varies ethnic groupings, example. Thus prevalence must quantified relation —function —ethnic group.Example 6.2  occurrence various values blood pressure among people, one descriptive parameter median pressure. (value prevalence exceedance 50%.) parameter, , constant nature depends age characteristics individuals. quantitative nature age relation systolic blood pressure, rule thumb used , mm Hg, 100 plus age years.\" rule expresses regression model - regression function - form P = + B x Age. example, P, occurrence parameter, median systolic blood pressure, = 100 mm Hg, B = 1 mm Hg/yr.Definition 6.2  (Determinants) characteristics magnitude occurrence parameter depends (causally otherwise) determinants parameter. Thus, examples given , ethnic grouping determinant prevalence given blood type, age determinant median systolic blood pressure.Determinant implication causality science — everyday locution: current age person “determined” /year birth (noncausally), just expected outcome disease “determined” treatment used (causally). relation occurrence measure determinant, set determinants, naturally termed occurrence relation occurrence function. relations general objects epidemiologic research.","code":""},{"path":"paras.html","id":"terminology","chapter":"6 Statistical Parameters","heading":"6.1.3 Terminology","text":"go , need adopt sensible terminology referring generically states, traits, conditions behaviours whose category-specific parameter values compared. use term determinant.28 several advantages many terms used different disciplines, exposure, agent, independent/explanatory variable, experimental condition, treatment, intervention, factor, risk factor, predictor.main advantage broader, closer causally neutral connotaion. Exposure environmental connotations, technically refers opportity injest mentally take board substance message. Agent causal connotations. term independent variable suggests investigator control laboratory setting. term explanatory ambiguous mechanism parameter value index category got different value index category. contrasts experimentally formed. term factor, thus term risk factor, avoided word factor derives Latin facere, (action ) , making, creating. Predictor makes one think future. term regressor (shorthand, ‘X’ ) won’t understood lay people.word determine can suggest causality (e.g., demand determines price), also refers fixing form, position, character beforehand: two points determine straight line; area circle determined radius.considerable philosophical debate whether something causes something else. argue extent genetics determines one’s personality causal concept. Others argue since one consider alternative, ones biological sex age can considered causal determinant risk factor (strict causal meaning word). prefer refer risk indicators.\nnow move parameter relations concerned , beginning simplest type.","code":""},{"path":"paras.html","id":"parameter-contrasts","chapter":"6 Statistical Parameters","heading":"6.2 Parameter Contrasts","text":"applied research, seldom interested single constant. Much often interested contrast (difference) parameter values different contexts/locations (Northern hemisphere vs Southern hemisphere), conditions/times (reaction times using right versus left hand, behaviour weekdays versus weekends), sub-domains sub-populations (females vs males). Contrasts involving ‘persons, places, times’ long history epidemiology.section, limit attention contrasts: compariosn parameter values 2 contexts/locations/sub-populations. Thus, parameter function just 2 possible ‘input’ values. next section address general parameter functions.","code":""},{"path":"paras.html","id":"reference-and-index-categories","chapter":"6 Statistical Parameters","heading":"6.2.1 Reference and Index categories","text":"many research contexts, choice ‘reference’ category (starting point, category category compared) obvious: status quo (standard care, prevailing condition usual situation, dominant hand, better known category). ‘index’ category category one curious wishes learn , contrasting parameter value parameter value reference category.contexts, less obvious category serve reference index categories, choice may merely matter persepctive. one famiar Northern hemisphere, serves natural starting point (‘corner’ use terminology Clayton Hills,29 reference category). choice reference category longevity contrast males females, -hospital mortality rates motor vehicle fatality rates weekends versus weekdays, might depend mechanism one wishes bring . one might close reference category one larger amount experience, maybe one lower parameter value, ‘index minus reference’ difference positive quantity, ‘index: reference ratio’ exceeds 1.","code":""},{"path":"paras.html","id":"parameter-relations-in-numbers-and-words","chapter":"6 Statistical Parameters","heading":"6.2.2 Parameter relations in numbers and words","text":"make concrete, use hypothetical (round) numbers pretend ‘know’ true parameter values – example\nmean depth ocean Northern hemisphere (reference category) Southern hemisphere (index category) – 3,600 metres (3.6Km) 4,500 metres (4.5Km) respectively. Thus, difference (South minus North) 900 metres 0.9Km.wished show two parameter values graphically, might using format Figure 6.1 panel (), shows 2 hemisphere-specific parameter values – forces reader calculate difference.Figure 6.1 panel (b) follows reader-friendy format, difference (quantity interest) isolated: original 2 parameters converted 2 new, relevant ones.Figure 6.1 panel (c) encodes relation displayed panel (b) single phrase applies categories: Onto ‘starting value’ 3.8Km, one adds \\(\\Delta \\mu\\) = 0.9 Km resulting parameter pertains Southern hemisphere. 0.9 Km toggled /one moves North South.\nFigure 6.1: Parameter relations numbers words\n","code":""},{"path":"paras.html","id":"parameter-relations-in-symbols-and-with-the-help-of-an-index-category-indicator","chapter":"6 Statistical Parameters","heading":"6.2.3 Parameter relations in symbols, and with the help of an index-category indicator","text":"Panels () (b) Figure 6.2 repeat information panels () (b) Figure 6.1, using Greek letters symbolically represent parameters. Just keep graphics uncluttered, labels North South abbreviated N S used subscripts. Also, brevity, expression \\[\\Delta \\mu =\\mu_S - \\mu_N.\\]relation encoded single phrase shown Figure 6.1 panel (c) compact form suitable verbal communication. representation can adapted suitable computer calculations. (benefit become obvious soon try learn parameter values fitting models actual data.) Depending whether hemisphere question northern southern hemisphere, expression/statement ‘specified hemisphere SOUTHERN hemisphere’ evaluates (logical) FALSE TRUE. binary coding used computers, evaluates 0 1, call 0/1 variable ‘indicator’ variable.speak INDICATOR variables, DUMMY variables. International Statistical Institute’s Dictionary Statistical Terms objects name: term used, rather laxly, denote artificial variable expressing qualitative characteristics. word DUMMY avoided. nothing dummy indicator variable.Definition 6.3  (Indicator Variable) variable 0 1 realizations, realization 1 indicating something particular.encourage use, coding, meaningful variable names .South .Southern (stands indicator ) .Male. Don’t use name sex gender, coding self evident. think .Male , use Male.Figure 6.2 panel (c), just keep graphics uncluttered, name indicator variable SOUTHERN abbreviated S, \\(\\mu_S\\) shorthand \\(\\mu\\) cooresponding whichever value (0 1) \\(S\\) specified (also write \\(\\mu | S\\), \\(\\mu\\) ‘given’ \\(S\\)). Thus, symbol \\(\\mu_0\\) refers \\(\\mu\\) \\(S=0\\), longerhand, \\(\\mu \\ | \\ S = 0\\).\nFigure 6.2: Parameter relations symbols.\nequation panel (c) remind ?Probably equation line. high school may learned form \\(+ B \\times X\\) Miettinen used describe relation median blood pressure age.Today, statistics, equations referred regression equations, statistical model called regression model. term regression unfortunate, since bears little relation original application. concerned phenomenon reversion first described Charles Darwin. Following first studies sizes daughter seeds sweet peas, nephew Francis Galton, described tendency:Offspring tend resemble parent seeds size, always mediocre (‘middling’, closer mean) — smaller parents, parents large; larger parents, parents small.30One first regression lines fitted human data Galton’s line depicting rate regression hereditary stature , using term ‘deviate’ today use ‘Z-scores.’Deviates Children Mid-Parents 2 3.31Because used Z scores (means parents children 0) equation line simplified \\[\\mu(\\textrm{Z-score children parents mean z}) = 0 + (2/3) \\times z\\]","code":""},{"path":"paras.html","id":"dont-we-need-a-cloud-of-points-to-have-a-regression-line","chapter":"6 Statistical Parameters","heading":"6.2.4 Don’t we need a cloud of points to have a regression line?","text":"Although many courses textbooks introduce regression concepts way, answer . nothing regression formulation specifies \\(X\\) values mean \\(Y\\) values determined. Unlike many textbboks start \\(X\\)s continuous scale, later deal 2-point (binary) \\(X\\), starting simplest case, move later.reasons: epidemiology, first simplest contrasts involve just two categories, reference category index category; simple subtraction 2 parameter values easier explain lay person; argument function behaves values 0 1. parameter values Male = 0.4 Male = 1.4, Male=0 Male=1.addition, easier learn fundamental concepts principles regression can easily see exactly going . Fewer blackbox formulae mean transparency understanding.see represent parameter values two determinant-categories, can easily extend two, ethnic groups.see later , value dental health parameter (eg mean number decayed, missing filled DMF teeth) \\(X = 0\\) parts per million fluoride drinking water, another parameter value \\(X = 1\\) parts per million, can look 2 parameter values. enough, need (obtain) parameter values intermediate fluoride levels, levels beyond 1 ppm, trace full parameter relation, namely mean-DMF varies function fluoride levels. large numbers observations level, DMF means trace smooth curve. data limited, trace jumpy/wobbly, probably resort sensible smooth function, coefficients estimated (fitted ) data.discussion leads naturally situations parameter varies quantitative levels determinant - topic considered next section.meantime, need answer question: limit subtraction? consider ratio two parameters, rather difference?","code":""},{"path":"paras.html","id":"relative-differences-ratios-in-numbers-first","chapter":"6 Statistical Parameters","heading":"6.2.5 Relative differences (ratios) in numbers first","text":"ratio can helpful difference, especially don’t sense large parameter value even reference category. example, average, many red blood cells men women? much faster gamers’ reaction times compared nongamers?Recall hypothetical mean ocean depths, 3.6 Km oceans Northern hemisphere (reference category) 4.5Km oceans Southern hemisphere (index category). Thus, S:N (South divided North) ratio 4.5/3.6 1.25.Figure 6.3 panel () leaves reader calculate ratio parameter values. panel (b) ratio (quantity interest) isolated: , original 2 parameters converted 2 new, relevant ones. Figure 6.3 panel (c) shows single master-equation applies hemispheres togging /ratio 4.5/3.6.\nFigure 6.3: Relative differences (ratios) numbers first.\n","code":""},{"path":"paras.html","id":"relative-differences-ratios-in-symbols","chapter":"6 Statistical Parameters","heading":"6.2.6 Relative differences (ratios) in symbols","text":"rewrite numbers symbolic equation suitable computer, convert logical ‘South’ numerical Southern-hemisphere-indicator, using binary variate \\(S\\) (short Southern) takes value 0 Northern hemisphere, 1 Southern hemisphere.go back long-forgotten mathematics high school able tell computer toggle ratio . Recall powers numbers, , example,\n\\(y\\) power 2, \\(y^2\\) square \\(y\\). two powers exploit 0 1.\n\\(y\\) power 1, \\(y^1\\) just \\(y\\) \\(y\\) power 0, \\(y^0\\) 1.take advantage write\n\\[\\mu_S = \\mu \\ | \\ S  \\ = \\mu_0 \\ \\times \\  \\Big\\{ \\frac{\\mu_{South}}{\\mu_{North}}\\Big\\}^S = \\mu_0 \\ \\times Ratio \\ ^ S.\\]can check works hemisphere setting \\(S=0\\) \\(S=1\\) turn. Thus, \\[\\log(y^S) = S \\times \\log(y)\\]\nFigure 6.4: Relative differences (ratios) symbols.\nAlthough compact direct way express parameter relation, well suited fitting equations data. However, high school mathematics courses, also learned logarithms. example, \n\\[\\log(\\times B) = \\log() + \\log(B); \\ \\  \\log(y^x) = x \\times \\log(y).\\]Thus, can rewrite equation Figure 6.4 panel (c) \\[\\log(\\mu_S) = \\log(\\mu \\ | \\ S)  \\ = \\underbrace{\\log(\\mu_0)} \\ +  \\underbrace{\\log(Ratio)} \\times \\ S.\\]\nlinear two parameters form one parameter difference: parameters \n\\(\\underbrace{\\log(\\mu_0)}\\) \\(\\underbrace{\\log(Ratio)}\\) made following ‘linear compound’ ‘linear predictor’:\n\\[\\log(\\mu_S) = \\log(\\mu \\ | \\ S)  \\ = \\underbrace{\\log(\\mu_0)} \\times \\ 1 \\ + \\underbrace{\\log(Ratio)} \\times \\ S.\\]course concerned using regression software fit/estimate 2 parameters \\(n\\) depth measurements indexed \\(S\\).","code":""},{"path":"paras.html","id":"parameter-functions","chapter":"6 Statistical Parameters","heading":"6.3 Parameter functions","text":"simple example function describes parameter values vary quantitative levels determinant straight line shown upper right panel next figure. determinant generic name X, equation \\(+ B \\times X\\) \\(B_0 + B_1 \\times X\\) \\(\\beta_0 + \\beta_1 \\times X\\) straight line form. Miettinen used convention upper case letters \\(\\) \\(B\\) used denote (true unknown) coefficient values, whereas lower case leters \\(\\) \\(b\\) used denote empirical counterparts, sometimes called estimated coefficients fitted coefficients. sensible simple convention also avoids need, one uses Greek letters theoretical coefficient values, put ‘hats’ refer empirical counterparts, ‘estimate/fit’ . Fortunately, journals don’t usually allow investigators use ‘beta-hats’; means investigators careful words terms.go left right Figure 6.5, models become complex. simplest one left, column 1, one JH refers mother regression models. refers single overall situation/population/domain, \\(X \\equiv 1\\), takes value 1 /every instance/member. parameter equation \\(\\mu_X = \\mu \\ | \\ X \\ = \\mu \\times 1.\\) column 2, 2 subdomains, indexed 2 values determinant (generically called ‘X’), namely \\(X = 0\\) \\(X = 1.\\) 3rd column, number parameters left unspecified, since numbers coefficients specify line/curve might vary 1 (describing volume cube dependeds , function , radius) 2 (straight line go origin, symmetric S curve) 2 (e.g., non-symmetric S curve, quadratic shape).\nFigure 6.5: Grid parameter contrasts aim cover course.\nremarks panels FigureThe 3 rows refer 3 core parameters given examples . 3 governed principles, although possibilities different possible scales parameters.3 rows refer 3 core parameters given examples . 3 governed principles, although possibilities different possible scales parameters.column 1: just 1 parameter (value shown dot) corresponding overall population entire domain. can think limiting degenerate case columns right. One can still write regression model.column 1: just 1 parameter (value shown dot) corresponding overall population entire domain. can think limiting degenerate case columns right. One can still write regression model.one parameter model sometimes referred null incercept regression model. exploit idea take holistic/general economical approach introductory course. Many textbooks/courses mention regression models quite late, spend lot time 1-sample (even 2-sample) problems without pointing merely sub-cases regression models. silos practice promoting/learning separate software routine dealing 1 sample problem, one can get answer regression routine, leads dead ends wastes time.\nget fitting/estimating mean (proportion rate) parameter /data, encourage within regression framework.setting (column) 2, 2 parameter values, one reference category one index category determinant. seen, relate can can expressed number different ways. common useful way via parameter equation contains parameter reference category comparative parameter (measure difference two parameters) – latter often interest.setting (column) 2, 2 parameter values, one reference category one index category determinant. seen, relate can can expressed number different ways. common useful way via parameter equation contains parameter reference category comparative parameter (measure difference two parameters) – latter often interest.setting (column) 3, parameter equation traces parameter continuum possible values determinant, using many coefficients needed. particular diagram, values determinant (X) shown starting X = 0, . data analysis, one often shifts X origin, ‘intercept’ makes sense. example, one plotting world temperatures, ice-melting dates (see Chapter Computning) calendar year, better incercept refer fitted temperature series begins, rather current Western calendat begins (year 0 AD). Likewise, describing relation ideal weight height good start near people’s heights . Thus, ‘100 pounds height 5 feet, five additional pounds added inch height’ women, \n‘106 pounds height 5 feet, six additional pounds every added inch height.’ men. course, wish, women use mathematically equivalent\n‘-300 pounds height 0 feet, five additional pounds added inch height’ easy remember, doesn’t apply much (unspecified) height range!setting (column) 3, parameter equation traces parameter continuum possible values determinant, using many coefficients needed. particular diagram, values determinant (X) shown starting X = 0, . data analysis, one often shifts X origin, ‘intercept’ makes sense. example, one plotting world temperatures, ice-melting dates (see Chapter Computning) calendar year, better incercept refer fitted temperature series begins, rather current Western calendat begins (year 0 AD). Likewise, describing relation ideal weight height good start near people’s heights . Thus, ‘100 pounds height 5 feet, five additional pounds added inch height’ women, \n‘106 pounds height 5 feet, six additional pounds every added inch height.’ men. course, wish, women use mathematically equivalent\n‘-300 pounds height 0 feet, five additional pounds added inch height’ easy remember, doesn’t apply much (unspecified) height range!remarks associated terminologyInstead regression models, textbooks courses refer linear models :Definition 6.4  (Linear model) Formulation mean/expectation (distribution ) random variate (Y) linear compound set \\(B_0 , B_1 , B_2 , \\dots\\) parameters: \\(B_0 + B_1 X_1 + B_2 X_2 + \\dots\\).32The meaning ‘linear’ appellation model nothing straight lines; refers mathematical concept linear compound: given quantities Q\\(_1\\), Q\\(_2\\), etc., linear compound sum C\\(_1\\)Q\\(_1\\) + C\\(_2\\)Q\\(_2\\) + …, C\\(_1\\) etc. ‘coefficients’ define particular linear compound set quantities constituted Qs., ‘general linear model’ linear sense dependent parameter, M, formulated linear compound independent parameters B0, B1, etc., coefficients linear compound 1, X1, etc. model , way, ‘linear parameters.’","code":""},{"path":"paras.html","id":"analysis-of-variance","chapter":"6 Statistical Parameters","heading":"6.4 Analysis of Variance","text":"Statistics courses social sciences, biological laboratory sciences, experimentally-based sciences, typically move 1- 2-sample procedures (unfortunately, mainly focusing statistical tests) ‘analysis variance’ modelsMiettinen explains ’analysis variance models way:33In ‘analysis variance model,’ random variate issue – Gaussian – mean whose value depends nominal-scale determinant, nominal scale characterized discrete categories without natural order among . names (nominal) categories, N number, Category 1, Category 2, … , Category N. term model misnomer. , issue analysis synthesis data, synthesis directed learning variance random variate; focuses mean, relation mean (nominal-scale) determinant .simple example models might address mean systemic blood-pressure – defined weighted average diastolic systolic pressures weights 2/3 1/3, respectively – relation ethnicity, represented three categories. ‘analysis-- variance’ model define random variate (Y) representing numerical value pressure (statistical variates inherently numerical) Gaussian distribution means M1, M2, M3 ethnicity categories 1, 2, 3, respectively, variance distribution invariant among . random variate (Y) ‘dependent’ variate meaning value mean depends ethnicity; ethnicity categories represented terms suitably-defined ‘independent’ – non-random – variates (Xs).form ‘analysis--variance’ model simple example : \\(M = B_0 + B_1 X_1 + B_2 X_2\\),\n\\(M\\) mean \\(Y\\) two independent variates indicators two particular ones three ethnic categories. One possibility framework take \\(X_1\\) \\(X_2\\) indicators Category 2 Category 3, respectively – indicator variate one takes value 1 category indicates, 0 otherwise.terms model, \\(B_0\\) value \\(M\\) \\(X_1 = X_2 = 0\\), , Category 1 (.e., \\(B_0 = M_1\\)); Category 2 Category 3 values \\(M\\) represented \\(B_0 + B_1\\) \\(B_0 + B_2\\), respectively (.e., \\(M_2 = B_0 + B_1\\), \\(M_3 = B_0 + B_2\\)). Thus, difference \\(M_1\\) \\(M_2\\) represented \\(B_1\\); \\(B_2\\) represents difference \\(M_1\\) \\(M_3\\); difference \\(M_2\\) \\(M_3\\) difference \\(B_1\\) \\(B_2\\).‘analysis--variance’ framework feasible accommodate, jointly, whatever number nominal-scale determinants magnitude mean dependent variate. simple example addition two categories gender consideration jointly three categories ethnicity. two determinants jointly imply single nominal-scale determinant six categories (three categories ethnicity split two subcategories based gender).involved definition independent variates single determinant, model said ‘one-way analysis variance’; two determinants corresponding term (naturally) ‘two-way analysis variance’; etc.","code":""},{"path":"paras.html","id":"phraseology-to-avoid","chapter":"6 Statistical Parameters","heading":"6.5 Phraseology to avoid","text":"quite common hear regression coefficient (fitted theoretical) interpreted way:“every 1 unit increase X, ‘Y’ parameter increases \\(\\beta_X\\) units.”follows“() increase X 1 unit, increase Y parameter \\(\\beta_X\\) units.”pick terminology early, maybe even back high school, people around us. , interpreting B = 1 mm Hg/yr Miettenen’s example (100 plus age years), use phrases?, since don’t know source , data behind rule, can take look distributions anthropometric chacacteristics (height, weight, forced expiratory volume, FEV) measured cross-sectionally, different populations – Busselton, Australia rural Southwest Ethiopia – 1972 1992. eye, try estimate slope get regressed age-sex-specific means medians\nages. summarize gradient across age.Remember subjecsts aren’t aging going anywhere, nobody watching age.accurate say:People aged +1 years time survey heights/weights/FEVs t.tt units higher/lower people aged years.orThe mortality rate u.u units higher/lower (u.u times higher/lower) experience index category reference category.way, telling reader static source, dynamic situation conditions manipulated investigators, subjects watched ages go [many readers, word ‘increased’ implies human force deliberately changed dial, turned X , one temperature humidity laboratory.]One JH’s favourite examples people misled thinking cross-sectional dataset allows say ‘people get older, …’ McGill epidemiology department’s studies, 1960s, health 10,000 millers miners asbestos. workers born 1890 1920. cross-sectional studies, gradients mean height across attained age. easy give ‘people get older, shrink height’ ‘gain height’ interpretation. easy overlook fact children adolescents Depression.","code":""},{"path":"paras.html","id":"exercises","chapter":"6 Statistical Parameters","heading":"6.6 Exercises","text":"far, dealt equations involving difference ratio two \\(\\mu\\) parameters.Extend graphs equations (difference means ratio means) \\(\\pi\\) parameter. Use example proportions surfaces Northern (reference category) Southern hemisphere (index category) covered water, .e, \\(\\pi_{North}\\) \\(\\pi_{South}.\\) Use hypothetical values \\(\\pi_{North} = 0.65\\) \\(\\pi_{South} = 0.75.\\)34Instead focusing proportions covered water, focus proportions covered land. difference two proportions relate difference calculated 1?35How ratio two proportions relate ratio calculated Q2? .e., one reciprocal ?36Can think different scale, ratio focus land just reciprocal ratio focus water?37If can, show log ratio focus land just negative log ratio focus water?38Extend graphs equations (difference means ratio means) \\(\\lambda\\) parameter. Use example mean number earthquakes per year Northern (reference category) Southern hemisphere (index category) , .e, \\(\\lambda_{North}\\) \\(\\lambda_{South}.\\) Use hypothetical values \\(\\lambda_{North} = 5.0\\) \\(\\lambda_{South} = 7.5\\)39","code":""},{"path":"ChapProbability.html","id":"ChapProbability","chapter":"7 Probability","heading":"7 Probability","text":"section adapted book Introduction Statistical Thinking (R, Without Calculus)40","code":""},{"path":"ChapProbability.html","id":"student-learning-objective","chapter":"7 Probability","heading":"7.1 Student Learning Objective","text":"section extends notion variability introduced \ncontext data situations. variability entire\npopulation concept random variable discussed. \nconcepts central development interpretation \nstatistical inference. end chapter student :Consider distribution variable population compute\nparameters distribution, mean standard\ndeviation.Consider distribution variable population compute\nparameters distribution, mean standard\ndeviation.Become familiar concept random variable.Become familiar concept random variable.Understand relation distribution population\ndistribution random variable produced sampling \nrandom subject population.Understand relation distribution population\ndistribution random variable produced sampling \nrandom subject population.Identify distribution random variable simple settings\ncompute expectation variance.Identify distribution random variable simple settings\ncompute expectation variance.","code":""},{"path":"ChapProbability.html","id":"different-forms-of-variability","chapter":"7 Probability","heading":"7.2 Different Forms of Variability","text":"Section 1 examined variability data. \nstatistical context, data obtained selecting sample \ntarget population measuring quantities interest \nsubjects belong sample. Different subjects sample may\nobtain different values measurement, leading variability \ndata.variability may summarized aid frequency table,\ntable relative frequency, via cumulative relative\nfrequency. graphical display variability data may \nobtained aid bar plot, histogram, boxplot. example, Figure 7.1 shows boxplot clutch.volume (frog dataset oibiostat package) alongside vertical dot plot.\nFigure 7.1: boxplot dot plot clutch.volume. horizontal dashes indicate bottom 50% data open circles represent top 50%.\nNumerical summaries may computed order characterize main\nfeatures variability. used mean median \norder identify location distribution. sample\nvariance, better yet sample standard deviation, well \ninter-quartile range described tools quantify \noverall spread data.aim graphical representations numerical summaries\ninvestigate variability data.subject chapter introduce two forms \nvariability, variability associated, least directly,\ndata observe. first type variability \npopulation variability. type variability \nvariability random variable.notions variability presented abstract, \ngiven terms data observe, \nmathematical-theoretical flavor . first, abstract\nnotions may look waste time may seem \nunrelated subject matter course. opposite true. \ncore statistical thinking relating observed data \ntheoretical abstract models phenomena. Via comparison, \nusing tools statistical inference presented \nsecond half book, statisticians can extrapolate insights make\nstatements regarding phenomena basis observed data.\nThereby, abstract notions variability introduced \nchapter, extended subsequent chapters, essential foundations practice \nstatistics.first notion variability variability associated\npopulation. similar nature variability \ndata. difference two types variability \nformer corresponds variability quantity interest\nacross members population \nselected sample.examine data set heights_sample.csv contains data sex height sample \n100 observations. also consider sex height \nmembers population sample selected.\nsize relevant population 100,000, including 100\nsubjects composed sample. examine values \nheight across entire population can see different people may\ndifferent heights. variability heights \npopulation variability.abstract type variability, variability random variable, mathematical concept. aim concept model\nnotion randomness measurements uncertainty regarding\noutcome measurement. particular initially consider\nvariability random variable context selecting one\nsubject random population.Imagine population size 100,000 select\nrandom one subject population. intend measure \nheight subject selected. Prior selection \nmeasurement certain value height obtained.\nOne may associate notion variability uncertainty — different\nsubjects selected may obtain different evaluations \nmeasurement know hand subject \nselected. resulting variability variability random\nvariable.Random variables can defined abstract settings. aim \nprovide models randomness uncertainty measurements. Simple\nexamples abstract random variables provided \nchapter. examples introduced subsequent chapters.\nabstract examples random variables need associated\nspecific population. Still, definitions used\nexample random variable emerges result \nsampling single subject population apply \nabstract constructions.types variability, variability data dealt \nwell two types variability, can displayed\nusing graphical tools characterized numerical summaries.\nEssentially type plots numerical summaries, possibly\nmodifications, may applied.point remember variability data relates \nconcrete list data values presented us. contrary \ncase variability data, types variability \nassociated quantities actually get observe. data \nsample get see data rest \npopulation. Yet, can still discuss variability population\n, even though observe list \nmeasurements entire population. (example give \nchapter population artificially constructed serves \nillustration . actual statistical context one obtain\nmeasurements entire population, subjects \nwent sample.) discussion variability context\ntheoretical nature. Still, theoretical discussion \ninstrumental understanding statistics.","code":""},{"path":"ChapProbability.html","id":"variability-of-a-population","chapter":"7 Probability","heading":"7.3 Variability of a population","text":"section introduce variability population present\nnumerical summaries characterizes variability. \n, let us review aid example numerical\nsummaries used characterization variability \ndata.Recall file heights_sample.csv contains data height sex \n100 subjects (data file can obtained ). read \ncontent file data frame name heights_sample apply \nfunction summary data frame:applied numeric sequence,\nfunction summary produces smallest largest values \nsequence, three quartiles (including median) mean. \ninput function factor outcome \nfrequency data levels factor. sex\nfactor two levels. summary can see 54 \nsubjects sample female 46 male.Notice input function summary data frame,\ncase example, output summary \nvariables data frame. example two variables\nnumeric (id height) one variable factor\n(sex).Recall mean arithmetic average data \ncomputed summing values variable dividing \nresult number observations. Hence, \\(n\\) number \nobservations (\\(n=100\\) example) \\(y_i\\) value \nvariable subject \\(\\), one may write mean formula form\n\\[\\bar y = \\frac{\\mbox{Sum values data}}{\\mbox{Number values data}} = \\frac{\\sum_{=1}^n y_i}{n}\\;,\\]\\(\\bar y\\) corresponds mean data symbol\n\\(\\sum_{=1}^n y_i\\) corresponds sum values data.median computed ordering data values selecting value\nsplits ordered data two equal parts. first third\nquartile obtained splitting halves two\nquarters.Let us discuss variability associated entire target\npopulation. file heights_population.csv contains population data can\nobtained . \nCSV file contains information sex height entire\nadult population imaginary city. (data heights_sample\ncorresponds sample city.) Read population data \nR examine :object heights_population data frame structure data\nframe heights_sample. contains three variables: unique identifier \nsubject (id), sex subject (sex), height\n(height). Applying function summary data frame produces\nsummary variables contains. particular, \nvariable sex, factor, produces frequency \ntwo categories – 48,888 female 51,112 – total 100,000 subjects.\nvariable height, numeric variable, produces\nextreme values, quartiles, mean.\nFigure 7.2: Bar Plot Height\nLet us concentrate variable height. bar plot \ndistribution heights entire population given \nFigure 7.2. Recall vertical bar placed\nvalue height appears population, \nheight bar representing frequency value \npopulation. One may read graph obtain numerical\nsummaries variable takes integer values range \n117 217 (heights rounded nearest centimeter). \ndistribution centered 170 centimeter, central 50% \nvalues spreading 162 178 centimeters.mean height entire population equal 170\ncentimeter. mean, just like mean distribution data,\nobtained summation heights population\ndivided population size. Let us denote size entire\npopulation \\(N\\). example \\(N = 100,000\\). (size \nsample data called \\(n\\) equal \\(n=100\\) \nparallel example deals data sample.) mean \nentire population denoted Greek letter \\(\\mu\\) read\n“mew”. (average data denoted \\(\\bar y\\)). formula \npopulation mean :\\[\\mu = \\frac{\\mbox{Sum values population}}{\\mbox{Number values population}}= \\frac{\\sum_{=1}^N y_i}{N}\\;.\\]Observe similarity definition mean data\ndefinition mean population. cases \narithmetic average computed. difference case\nmean data computation respect values\nappear sample whereas population values \npopulation participate computation.actual life, values variable \nentire population. Hence, able compute actual\nvalue population mean.However, still meaningful talk\npopulation mean number exists, even though \nknow value . matter fact, one issues statistics try estimate unknown quantity basis data sample.characteristic distribution entire population called \nparameter. Hence, \\(\\mu\\), population average, parameter. \nexamples parameters population median population\nquartiles. parameters defined exactly like data\ncounterparts, respect values entire population\ninstead observations sample alone. See Section 6 details statistical parameters.Another example parameter population variance. Recall \nsample variance defined aid deviations\n\\(y_i - \\bar y\\), \\(y_i\\) value measurement \n\\(\\)th subject \\(\\bar y\\) mean data. order compute\nsample variance deviations squared produce squared\ndeviations. squares summed divided sample\nsize minus one (\\(n-1\\)). sample variance, computed data, \ndenoted \\(s^2\\).population variance defined similar way. First, \ndeviations population mean \\(y_i - \\mu\\) considered \nmembers population. deviations squared \naverage squares computed. denote parameter \n\\(\\sigma^2\\) (read “sigma square”). minor difference \nsample variance population variance latter \ndivide sum squared deviations population size (\\(N\\))\npopulation size minus one (\\(N-1\\)):\\[\\begin{aligned}\n\\sigma^2 =& \\mbox{average square deviation population}\\\\\n         =& \\frac{\\mbox{Sum squares deviations population}}{\\mbox{Number values population}}\\\\\n         =& \\frac{\\sum_{=1}^N (y_i-\\mu)^2}{N}\\;.\\end{aligned}\\]standard deviation population, yet another parameter, \ndenoted \\(\\sigma\\) equal square root variance. \nstandard deviation summarizes overall variability measurement\nacross population. , typical situation \nknow actual value standard deviation population\n. Yet, may refer quantity may try estimate \nvalue based data sample.height subjects imaginary city get \nvariance equal \\(\\sigma^2 =126.1576\\). standard deviation \nequal \\(\\sigma = \\sqrt{126.1576} = 11.23199\\). quantities can \ncomputed example data frame heights_population aid \nfunctions var sd, respectively41.","code":"\nlibrary(dplyr)\nlibrary(rio)\nheights_sample <- rio::import(here::here(\"inst/data/heights_sample.csv\"))\nheights_sample <- heights_sample %>% \n  dplyr::mutate(sex = factor(sex))\nsummary(heights_sample)\n#>        id              sex         height     \n#>  Min.   :1538611   FEMALE:54   Min.   :117.0  \n#>  1st Qu.:3339583   MALE  :46   1st Qu.:158.0  \n#>  Median :5105620               Median :171.0  \n#>  Mean   :5412367               Mean   :170.1  \n#>  3rd Qu.:7622236               3rd Qu.:180.2  \n#>  Max.   :9878130               Max.   :208.0\nheights_population <- rio::import(here::here(\"inst/data/heights_population.csv\"))\nheights_population <- heights_population %>% \n  dplyr::mutate(sex = factor(sex))\nsummary(heights_population)\n#>        id              sex            height   \n#>  Min.   :1000082   FEMALE:48888   Min.   :117  \n#>  1st Qu.:3254220   MALE  :51112   1st Qu.:162  \n#>  Median :5502618                  Median :170  \n#>  Mean   :5502428                  Mean   :170  \n#>  3rd Qu.:7757518                  3rd Qu.:178  \n#>  Max.   :9999937                  Max.   :217\nlibrary(ggplot2)\nlibrary(cowplot)\nggplot2::theme_set(cowplot::theme_cowplot())\np <- ggplot(data = heights_population, \n            mapping = aes(x = height))\np + geom_bar()"},{"path":"ChapProbability.html","id":"variability-of-a-random-variable","chapter":"7 Probability","heading":"7.4 Variability of a random variable","text":"previous section dealt variability population.\nNext consider variability random variable. example,\nconsider taking sample size \\(n=1\\) population (single\nperson) measuring /height. apply function dplyr::sample_n sample one row data frame:first entry function data frame. \nset second argument size = 1 function selects one \nentries data frame random, entry \nlikelihood selected. Let us run function :instance entry different value selected. Try \nrun command several times see get. \nnecessarily obtain different value run?Now let us enter command without pressing return key:Can tell, pressing key, value get?answer question course “”. 100,000\nentries. principle, \nvalues may selected way telling advance \nvalues turn outcome.random variable future outcome measurement, \nmeasurement taken. specific value, rather\ncollection potential values distribution values.measurement taken specific value revealed \nrandom variable ceases random variable! Instead, becomes\ndata.Although one able say outcome random variable\nturn . Still, one may identify patterns potential\noutcome. example, knowing distribution heights \npopulation ranges 117 217 centimeter one may say advance\noutcome measurement must also interval.\nMoreover, since total 3,476 subjects height equal \n168 centimeters since likelihood subject selected\nequal likelihood selecting subject height \n3,476/100,000 = 0.03476. context random variables call \nlikelihood probability. vain, frequency subjects\nhight 192 centimeter 488, therefore probability \nmeasuring height 0.00488. frequency subjects \nheight 200 centimeter 393, hence probability \nobtaining measurement range 200 217 centimeter \n0.00393.","code":"\nheights_population %>% \n  dplyr::sample_n(size = 1)\n#>        id  sex height\n#> 1 7984250 MALE    186\nheights_population %>% \n  dplyr::sample_n(size = 1)\n#>        id    sex height\n#> 1 5407059 FEMALE    166\nheights_population %>% \n  dplyr::sample_n(size = 1)"},{"path":"ChapProbability.html","id":"sample-space-and-distribution","chapter":"7 Probability","heading":"7.4.1 Sample Space and Distribution","text":"Definition 7.1  (Random Variable) random variable refer numerical values, typically outcome \nobservation, measurement, function thereof.random variable characterized via collection potential\nvalues may obtain, known sample space likelihood \nobtaining values sample space (namely, \nprobability value). given example, sample space\ncontains 94 integer values marked \nFigure 7.2. probability value height\nbar value, divided total frequency 100,000\n(namely, relative frequency population).denote random variables capital Latin letters \\(X\\),\n\\(Y\\), \\(Z\\). Values may obtain marked small Latin\nletters \\(x\\), \\(y\\), \\(z\\). probability values use\nletter “\\(\\operatorname{P}\\)”. Hence, denote \\(Y\\) measurement \nheight random individual sampled given population\n:\\[\\operatorname{P}(Y = 168) = 0.03476\\] \n\\[\\operatorname{P}(Y \\geq 200) = 0.00393\\;.\\]Consider, yet another example, probability height \nrandom person sampled population differs 170 centimeter \n10 centimeters. (words, height \n160 180 centimeters.) Denote \\(Y\\) height random\nperson. interested probability\n\\(\\operatorname{P}(|Y -170| \\leq 10)\\).42The random person can subjects population \nequal probability. Thus, sequence heights 100,000\nsubjects represents distribution random variable \\(Y\\):Notice object Y sequence length 100,000 stores\nheights population. probability seek \nrelative frequency sequence values 160 180. First\ncompute probability explain method computation:get height person randomly sampled population\n160 180 centimeters probability 0.64541.Let us produce small example help us explain computation\nprobability. start forming sequence 10 numbers:goal compute proportion numbers range\n\\([4,6]\\) (, equivalently, \\(\\{|Y-5| \\leq 1\\}\\)).function abs computes absolute number input argument.\nfunction applied sequence Y-5 produces \nsequence length distances components \nY number 5:Compare resulting output original sequence. first value\ninput sequence 6.3. distance 5 indeed 1.3. \nfourth value input sequence 3.4. difference 3.4 - 5 \nequal -1.6, absolute value taken get distance \n1.6.function <= expects argument right argument \nleft. compares component left parallel\ncomponent right returns logical value, TRUE \nFALSE, depending whether relation tested holds \n:Observe example function <= produced 10 logical\nvalues, one elements sequence left .\nfirst input sequence Y 6.3, one\nunit away 5. Hence, first output logical expression \nFALSE. hand, last input sequence Y \n5.3, within range. Therefore, last output \nlogical expression TRUE.Next, compute proportion TRUE values sequence:sequence logical values entered function\nmean function replaces TRUE’s 1 FALSE’s\n0. average produces relative frequency TRUE’s \nsequence required. Specifically, example 4\nTRUE’s 6 FALSE’s. Consequently, output final\nexpression \\(4/10 = 0.4\\).computation probability sampled height falls within\n10 centimeter 170 based code. differences \ninput sequence Y replaced sequence population\nheights Y input. number 5 replaced number\n170 number 1 replaced number 10. \ncases result computation relative proportion \ntimes values input sequence fall within given range \nindicated number.probability function random variable defined value\nrandom variable may obtain produces distribution \nrandom variable. probability function may emerge relative\nfrequency given example may result theoretical\nmodeling. Examples theoretical random variables presented subsequent chapters.Consider example random variable. sample space \nprobability function specify distribution random variable.\nexample, assume known random variable \\(Y\\) may obtain\nvalues 0, 1, 2, 3. Moreover, imagine known \n\\(\\operatorname{P}(Y=1) = 0.25\\), \\(\\operatorname{P}(Y=2) = 0.15\\), \\(\\operatorname{P}(Y=3)= 0.10\\). \n\\(\\operatorname{P}(Y=0)\\), probability \\(Y\\) equal 0?sample space, collection possible values random\nvariable may obtain collection \\(\\{0,1,2,3\\}\\). Observe \nsum positive values :\\[\\operatorname{P}(Y > 0) = \\operatorname{P}(Y=1) + \\operatorname{P}(Y=2) + \\operatorname{P}(Y=3) = 0.25 + 0.15 + 0.10 = 0.50\\;.\\]\nfollows, since sum probabilities entire sample space\nequal 1, \\(\\operatorname{P}(Y=0) = 1- 0.5 = 0.5\\).Table 7.1: Distribution \\(Y\\)Table 7.1 summarizes distribution random\nvariable \\(Y\\). Observe similarity probability function\nnotion relative frequency discussed \nSection 1. quantities describe distribution. \nnon-negative sum 1. Likewise, notice one may define \ncumulative probability way cumulative relative frequency \ndefined: Ordering values random variable smallest \nlargest, cumulative probability given value sum \nprobabilities values less equal given value.Knowledge probabilities random variable (cumulative\nprobabilities) enables computation probabilities \nassociated random variable. example, considering random\nvariable \\(Y\\) Table 7.1, may calculate \nprobability \\(Y\\) falling interval \\([0.5, 2.3]\\). Observe \ngiven range contains two values sample space, 1 2,\ntherefore:\\[\\operatorname{P}(0.5 \\leq Y \\leq 2.3) = \\operatorname{P}(Y=1) + \\operatorname{P}(Y = 2) = 0.25 + 0.15 = 0.40\\;.\\]\nLikewise, may produce probability \\(Y\\) obtaining odd value:\\[\\operatorname{P}(Y = \\mbox{odd}) = \\operatorname{P}(Y=1) + \\operatorname{P}(Y=3) = 0.25 + 0.10 = 0.35\\;.\\]\nObserve \\(\\{0.5 \\leq Y \\leq 2.3\\}\\) \\(\\{Y = \\mbox{odd}\\}\\)\nrefer subsets values sample space. subsets denoted\nevents. examples probability event computed \nsummation probabilities associated values belong \nevent.","code":"\nY <- heights_population$height\nmean(abs(Y-170) <= 10)\n#> [1] 0.64541\nY <- c(6.3, 6.9, 6.6, 3.4, 5.5, 4.3, 6.5, 4.7, 6.1, 5.3)\nabs(Y-5)\n#>  [1] 1.3 1.9 1.6 1.6 0.5 0.7 1.5 0.3 1.1 0.3\nabs(Y - 5) <= 1\n#>  [1] FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n#> [10]  TRUE\nmean(abs(Y - 5) <= 1)\n#> [1] 0.4"},{"path":"ChapProbability.html","id":"expectation-and-standard-deviation","chapter":"7 Probability","heading":"7.4.2 Expectation and Standard Deviation","text":"may characterize center distribution random variable\nspread distribution ways similar used \ncharacterization distribution data distribution \npopulation.expectation marks center distribution random\nvariable. equivalent data average \\(\\bar y\\) \npopulation average \\(\\mu\\), used order mark location\ndistribution data population, respectively.average data can computed weighted average values present\ndata, weights given relative frequency. Specifically,\ndata\\[1,\\; 1,\\; 1,\\; 2,\\; 2,\\; 3,\\; 4,\\; 4,\\; 4,\\; 4,\\; 4,\\] mean can calculated via\\[\\frac{1 + 1 + 1 + 2 + 2 + 3 + 4 + 4 + 4 + 4 + 4}{11} =\n1\\times \\frac{3}{11} + 2 \\times \\frac{2}{11} + 3 \\times \\frac{1}{11} + 4 \\times \\frac{5}{11}\\;,\\]\nproducing value \\(\\bar y =2.727\\) representations. Using \nformula, equality two ways computing mean \ngiven terms equation:\\[\\bar y = \\frac{\\sum_{=1}^n y_i}{n} = \\sum_y \\big(y \\times (f_y/n)\\big)\\;,\\]\n\\(f_y/n\\) represents frequency \\(y\\) data.\nfirst representation arithmetic mean, average \ncomputed summation data points dividing sum \nsample size. second representation, uses weighted sum, \nsum extends unique values appear data. \nunique value value multiplied relative frequency \nvalue data. multiplications summed produce \nmean.expectation random variable computed spirit \nsecond formulation. expectation random variable marked \nletter “\\(\\operatorname{E}\\)” define via equation:\\[\\operatorname{E}(Y) = \\sum_y \\big(y \\times \\operatorname{P}(y)\\big)\\;.\\] definition\nunique values sample space considered. value\nproduct value probability value taken. \nexpectation obtained summation products. \ndefinition probability \\(\\operatorname{P}(y)\\) replaces relative frequency\n\\(f_y/n\\) otherwise, definition expectation second\nformulation mean identical .Consider random variable \\(Y\\) distribution described \nTable 7.1. order obtain expectation \nmultiply value sample space probability value.\nSummation products produces expectation (see\nTable 7.1:\\[\\operatorname{E}(Y) = 0 \\times 0.5 + 1 \\times 0.25 + 2 \\times 0.15 + 3\\times 0.10 = 0.85\\;.\\]Table 7.2: Expectation \\(X\\)example height get expectation equal 170.035\ncentimeter. Notice expectation equal \\(\\mu\\), mean \npopulation43. accident. expectation potential\nmeasurement randomly selected subject population equal \naverage measurement across subjects.sample variance (\\(s^2\\)) obtained sum squared\ndeviations average, divided sample size (\\(n\\)) minus 1:\\[s^2 = \\frac{\\sum_{=1}^n (y_i - \\bar y)^2}{n-1}\\;.\\] second\nformulation computation quantity via use \nrelative frequencies. formula sample variance takes form\\[s^2 = \\frac{n}{n-1}\\sum_y \\big((y - \\bar y)^2\\times (f_y/n)\\big)\\;.\\]\nformulation one considers unique value \npresent data. value deviation value \naverage computed. deviations squared \nmultiplied relative frequency. products summed .\nFinally, sum multiplied ratio sample size \\(n\\)\n\\(n-1\\) order correct fact sample variance\nsum squared deviations divided sample size minus 1 \nsample size.similar way, variance random variable may defined via\nprobability values make sample space. \nvalue one computes deviation expectation. deviation \nsquared multiplied probability value. \nmultiplications summed order produce variance:\\[\\operatorname{Var}(Y) = \\sum_y\\big( (y-\\operatorname{E}(Y))^2 \\times \\operatorname{P}(y)\\big)\\;.\\] Notice\nformula computation variance random\nvariable similar second formulation computation\nsample variance. Essentially, mean data replaced \nexpectation random variable relative frequency \nvalue replaced probability value. Another difference \ncorrection factor used variance random\nvariable.Table 7.3: Variance \\(Y\\)example consider variance random variable \\(Y\\). \ncomputation variance random variable carried \nTable 7.3. sample space, values \nrandom variable may obtain, given first column \nprobabilities values given second column. third\ncolumn deviation value expectation\n\\(\\operatorname{E}(Y) = 0.85\\) computed value. 4th column contains\nsquare deviations 5th last column involves \nproduct square deviations probabilities. variance \nobtained summing products last column. given\nexample:\\[\\begin{aligned}\n\\operatorname{Var}(Y) = & (0-0.85)^2 \\times 0.5  + (1-0.85)^2 \\times 0.25 \\\\ &+ (2-0.85)^2\\times 0.15 + (3-0.85)^2\\times 0.10= 1.0275\\;.\\end{aligned}\\]standard deviation random variable square root \nvariance. standard deviation \\(Y\\) \n\\(\\sqrt{\\operatorname{Var}(Y)} = \\sqrt{1.0275} = 1.013657\\).example involves height subject selected \npopulation random obtain variance \\(126.1576\\), equal \npopulation variance, standard deviation 11.23199, \nsquare root variance.characterization distribution computed data,\nmedian, quartiles, etc., may also defined random\nvariables.","code":"\ny <- c(0,1,2,3)\np <- c(.5,.25,.15,.1)\nsum(p*y)\n#> [1] 0.85\ny <- c(0,1,2,3)\np <- c(0.5,0.25,0.15,0.1)\nsum(p * (y - sum(p*y))^2 )\n#> [1] 1.0275"},{"path":"ChapProbability.html","id":"probability-and-statistics","chapter":"7 Probability","heading":"7.5 Probability and Statistics","text":"Modern science may characterized systematic collection \nempirical measurements attempt model laws nature using\nmathematical language. drive deliver better measurements led \ndevelopment accurate sensitive measurement tools.\nNonetheless, point became apparent measurements may \nperfectly reproducible repeated measurement presumably \nexact phenomena typically produce variability outcomes.\nhand, scientists also found general laws\ngovern variability repetitions. example, \ndiscovered average several independent repeats \nmeasurement less variable reproducible \nsingle measurements .Probability first introduced branch mathematics \ninvestigation uncertainty associated gambling games \nchance. early 19th century probability began used \norder model variability measurements. application \nprobability turned successful. Indeed, one major\nachievements probability development mathematical\ntheory explains phenomena reduced variability \nobserved averages used instead single measurements. \nfollowing chapters discuss conclusions theory.Statistics study methods inference based data. Probability serves\nmathematical foundation development statistical\ntheory. chapter introduced probabilistic concept \nrandom variable. concept key understanding statistics. next discuss probability theory used statistical inference. Statistical inference \ndiscussed later course.","code":""},{"path":"ChapProbability.html","id":"exercises-1","chapter":"7 Probability","heading":"7.6 Exercises","text":"Table 7.4: Distribution \\(Y\\)Table 7.4 presents probabilities random variable \\(Y\\).probabilities function number \\(p\\), probability value “0”. Answer following questions:value \\(p\\)?44\\(\\operatorname{P}(Y <3 )\\) = ?45\\(\\operatorname{P}(Y = \\mbox{odd})\\) = ?46\\(\\operatorname{P}(1 \\leq Y < 4)\\) = ?47\\(\\operatorname{P}(|Y -3| < 1.5)\\) = ?48\\(\\operatorname{E}(Y)\\) = ?49\\(\\operatorname{Var}(Y)\\) = ?50What standard deviation \\(Y\\)?51One invests $2 participate game chance. game coin tossed three times. tosses end “Head” player wins $10. Otherwise, player losses investment.probability winning game?52What probability losing game?53What expected gain player plays game?54","code":""},{"path":"ChapProbability.html","id":"glossary","chapter":"7 Probability","heading":"7.7 Glossary","text":"Random Variable:probabilistic model value measurement, \nmeasurement taken.\nprobabilistic model value measurement, \nmeasurement taken.Sample Space:set values random variable may obtain.\nset values random variable may obtain.Probability:number 0 1 assigned subset sample\nspace. number indicates likelihood random variable\nobtaining value subset.\nnumber 0 1 assigned subset sample\nspace. number indicates likelihood random variable\nobtaining value subset.Expectation:central value random variable. expectation \nrandom variable \\(Y\\) marked \\(\\operatorname{E}(Y)\\).\ncentral value random variable. expectation \nrandom variable \\(Y\\) marked \\(\\operatorname{E}(Y)\\).Variance:(squared) spread random variable. variance \nrandom variable \\(Y\\) marked \\(\\operatorname{Var}(Y)\\). standard deviation\nsquare root variance.\n(squared) spread random variable. variance \nrandom variable \\(Y\\) marked \\(\\operatorname{Var}(Y)\\). standard deviation\nsquare root variance.","code":""},{"path":"ChapProbability.html","id":"summary-of-formulas","chapter":"7 Probability","heading":"7.8 Summary of Formulas","text":"Population Size:\\(N\\) = number people, things, etc. population.\n\\(N\\) = number people, things, etc. population.Population Average:\\(\\mu = (1/N)\\sum_{=1}^N y_i\\)\n\\(\\mu = (1/N)\\sum_{=1}^N y_i\\)Expectation Random Variable:\\(\\operatorname{E}(Y) = \\sum_y \\big(y \\times \\operatorname{P}(y)\\big)\\)\n\\(\\operatorname{E}(Y) = \\sum_y \\big(y \\times \\operatorname{P}(y)\\big)\\)Population Variance:\\(\\sigma^2 = (1/N)\\sum_{=1}^N (y_i-\\mu)^2\\)\n\\(\\sigma^2 = (1/N)\\sum_{=1}^N (y_i-\\mu)^2\\)Variance Random Variable:\\(\\operatorname{Var}(Y) = \\sum_y\\big( (y-\\operatorname{E}(Y))^2 \\times \\operatorname{P}(y)\\big)\\)\n\\(\\operatorname{Var}(Y) = \\sum_y\\big( (y-\\operatorname{E}(Y))^2 \\times \\operatorname{P}(y)\\big)\\)","code":""},{"path":"randomVariables.html","id":"randomVariables","chapter":"8 Random Variables","heading":"8 Random Variables","text":"","code":""},{"path":"randomVariables.html","id":"objectives-1","chapter":"8 Random Variables","heading":"8.1 Objectives","text":"central chapter addresses fundamental concept, namely variance random variable. gives laws governing variance sum 2, (especially) \\(n\\) random variables – even importantly – laws governing variance difference two random variables. latter central, just simple contrasts involving 2 sample means proportions, also much wider world regression, since variance (sampling variability) regression slope can viewed variance linear combination random errors, random deviations, random variables. , one master formula pay attention , one variance linear combination random variables. others special cases ., specific objectives truly understandthe concept random variable.concept random variable.concept expectation variance random variable.concept expectation variance random variable., dealing sum two independent random variables, standard deviations sum (add), rather variances., dealing sum two independent random variables, standard deviations sum (add), rather variances.likewise, dealing difference two independent random variables, linear combination \\(n\\) independent random variables involving positive negative weights, component variances add, weights.likewise, dealing difference two independent random variables, linear combination \\(n\\) independent random variables involving positive negative weights, component variances add, weights.","code":""},{"path":"randomVariables.html","id":"random-variables","chapter":"8 Random Variables","heading":"8.2 Random Variables","text":"Recall definition random variable:Definition 8.1  (Random variable) random variable refer numerical values, typically outcome observation, measurement, function thereof.Let’s review two types random variables focus course.Definition 8.2  (Discrete Random Variable) random variable assumes finite (countably infinite) number distinct values. Discrete random variables finite countably infinite number possible values, positive zero probability.Definition 8.3  (Probability mass function) probability mass function (PMF) discrete random variable \\(Y\\) provides possible values \\(y\\) associated probabilities \\(\\operatorname{P}(Y=y)\\). sum probabilities must sum 1, .e. \\(\\sum_{y} \\operatorname{P}(Y=y) = 1\\).Definition 8.4  (Continuous Random Variable) random variable continuous following apply:\n1. set possible values consists either numbers single interval number line (possibly infinite extent, e.g., \\(-\\infty\\) \\(\\infty\\)) numbers disjoint union intervals (e.g., [0,10] \\({\\displaystyle \\cup }\\) [20, 30]).\n2. possible value variable positive probability, , \\(\\operatorname{P}(X = c)\\) = 0 possible value \\(c\\).Definition 8.5  (Probability density function) Let \\(Y\\) continuous random variable. probability distribution probability density function (pdf) \\(Y\\) function \\(f(y)\\) two numbers \\(\\) \\(b\\) \\(\\leq b\\),\\[ \\operatorname{P}(\\leq Y \\leq b) = \\int_a^b f(y) \\partial y\\;.\\]\n, probability \\(Y\\) takes value interval \\([, b]\\) area\ninterval graph density function, illustrated\nFigure 8.1. graph \\(f(y)\\) often referred density curve.\nFigure 8.1: \\(\\operatorname{P}(\\leq Y \\leq b)\\) = area density curve \\(\\) \\(b\\).\nAlthough interval number line contains infinite number numbers, can shown way create infinite listing values - just many . second condition describing continuous random variable perhaps counterintuitive, since seem imply total probability zero possible values. see Chapter specific types continous random variables, intervals values positive probability; probability interval decrease zero width interval shrinks zero.One might argue although principle variables height, weight,\ntemperature continuous, practice limitations measuring\ninstruments restrict us discrete (though sometimes finely subdivided)\nworld. However, continuous models often approximate real-world situations \nwell, continuous mathematics (calculus) frequently easier work \nmathematics discrete variables distributions.","code":""},{"path":"randomVariables.html","id":"expectation-of-a-random-variable","chapter":"8 Random Variables","heading":"8.3 Expectation of a Random Variable","text":"","code":""},{"path":"randomVariables.html","id":"discrete-random-variable","chapter":"8 Random Variables","heading":"8.3.1 Discrete Random Variable","text":"Definition 8.6  (Expected value discrete random variable) Let \\(Y\\) discrete random variable set possible values \\(D=\\left\\lbrace y_1, y_2, \\ldots,y_k \\right\\rbrace\\) corresponding probabilities value, e.g., \\(y_1\\) probability \\(\\operatorname{P}(y_1)\\), \\(y_2\\) probability \\(\\operatorname{P}(y_2)\\), \\(y_3\\) probability \\(\\operatorname{P}(y_3)\\), \\(\\ldots\\), \\(y_k\\) probability \\(\\operatorname{P}(y_k)\\), expected value random variable \\(Y\\) :\\[\\operatorname{E}(Y) =  \\sum_{y \\D} y \\times \\operatorname{P}(y)\\;.\\]\nexpected value exist provided \\(\\sum_{y \\D} |y| \\cdot \\operatorname{P}(y) < \\infty\\).\\(\\operatorname{E}(Y)\\) mean uses expected (.e. unobservable theoretical long run) relative frequencies \\(\\operatorname{P}(\\cdot)\\). Whereas \\(\\bar{y}\\) uses observed relative frequencies. can think \\(\\operatorname{E}(Y)\\) center mass \\(\\operatorname{P}(\\cdot)\\).","code":""},{"path":"randomVariables.html","id":"continuous-random-variable","chapter":"8 Random Variables","heading":"8.3.2 Continuous Random Variable","text":"Definition 8.7  (Expected value continuous random variable) expected value continuous random variable \\(Y\\) probability density function \\(f(y)\\) \n\\[ \\operatorname{E}(Y) = \\int_{-\\infty}^\\infty y \\cdot f(y) \\partial y \\;.\\]\nexpected value exist provided \\(\\int_{-\\infty}^\\infty |y| \\cdot f(y) \\partial y < \\infty\\).","code":""},{"path":"randomVariables.html","id":"why-should-we-care-about-the-expectation","chapter":"8 Random Variables","heading":"8.3.3 Why should we care about the expectation?","text":"expectation acts mean variable conception repetition infinite sample size. expected value random variable usually terms population parameters. present examples expected value random variable quantity interest.Example 8.1  Suppose, example, death rate year 1 every 1000 people,\nanother 2 1000 suffer kind disability. can display \nprobability model insurance policy shown Table 8.1To see insurance company can expect, imagine insures exactly 1000\npeople. imagine , perfect accordance probabilities, 1 \npolicyholders dies, 2 disabled, remaining 997 survive year unscathed.\ncompany pay $10,000 one client $5000 2 clients. ’s\ntotal $20,000, average 20000/1000 = $20 per policy. Since charging people $50 policy, company expects make profit $30 per customer. bad!can’t predict happen given year, can say expect happen. , (, rather, insurance company) need probability model (PMF). expected value policy parameter model. fact, ’s mean. isn’t average data values, won’t estimate . Instead, assume probabilities known simply calculate expected value . come $20 expected value policy payout?\nTable 8.1: Probability mass function insurance policy\nExample 8.2  (Expected number children household) table shown , expected value 2.49 children per householdExample 8.3  (Unbiased Estimator) point estimator \\(\\widehat{\\theta}\\) said unbiased estimator \\(\\theta\\) \n\\[\\operatorname{E}(\\widehat{\\theta}) = \\theta\\;,\\] every possible value \\(\\theta\\).example, let \\(\\hat{p}\\) proportion successes sample \\(n\\) individuals. \\(\\operatorname{E}(\\hat{p}) = \\pi\\) \\(\\pi\\) population proportion successes. Therefore, \\(\\hat{p}\\) unbiased estimator \\(\\pi\\).","code":""},{"path":"randomVariables.html","id":"expected-value-of-a-function-of-a-random-variable","chapter":"8 Random Variables","heading":"8.4 Expected value of a function of a random variable","text":"Definition 8.8  (Expected value function discrete random variable) Let \\(Y\\) discrete random variable set possible values \\(D=\\left\\lbrace y_1, y_2, \\ldots,y_k \\right\\rbrace\\) corresponding probabilities value, e.g., \\(y_1\\) probability \\(\\operatorname{P}(y_1)\\), \\(y_2\\) probability \\(\\operatorname{P}(y_2)\\), \\(y_3\\) probability \\(\\operatorname{P}(y_3)\\), \\(\\ldots\\), \\(y_k\\) probability \\(\\operatorname{P}(y_k)\\). Furthermore, let \\(g(Y)\\) real-valued function \\(Y\\). expected value \\(g(Y)\\) :\\[\\operatorname{E}(g(Y)) =  \\sum_{y \\D} g(y) \\times \\operatorname{P}(y)\\;.\\]\n.e. weighted mean \\(g(y)\\)’s, \\(\\operatorname{P}(y)\\)’s weights.instances, expectation \\(g(Y)\\) \\(g(\\operatorname{E}(Y))\\), others complex. Can figure /following instances?","code":""},{"path":"randomVariables.html","id":"examples-1","chapter":"8 Random Variables","heading":"8.4.1 Examples","text":"\\(Y\\) = Noon Temperature (C) Montreal randomly selected day year;\\(g(Y)\\) = Temperature (F) = 32 + (9/5) \\(Y\\)\n\\(Y\\) = Weight Kg (Height cm) randomly selected person;\ng(Y) = Weight Kg (Height inches)\n\\(Y_1\\) \\(Y_2\\) two random variables might might related;\n.\n\\(g(Y_1, Y_2) = Y_1 + Y_2,\\) \\(E[g(Y_1, Y_2)] = E[Y_1] + E[Y_2].\\)\n.\n\\(g(Y_1, Y_2) = \\frac{Y_1 + Y_2}{2},\\) \\(E[g(Y_1, Y_2)] = \\frac{E[Y_1] + E[Y_2]}{2},\\)\n.\n, analogy, sum mean \\(n\\) related unrelated random variables.\n\\(Y\\) = diameter randomly chosen sphere;\\(g(Y)\\) = Volume sphere = \\(\\frac{\\pi}{6} Y^3.\\)\n\\(Y\\) = fuel consumption, liters/100km, randomly selected make car;\\(g(Y)\\) = miles per gallon Km per liter (reciprocal)\n\\(Y\\) = 3 unequally spaced elevators shows next.\\(\\operatorname{P}\\)(#1) = \\(\\operatorname{P}\\)(#2) = \\(\\operatorname{P}\\)(’s #3) = 1/3.\n.\\(g(Y)\\) = Distance elevator. mimimize E(distance)?\n.\\(g(Y)\\) = Squared Distance elevator. mimimize E(\\(g(Y)\\))?\nRandom Variable Y Expectation Mean \\(\\mu\\)\\(g(Y) = (Y - \\mu)^2\\), squared deviation mean\n\\(Y\\) = Noon Temperature (C) Montreal randomly selected day year;\\(g(Y)\\) = Temperature (F) = 32 + (9/5) \\(Y\\)\\(Y\\) = Weight Kg (Height cm) randomly selected person;\ng(Y) = Weight Kg (Height inches)\\(Y\\) = Weight Kg (Height cm) randomly selected person;\ng(Y) = Weight Kg (Height inches)\\(Y_1\\) \\(Y_2\\) two random variables might might related;\n.\n\\(g(Y_1, Y_2) = Y_1 + Y_2,\\) \\(E[g(Y_1, Y_2)] = E[Y_1] + E[Y_2].\\)\n.\n\\(g(Y_1, Y_2) = \\frac{Y_1 + Y_2}{2},\\) \\(E[g(Y_1, Y_2)] = \\frac{E[Y_1] + E[Y_2]}{2},\\)\n.\n, analogy, sum mean \\(n\\) related unrelated random variables.\\(Y_1\\) \\(Y_2\\) two random variables might might related;\n.\n\\(g(Y_1, Y_2) = Y_1 + Y_2,\\) \\(E[g(Y_1, Y_2)] = E[Y_1] + E[Y_2].\\)\n.\n\\(g(Y_1, Y_2) = \\frac{Y_1 + Y_2}{2},\\) \\(E[g(Y_1, Y_2)] = \\frac{E[Y_1] + E[Y_2]}{2},\\)\n.\n, analogy, sum mean \\(n\\) related unrelated random variables.\\(Y\\) = diameter randomly chosen sphere;\\(g(Y)\\) = Volume sphere = \\(\\frac{\\pi}{6} Y^3.\\)\\(Y\\) = diameter randomly chosen sphere;\\(g(Y)\\) = Volume sphere = \\(\\frac{\\pi}{6} Y^3.\\)\\(Y\\) = fuel consumption, liters/100km, randomly selected make car;\\(g(Y)\\) = miles per gallon Km per liter (reciprocal)\\(Y\\) = fuel consumption, liters/100km, randomly selected make car;\\(g(Y)\\) = miles per gallon Km per liter (reciprocal)\\(Y\\) = 3 unequally spaced elevators shows next.\\(\\operatorname{P}\\)(#1) = \\(\\operatorname{P}\\)(#2) = \\(\\operatorname{P}\\)(’s #3) = 1/3.\n.\\(g(Y)\\) = Distance elevator. mimimize E(distance)?\n.\\(g(Y)\\) = Squared Distance elevator. mimimize E(\\(g(Y)\\))?\\(Y\\) = 3 unequally spaced elevators shows next.\\(\\operatorname{P}\\)(#1) = \\(\\operatorname{P}\\)(#2) = \\(\\operatorname{P}\\)(’s #3) = 1/3.\n.\\(g(Y)\\) = Distance elevator. mimimize E(distance)?\n.\\(g(Y)\\) = Squared Distance elevator. mimimize E(\\(g(Y)\\))?Random Variable Y Expectation Mean \\(\\mu\\)\\(g(Y) = (Y - \\mu)^2\\), squared deviation meanRandom Variable Y Expectation Mean \\(\\mu\\)\\(g(Y) = (Y - \\mu)^2\\), squared deviation mean","code":""},{"path":"randomVariables.html","id":"variance-and-thus-sd-of-a-random-variable","chapter":"8 Random Variables","heading":"8.5 Variance (and thus, SD) of a random variable","text":"","code":""},{"path":"randomVariables.html","id":"definitions","chapter":"8 Random Variables","heading":"8.5.1 Definitions","text":"Definition 8.9  (Variance) variance random variable \\(Y\\) given \\[\\operatorname{E}[(Y - \\mu)^2].\\]\nusually shortened \\(\\operatorname{Var}(Y)\\)., positive square root, called standard deviation \\(Y\\), SD(\\(Y\\)), two commonly used measures variability spread uncertainty.Computationally, \\(\\operatorname{Var}(Y) = \\operatorname{E}[(Y - \\mu)^2]\\) = \\(\\sum(y - \\mu)^2 \\times f(y),\\) Mean Squared Deviation, andComputationally, \\(\\operatorname{Var}(Y) = \\operatorname{E}[(Y - \\mu)^2]\\) = \\(\\sum(y - \\mu)^2 \\times f(y),\\) Mean Squared Deviation, andStandard Deviation, SD(\\(Y) = \\sqrt{(\\operatorname{Var}(Y)}= \\sqrt{\\operatorname{E}[ (Y — \\mu)^2]},\\) Root Mean Squared DeviationStandard Deviation, SD(\\(Y) = \\sqrt{(\\operatorname{Var}(Y)}= \\sqrt{\\operatorname{E}[ (Y — \\mu)^2]},\\) Root Mean Squared DeviationIn French, Standard Deviation called écart type.\nFrench->English dictionary translates (noun) écart space, gap, distance objects, interval, gap dates, difference numbers. adjective type translates typical, standard. adjective better describes meaning ‘standard’ . See history term standard deviation.French, Standard Deviation called écart type.\nFrench->English dictionary translates (noun) écart space, gap, distance objects, interval, gap dates, difference numbers. adjective type translates typical, standard. adjective better describes meaning ‘standard’ . See history term standard deviation.Figure 8.2, graphically numerically illustrated, three (many) ways measure variability random variable.\nFigure 8.2: 6 symmetrically distributed random variables, 3 ways measuring spreads common mean. [Mosteller, Rourke Thomas. Probability statistical applications 2nd Ed, p205]\npractice, mean absolute deviation often quite close SD, certainly easier explain explain non-statisticians. However, computing hand laborious, took two passes data compute , whereas, SD computed one.Example 8.4  (Variance SD number children <= 12 years households least 1 child) calculation shown Table belowWhich primary, Standard Deviation Variance?Although first define variance take square root reach SD, think SD primary, least descriptive purposes. example, Total Fertility Rate measures average number children per woman just 2.5 children per woman. Suppose variation country country standard deviation 1.2 variance 1.44. awkward say variance 1.44 square children per square woman.However, good mathematical reasons work variance.","code":""},{"path":"randomVariables.html","id":"why-use-the-variance","chapter":"8 Random Variables","heading":"8.5.2 Why use the variance?","text":"","code":""},{"path":"randomVariables.html","id":"additivity","chapter":"8 Random Variables","heading":"8.5.2.1 Additivity","text":"variance sum two independent random variables sum variances, even two variables dependent variability sum simple formula. SD’s dont add; squares . quote physicists, errors ‘Errors add quadrature, like lengths sides Pythagoras’ triangle. took mathematicians long time discover, , blunders along way told fascinating chapter readable book Seven Pillars \nStatistical Wisdom.","code":""},{"path":"randomVariables.html","id":"the-central-limit-theorem","chapter":"8 Random Variables","heading":"8.5.2.2 The Central Limit Theorem","text":"limiting behavior random variable sum large number independent random variables depends variances random variables.","code":""},{"path":"randomVariables.html","id":"variance-and-sd-of-a-function-of-a-random-variable","chapter":"8 Random Variables","heading":"8.6 Variance and SD of a function of a random variable","text":"go back examples listed can reason law must :\\(Y\\) = Noon Temperature (C) Mtl randomly selected day year; \\(g(Y)\\) = Temperature (F) = 32 + (9/5) \\(Y\\)\nSD say 10 C, surely SD 18 F. , Temperature Temperature, changing fundamental variable, rather changing scale temperature variable. , SD scales 9/5, , average square, variance scales \\((9/5)^2\\). going way, larger F scale smaller C scale, scalings (5/9) SD, \\((5/9)^2\\) variance. generally,\\[SD[constant  \\times RV] = constant  \\times SD[RV] \\]\\[Var[constant  \\times RV] = constant^2  \\times Var[RV]\\]example also shows another law related spread: shifting left constant amount (eg. suppose conversion \\(F = 10 + (9/5)C\\) instead \\(F = 32 + (9/5)C,\\) alter spread. Thus,\\[SD[ RV + constant ] = SD[RV]\\]\\[Var[ RV + constant ] = Var[RV]\\]\\(Y\\) = Weight Kg (Height cm) randomly selected person; g(Y) = Weight Kg (Height inches). involves just scaling, shift. SD 10 Kg, 22 lbs, variances 2 scales \\(100 \\ Kg^2\\) \\(484 \\ lb^2\\).\n\\(Y\\) = Years publication books McGill Library, Years measured AD (Anno Dominini, ‘year Lord’). SD measured Year 1439 AD (\\(W = Y\\) - 1439) Gutenberg first European use movable type, 1492 AD Christopher Columbus reached North America. , instead, calculated age book year 2020, .e. \\(W = 2020 - Y\\)? scale now reversed. Instead extreme left, older books right hand scale, vice versa. spread still , even though shape new distribution mirror image old one. measured age decades, .e., \\(W = \\frac{2020 - Y}{10}\\) centuries .e., \\(W = \\frac{2020 - Y}{100}\\)? SDs scaled 10 100, variances \\(10^2\\) \\(100^2.\\)\n\\(Y\\) = Ocean Depth randomly chosen location, measured metres. origin ocean floor, depths positive; surface ocean, negative. spread, SD Variance stay , shapes distributions mirror images .\n\\(Y\\) = diameter randomly chosen sphere; \\(g(Y)\\) = Volume sphere = \\(\\frac{\\pi}{6} Y^3.\\) one complicated – might guessed just trying compute expectation. fact scaling different different values \\(Y\\) complicates matters. approximate formula, depends scaling ‘representative’ value \\(Y\\), typically mean mode. , see examples expository piece.\n\\(Y\\) = fuel consumption, liters/100km, randomly selected make car; \\(g(Y)\\) = Km per liter (reciprocal). exact closed form, approximation works well values well away zero, spread .\n\\(Y\\) = Weight Kg (Height cm) randomly selected person; g(Y) = Weight Kg (Height inches). involves just scaling, shift. SD 10 Kg, 22 lbs, variances 2 scales \\(100 \\ Kg^2\\) \\(484 \\ lb^2\\).\\(Y\\) = Years publication books McGill Library, Years measured AD (Anno Dominini, ‘year Lord’). SD measured Year 1439 AD (\\(W = Y\\) - 1439) Gutenberg first European use movable type, 1492 AD Christopher Columbus reached North America. , instead, calculated age book year 2020, .e. \\(W = 2020 - Y\\)? scale now reversed. Instead extreme left, older books right hand scale, vice versa. spread still , even though shape new distribution mirror image old one. measured age decades, .e., \\(W = \\frac{2020 - Y}{10}\\) centuries .e., \\(W = \\frac{2020 - Y}{100}\\)? SDs scaled 10 100, variances \\(10^2\\) \\(100^2.\\)\\(Y\\) = Years publication books McGill Library, Years measured AD (Anno Dominini, ‘year Lord’). SD measured Year 1439 AD (\\(W = Y\\) - 1439) Gutenberg first European use movable type, 1492 AD Christopher Columbus reached North America. , instead, calculated age book year 2020, .e. \\(W = 2020 - Y\\)? scale now reversed. Instead extreme left, older books right hand scale, vice versa. spread still , even though shape new distribution mirror image old one. measured age decades, .e., \\(W = \\frac{2020 - Y}{10}\\) centuries .e., \\(W = \\frac{2020 - Y}{100}\\)? SDs scaled 10 100, variances \\(10^2\\) \\(100^2.\\)\\(Y\\) = Ocean Depth randomly chosen location, measured metres. origin ocean floor, depths positive; surface ocean, negative. spread, SD Variance stay , shapes distributions mirror images .\\(Y\\) = Ocean Depth randomly chosen location, measured metres. origin ocean floor, depths positive; surface ocean, negative. spread, SD Variance stay , shapes distributions mirror images .\\(Y\\) = diameter randomly chosen sphere; \\(g(Y)\\) = Volume sphere = \\(\\frac{\\pi}{6} Y^3.\\) one complicated – might guessed just trying compute expectation. fact scaling different different values \\(Y\\) complicates matters. approximate formula, depends scaling ‘representative’ value \\(Y\\), typically mean mode. , see examples expository piece.\\(Y\\) = diameter randomly chosen sphere; \\(g(Y)\\) = Volume sphere = \\(\\frac{\\pi}{6} Y^3.\\) one complicated – might guessed just trying compute expectation. fact scaling different different values \\(Y\\) complicates matters. approximate formula, depends scaling ‘representative’ value \\(Y\\), typically mean mode. , see examples expository piece.\\(Y\\) = fuel consumption, liters/100km, randomly selected make car; \\(g(Y)\\) = Km per liter (reciprocal). exact closed form, approximation works well values well away zero, spread .\\(Y\\) = fuel consumption, liters/100km, randomly selected make car; \\(g(Y)\\) = Km per liter (reciprocal). exact closed form, approximation works well values well away zero, spread .","code":""},{"path":"randomVariables.html","id":"sums-means-differences-of-random-variables","chapter":"8 Random Variables","heading":"8.7 Sums, means, differences of random variables","text":"","code":""},{"path":"randomVariables.html","id":"a-sum-of-2-or-n-random-variables","chapter":"8 Random Variables","heading":"8.7.1 A sum of 2 or \\(n\\) random variables","text":"keep simple, allow us see going , let’s consider two simple random variables (RV’s), taking just 2 values, equal probabilities. can check later law applies random variables take 2 values, uneven probability distributions. key 2 variables independent .Figures 8.3 8.4 show two RV’s. \\(RV_1\\)  (6   12)   \\(RV_2\\)  (8   16)\\[Sum = RV_1 \\ (6 \\ \\ 12) \\ + \\ RV_2 \\ (8 \\ \\ 16) \\ = \\ 14 \\ \\ 20 \\ \\ 22 \\ \\ 28.\\]\ncome back much later choices specific values random variables; purposes , main point equally-likely values even number apart, SDs integers, calculations involve integers. Note RV equally likely values 1 7 (3 9, -1 + 5) SD RV equally likely values 6 12: distance determines SD. Note also, half values one extreme half , values exactly one SD mean.\nFigure 8.3: Variance RV1\n\nFigure 8.4: Variance RV2\nnow imagine taking random value \\(RV_1\\) random value \\(RV_2\\) summing . 4 possible sums, case distinct (isn’t always case, values two RV’s example deliberately chosen make distinct, avoid grouping RV combinations sum.) probability tree (next panel) helps see 4 possibilities, add extra feature: let lengths branches denote values RVs.example, 4 equally likely sums 14, 20, 22 28, mean \\(\\frac{14+20+22+28}{4} = 21.\\) mean (expected value) sum equals sum 2 individual means expected values, hardly surprising (even true 2 RV’s independent). 4 equally likely deviations 21 -7, -1, +1, +7, , 1st principles, Variance sum 2 RVs \n\\[\\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \\frac{100}{4} = 25.\\]\nThus, SD sum 2 RVs \\(\\sqrt{25}\\) = 5.Fortunately, don’t go back 1st principles calculate SD sum 2 RVs – adding 2 SDs. variances add.\nFigure 8.5: Variance (thus SD) SUM two independent random variables, RV1 RV2, shown . 4 equally likely deviations sum expectation decomposed 2 components. , (expansion rule (+b) squared learned high school), squared deviation becomes sum 2 squares, ‘cross-product’. 4 cross-products cancel , left sums two squares, original 3-squared original 4-squared, .e. variances RV1 RV2.\nWhereas one numerical example doesn’t prove variances sum independent RVs sum variances rule, can check complicated examples values, uneven distributions – use simulation – satisfy general rule. Indeed, formal mathematical statistics proof uses exact method used panel, except replaces number symbol. can sum forgoing numerical example saying:\\[ SD_1 = 3; SD_2 =4;\\  \\ SD[Sum] \\ \\ne \\ 3 + 4. \\ \\ \\  SD[Sum] \\ = \\ \\sqrt{3^2 + 4^2} = 5.\\]Now can see mathematical statisticians like work squared SD’s. can see chose nice variance values 3 4. Just like right-angled triangles Pythagoras’ theorem, length hypotenuse square root squares lengths sides, also orthogonal independent random variables: SD sum square root sum squared SD’s individual RVs.Since theorem works sum 2 independent RV’s, also works sum 3, sum \\(n\\) RVs.","code":""},{"path":"randomVariables.html","id":"measurement-errors","chapter":"8 Random Variables","heading":"8.7.2 Measurement Errors","text":"important issue non-experimental – even experimental – sciences. simplest case (called ‘classical’ error model) errors independent true value, variance observed (error-containing) values sum variance ‘true’ (errorless) values, variance errors.Even though many people think random measurement errors cancel , especially large datasets, . Even affect \\(Y\\) left side side regression model, add ‘noise’ slope estimates. affect \\(X\\) right hand side regression model, even correlation, effects insidious. See example, pages 19-21 Notes exercises 6, 8, 9, 18 21 follow.measurement error model, called ‘Berkson’ error, described :, less common, less nasty effects.following diagram shows classical error model, one important metrics measure extent error, namely intra-class correlation coefficient. ‘ICC’ relevant matter whether variable left right side regression model. Even though named random variable \\(Y\\), mean measurement issues apply \\(Y\\) variables. fact, errors \\(X\\) variables nastier effects. chose name \\(Y\\) don’t treat errorless \\(X\\) regression model random variable, seldom interested inferences regarding !\nFigure 8.6: Random Measurement Error (E) added Random Variable. left distribution Random Variable Y, stickfigure representing large number individuals. right distribution Random Variable Y’, Y’ = Y + E, E independent Y, 2-point distribution, namely -0.5 +0.5, equal probabilities. (, provenance/origin Y’ value shown colour, practice luxury knowing ‘true’ [errorless] value ). variation get observe/measure. variance 1.93, 1.68 ‘real’/‘genuine’, remainder, 0.25 measurement error. genuine variance interest, 1.68, expressed proportion observable variance 1.93, namely 1.68/(1.68 + 0.25) = 0.87 87%. proportion real called INTRA-CLASS CORRELATION (ICC) intra-class correlation coefficient, important indicator quality measurement Y.\nconcept ICC depends law ‘variances add.’","code":""},{"path":"randomVariables.html","id":"mean-of-2-or-n-random-variables","chapter":"8 Random Variables","heading":"8.7.3 Mean of 2 or \\(n\\) random variables","text":", know compute SD SUM \\(n\\) independent RV’s, automatically know compute SD MEAN \\(n\\) independent RV’s. Recall earlier example, SD set temperatures measured larger degrees F scale: \nSD set temperatures measured smaller degrees C scale just 5/9ths one F scale.Going sum \\(n\\) RVs mean \\(n\\) RVs involves going scale (1/n)-th spread sum scale., example, \\(n\\) = 2, SD mean 2 RVs (1/2) SD sum, .e.,\\[ SD\\bigg(\\frac{RV_1 +RV_2}{2}\\bigg) = \\frac{SD(RV_1+RV_2)}{2} = \\frac{5}{2}.\\]SPECIAL CASE (quite common) SDs identical:now, keep things general, used \\(n\\) non-identical – independent – random variables. \nconsider Variance sum \\(n\\) IDENTICAL – independent – random variables, \\(n\\) Variances (abbreviated Var) equal, laws simplifyFirst, since variances add, \\[ \\operatorname{Var}(RV_1 + RV_2 + \\dots + RV_n) = \\operatorname{Var}_1 + \\operatorname{Var}_2 + \\dots + \\operatorname{Var}_n = n \\times \\ \\ \\operatorname{Var}.\\], taking square roots,\\[ SD( \\ RV_1 + RV_2 + \\dots + RV_n \\ ) = \\sqrt{ \\ n \\times \\ \\ \\operatorname{Var}} = \\sqrt{n} \\ \\times \\ \\ SD\\]go sum \\(n\\) IDENTICAL independent RVs mean \\(n\\) IDENTICAL RVs, go scale (1/n)-th spread sum scale. , , just went larger degrees F scale smaller degrees C scale, \\[ SD\\bigg(\\frac{RV_1 + RV_2 + \\dots + RV_n}{n}\\bigg) = \\frac{\\sqrt{n} \\ \\times \\ \\ SD}{n} = \\frac{common \\ SD}{\\sqrt{n}} .\\]\nSometimes, need work variances. , use law:\\[ \\operatorname{Var}\\bigg(\\frac{RV_1 + RV_2 + \\dots + RV_n}{n}\\bigg) = \\frac{common \\ \\operatorname{Var}}{n} .\\]Example 8.5  (Lengths words book) number dashes row first panel Figure 8.7 number letters randomly selected word book: dashes better visibility. words (rows) sorted length, form shortest longest. One judge full distribution just limited set, (even though shape doesn’t matter much big scheme things) get sense shape. entire book, mean word length 4.5 letters, SD \n2.4 letters. (fact distance minimum word length (1 letter) mean word length less 2 SDs hints full distribution long right tail.)row colored panels, 4 (9) randomly sampled words (like Galton’s peas) pushed right , without spaces, shown mono-spaced font, ‘line’ ends indicates total number letters 4 (9) words. Since small number rows (possibilities) small give good sense sampling distribution, smooth purple histograms calculated exactly.\nFigure 8.7: Illustrations SD’s Sums Means n = 1, 4 9 independent identically distributed random variables. RV length randomly selected word certain book. [, compare mean word length book mean book competitor]. distributions purple computed theoretically, using convolutions. row shows 1 ‘realization’ n random variables, word different color. rows sorted according values total [mean] numbers letters (chars) sample n words. panels n 4 9, leftmost n-1 characters n concatenated words cropped, total/mean length correct. top panel lists ‘per word’ variation words book, SD, sometimes called ‘unit’ variability. can also think length unit mean sample size n = 1. second panel shows reduce sampling variation (SD) mean half, one needs quadruple n. third panel shows reduce sampling variation (SD) 1/3, one needs multiply n 9. Note, passing, \\(n\\) = 9, via Central Limit Theorem, fact original distribution ‘CLT friendly’ (mode either extreme, tails don’t extend indefinitely), shape sampling distribution already close Gaussian.\nexpected, purple sampling distributions narrow increasing sample size, narrowing factor \\(n\\), factor \\(\\sqrt{n}\\). billions possible means samples size \\(n\\) = 4\nSD 2.4/\\(\\sqrt{4}\\) = 2.4/2 = 1.2. possible means samples size \\(n\\) = 9 SD 2.4/\\(\\sqrt{9}\\) = 2.4/3 = 0.8.\nNote careful choice words italics: reality, observe one billions possibilities, sampling distribution imaginary thus SD also imaginary SD conceptual sampling distribution () imaginary 1.2 0.8. reason able show purple distributions laws statistics, applied words book, know mean unit SD.","code":""},{"path":"randomVariables.html","id":"difference-of-2-random-variables","chapter":"8 Random Variables","heading":"8.7.4 Difference of 2 Random Variables","text":"keep simple, let’s consider two simple random variables (RV’s), taking just 2 values, equal probabilities, independent . suppose now interested difference\\[Difference = RV_1 \\ (6 \\ \\ 12) \\ - \\ RV_2 \\ (8 \\ \\ 16) \\ = \\ -10 \\ \\ -4 \\ \\ -2 \\ \\ +4.\\]Now, imagine taking random value \\(RV_1\\) subtracting random value \\(RV_2\\). 4 possible differences, deliberately constricted example, distinct. probability tree (next panel) helps see 4 possibilities, lengths branches denote values RVs.4 equally likely differences -10, -4, -2 +4, mean \\(\\frac{-10 -4 -2 +4}{4} = -3.\\) mean (expected value) difference equals difference 2 individual means expected values, hardly surprising (even true 2 RV’s independent). 4 equally likely deviations -3 -7, -1, +1, +7, , 1st principles, Variance difference 2 RVs \n\\[\\frac{(-7)^2 + (-1)2 +  (+1)^2 +  (+7)^2}{4} = \\frac{100}{4} = 25.\\]\nThus, SD difference 2 RVs \\(\\sqrt{25}\\) = 5.Fortunately, don’t go back 1st principles calculate SD differences 2 RVs – adding 2 SDs. VARIANCES ADD!\nFigure 8.8: Variance (thus SD) DIFFERENCE, RV1 - RV2, two independent random variables, RV1 RV2, shown . 4 equally likely deviations difference expectation decomposed 2 components. squared deviation becomes sum 2 squares, ‘cross-product’. 4 cross-products cancel , left SUMS two squares, original 3-squared original 4-squared, .e. variances RV1 RV2.\none numerical example doesn’t prove ‘variances difference two independent RVs sum variances’ rule, can check complicated examples values, uneven distributions – use simulation – satisfy general rule.can sum forgoing numerical example saying:\\[ SD_1 = 3; SD_2 =4;\\  \\ SD[Difference] \\ \\ne \\ 3 + 4. \\ \\ \\  SD[Difference] \\ = \\ \\sqrt{3^2 + 4^2} = 5.\\]\nturns , already knew sum 2 independent RVs, anticipated law. didn’t need go formulae scratch . reason ‘mirror image’ distributions, depths ocean, saw . spread (SD, variance) , whether one writes/reads/computes left right, right left! words, variance random variable \\(-RV_2\\) random variable \\(RV_2.\\) , writing \\(RV_1 - RV_2\\) sum, using law variance sum, arrive \n\\[Var[RV_1 - RV_2] = Var[RV_1 + (-RV_2)] = Var[RV_1] + Var[(-RV_2)] = Var[RV_1] + Var[RV_2].\\]","code":""},{"path":"randomVariables.html","id":"linear-combinations-of-random-variables-regression-slopes","chapter":"8 Random Variables","heading":"8.8 Linear combinations of random variables (regression slopes)","text":"non-experimental research especially, focus typically fitted regression slope/coefficient, rather simple difference\n\\(\\bar{y}_1\\) - \\(\\bar{y}_0\\) means \\(y\\)s observed two investigator-chosen values (\\(X=1\\) \\(X=0\\)) determinant (\\(X\\)) studied.Even estimator closed form, fitted slope(s)/coefficient(s) linear combinations \\(y\\)’s \\(x\\)’s. Thus, since \\(n\\) \\(y\\)’s contains random element, slope (\\(\\hat{\\beta}\\)) \\(x\\)-based linear combination \\(n\\) random variables.Thus one can view variances (thus standard errors) unified way, learn separate laws separate chapters. see unified view avoids typical ‘silo’ approach statistical tecnniques, see Sample Size, Precision Power Calculations: Unified Approach. [Software developers thrive separate ‘niche’ markets threatened parsimonious approach, just write 800-page textbooks separate chapters t-tests,l proportions, regression, multiple regression, logiostic regression, Cox regression, survival analysis,etc.]past, first introduced simple linear regression, common learn estimator formula Variance formula heart, use compute fitted slope standard error fitted slope hand,\n\\[\\hat{\\beta} = \\frac{\\sum (y-\\bar{y})(x-\\bar{x})}{ \\sum (x-\\bar{x})^2} \\ ; \\ \\ Var[ \\hat{\\beta} ] = \\frac{\\sigma_e^2}{ \\sum (x-\\bar{x})^2} \\ ; \\ SE[ \\hat{\\beta} ] \\ = \\sqrt{Var[ \\hat{\\beta} ]} \\ .\\]\nvariance formula, \\(\\sigma_e^2\\) variance ‘errors’ \\(y\\)’s. practice, estimate quantirty, example, disdactic purposes, pretend ‘know’ value.Sadly, formidable formulaa hide going .truly understand going , lets consider simple example can see happening. student country uses Fahrenheit (F) system moves Montreal, wishes know translate outside temperature, expressed number degrees Celsius (C) shown Metro, heard radio stations, back F scale (s)familiar . student knows conversion form F = \\(\\alpha\\) + \\(\\beta \\times C\\) , rather look Google, decides estimate \\(\\alpha\\) \\(\\beta\\) pairs (C,F) readings, taking C readings directly Metro screen, F ones /portable thermometer. Suppose C readings displayed 1 decimal place, F thermometer displays F nearest integer.Now, knowing one just takes 2 data points draw line, student takes F measurements two occasions, displayed temperature 12.5 C one 17.5 C. (student didn’t know real temperature close xx.5 F, 50% chance rounded xx+1 F, thereby creating error +0.5 F, 50% chance rounded xx-1 F, producing error -0.5 F. Thus, variance error \\((-0.5)^2 \\times (1/2)\\) + \\((+0.5)^2 \\times (1/2)\\) = \\(0.5^2,\\) SD 0.5. (computer exercises, treat broader type random errors F readings).Given two C settings correspond 54.5 F 63.5 F, true temperature may slightly one side thse two values, possible \\(\\beta\\) estimates? , variable ?4 possibilities, shown slopes 4 fitted lines shown black , (64-55)/5, (64-54)/5, (63-55)/5, (63-54)/5, , sorted, 1.6, 1.8, 1.8 2.0 degrees F per degree C, probability 1/4, variance equally likely slopes \n\\[Var[ \\hat{\\beta} ] =  \\frac{(1.6 - 1.8)^2 + (1.8 - 1.8)^2  + (1.8 - 1.8)^2 + (2.0 - 1.8)^2}{4} = \\frac{1}{50} \\ =  \\ 0.02,\\]\nSE \\(\\sqrt{0.02} = 0.14\\) degrees F per degree C.Also shown diagram ‘anatomy’ ‘random slope’. possible slopes displayed single expression 2 random elements (.e 2 random variables, two ‘errors’ e\\(_1\\) e\\(_2\\)) isolated. random slope form constant (9/5) plus another constant (1/5) times difference two independent random errors. Applying variance rules , \n\\[Var[random  \\ slope] = (1/5)^2 \\times ( Var[e_1] + Var[e_2])  \\ = \\ \\frac{1}{50} \\ .\\]\nFigure 8.9: 4 lines (black) fitted 4 possible equally likely pairs data points ( ‘true’ values shown blue). algebraicly isolating contributions 2 random errors F variation 4 slopes, variance (random) slopes easily computed using laws variance combination 2 independent random variables.\nimportant point simple regression example even though practice many data points, principle/law used calculate sampling variation slope based number datapoints remains : fitted slope still \\(x\\)-based linear combination \\(y\\)’s, (case, closed form combination) thus \\(x\\)-based linear combination random errors – broadly random deviations \\(x\\)-conditional means ‘\\(Y\\)’ variables. return later factors influence narrowness/width sampling distributions generally, can maybe already see ‘algebraicly isolated’ representation slope datapoints – wider apart \\(x\\) axis smaller magnitudes errors — reproducible slope. influence latter factors less evident textbook formula Variance SE. little exercise (next) help figure factors come . piece also focuses isssues transparent way. See exercise .","code":""},{"path":"randomVariables.html","id":"exercises-2","chapter":"8 Random Variables","heading":"8.9 Exercises","text":"Suppose get life insurance business small way, just taking one client. client pays premium $100 beginning year 5 years. client dies within next 5 years, pay client’s estate $20,000. Thus, end 5 years, possible earnings single client, along associated (actuarily-based) probablities :Compute expected earnings\nCompute variance (thus SD) possible earnings () using definition (b) using computational shortcut\nCompute ‘risk’, SD percentage mean, investors ranking risky various stocks .\nstatistics, especially applied statistics, name SD percentage mean?\nCompute variance (thus SD) possible earnings () using definition (b) using computational shortcutCompute ‘risk’, SD percentage mean, investors ranking risky various stocks .statistics, especially applied statistics, name SD percentage mean?Errors caused rounding. Suppose one analyze large number 3 digit numbers. make job easier, one rounds number nearest 10, e.g.,460 <-- 460 461 462 463 464 ; 465 466 467 468 469 --> 470.\nending numbers unrounded data uniformly distributed (ending digit probability 1/10), calculate:\naverage error per (rounded) number\naverage absolute error per (rounded) number\nsquare root average squared error per (rounded)\nnumber [‘root mean squared error’, ‘RMSE’ short]\nErrors caused rounding. Suppose one analyze large number 3 digit numbers. make job easier, one rounds number nearest 10, e.g.,460 <-- 460 461 462 463 464 ; 465 466 467 468 469 --> 470.\nending numbers unrounded data uniformly distributed (ending digit probability 1/10), calculate:average error per (rounded) numberthe average absolute error per (rounded) numberthe square root average squared error per (rounded)\nnumber [‘root mean squared error’, ‘RMSE’ short]Correcting guessing multiple choice exams.\nSuppose one wishes estimate via multiple choice examination [\\(k\\) answers choose question], proportion \\(\\pi\\) questions student knows answer (excuse dangling preposition!). Imagine \\(\\pi\\) refers N (>> n) questions much larger bank questions \\(n\\) exam questions randomly selecetd.\nShow simple proportion \\(p\\) correctly answered questions gives biased () estimate \\(\\pi\\) student simply randomly guesses among \\(k\\) answers questions (s)doesn’t know answer. calculating expected value p (.e. average mark per question) answer marked 1 correct 0 . (Hint: tree diagram may help).\nOne can ‘de-bias’ estimate giving correct answer mark 1 incorrect answer negative mark. negative mark (penalty) provide unbiased estimate \\(\\pi\\)? Begin finding expected mark per question, set \\(\\pi\\) solve penalty. (Hint: prefer, use concrete values \\(\\pi\\) \\(k\\) see penalty needed.)\nCorrecting guessing multiple choice exams.\nSuppose one wishes estimate via multiple choice examination [\\(k\\) answers choose question], proportion \\(\\pi\\) questions student knows answer (excuse dangling preposition!). Imagine \\(\\pi\\) refers N (>> n) questions much larger bank questions \\(n\\) exam questions randomly selecetd.Show simple proportion \\(p\\) correctly answered questions gives biased () estimate \\(\\pi\\) student simply randomly guesses among \\(k\\) answers questions (s)doesn’t know answer. calculating expected value p (.e. average mark per question) answer marked 1 correct 0 . (Hint: tree diagram may help).One can ‘de-bias’ estimate giving correct answer mark 1 incorrect answer negative mark. negative mark (penalty) provide unbiased estimate \\(\\pi\\)? Begin finding expected mark per question, set \\(\\pi\\) solve penalty. (Hint: prefer, use concrete values \\(\\pi\\) \\(k\\) see penalty needed.)Half purchases eggs market 6 eggs half 12. percentage purchases quantity 1 SD mean? less 1 SD?Half purchases eggs market 6 eggs half 12. percentage purchases quantity 1 SD mean? less 1 SD?Half people population 2 organs certain type half none. standard deviation number organs randomly selecetd person ?Half people population 2 organs certain type half none. standard deviation number organs randomly selecetd person ?Verify variances displayed Figure showing distribution random variable, measurement errors added . subtract smaller variance larger one estimate error variance. Finally show, separate calculation, answer ‘fits’ details error-containing variable constructed.Verify variances displayed Figure showing distribution random variable, measurement errors added . subtract smaller variance larger one estimate error variance. Finally show, separate calculation, answer ‘fits’ details error-containing variable constructed.Consider children parents carry single copy CF gene. (absence ..) many offspring 0, 1 2 copies?Consider children parents carry single copy CF gene. (absence ..) many offspring 0, 1 2 copies?Half large number orders placed Tuesday half Thursday. combined orders jumbled together shipped 3 equal sized shipments Monday Wednesday Friday following week, arriving day shipped. Calculate mean standard deviation number days ordering arrival. Use probability tree depict randomness, show calculations.Half large number orders placed Tuesday half Thursday. combined orders jumbled together shipped 3 equal sized shipments Monday Wednesday Friday following week, arriving day shipped. Calculate mean standard deviation number days ordering arrival. Use probability tree depict randomness, show calculations.Refer example student tries estimate scaling factor degrees C degrees F.Refer example student tries estimate scaling factor degrees C degrees F.student took F readings two C values 10 C (rather 5 C) apart?, .e. 12.5 C 22.5 C?\nVariance SE altered?\nstudent took F readings four equally spaced C values 5 C apart, .e., 7.5 C, 12.5 C, 17.5C 22.5 C?.\n+limited \\(n\\) C values, decide place ?\n, rather \\(0.5^2\\), ‘errors’ F variance \\(1^2\\) \\(2^2\\)?\nVariance SE altered?student took F readings four equally spaced C values 5 C apart, .e., 7.5 C, 12.5 C, 17.5C 22.5 C?.\n+limited \\(n\\) C values, decide place ?, rather \\(0.5^2\\), ‘errors’ F variance \\(1^2\\) \\(2^2\\)?Random Variable: following DISCRETE CONTINUOUS?long wait bus / elevator / surgery/ download completethe blood group n = 1 randomly selected personhow many tries pass coursehow many n = 20 randomly selected persons return questionnaire pilot studylength song CDmean cholesterol level sample n = 30 randomly selected personshow hot going todayhow much snow get next wintertime someone called (answering machine)value test-statistic 2 populations sampled meanhow much ice McDonalds puts soft drinkhow many calories hamburgerhow many numbers get correct 6/49?roulette wheel stopshow many “wrong number” calls receivedhow many keys try get right onehow much water consumed 100 homes","code":"\n\npossible.earnings = c( seq(-19900,-19500,100), 500 ) \nprobability = c(183,186,189,191,193,99058)/100000 \n\ncbind(possible.earnings,probability)\n#>      possible.earnings probability\n#> [1,]            -19900     0.00183\n#> [2,]            -19800     0.00186\n#> [3,]            -19700     0.00189\n#> [4,]            -19600     0.00191\n#> [5,]            -19500     0.00193\n#> [6,]               500     0.99058"},{"path":"randomVariables.html","id":"summary","chapter":"8 Random Variables","heading":"8.10 Summary","text":"concepts random variable, expectation variance, underpin statistical inference. chapter central, even don’t apply laws hand.concepts random variable, expectation variance, underpin statistical inference. chapter central, even don’t apply laws hand.laws governing variance sum mean \\(n\\) random variables basis Standard Errors (SEs) statistics (parameter estimates based aggregated data).laws governing variance sum mean \\(n\\) random variables basis Standard Errors (SEs) statistics (parameter estimates based aggregated data).assessing sampling variability sum mean independent random variables, standard deviations sum (add), rather variances.assessing sampling variability sum mean independent random variables, standard deviations sum (add), rather variances.\\(\\sqrt{n}\\) law Statistics. SE statistic directly proportional \\(\\sqrt{n}\\) dealing sum, inversely proportional \\(\\sqrt{n}\\) (proportional 1/\\(\\sqrt{n}\\) ) dealing mean.\\(\\sqrt{n}\\) law Statistics. SE statistic directly proportional \\(\\sqrt{n}\\) dealing sum, inversely proportional \\(\\sqrt{n}\\) (proportional 1/\\(\\sqrt{n}\\) ) dealing mean.Since proportions means (albeit RVs just take two possibvle values), laws apply well.Since proportions means (albeit RVs just take two possibvle values), laws apply well.law understood/appreciated recent centuries. Statistical historial Stephen Stigler nice example, article, retold book Seven Pillars Statistical Wisdom, failure understand wrong gave people quite bit leeway cheat.law understood/appreciated recent centuries. Statistical historial Stephen Stigler nice example, article, retold book Seven Pillars Statistical Wisdom, failure understand wrong gave people quite bit leeway cheat.’s also statisticians forced work variances developing properties estimators. end user, typically work square roots , speak number children per parent rather square children per square parent.’s also statisticians forced work variances developing properties estimators. end user, typically work square roots , speak number children per parent rather square children per square parent.law governing variance difference two random variables even important, since often interested contrasts level single context.law governing variance difference two random variables even important, since often interested contrasts level single context.Whether add subtract independent random variables, result variable parts.Whether add subtract independent random variables, result variable parts.regression slope can represented linear combination (varying positive negative combining weights) random variables, variance SD sampling distribution slope gioverned fundamental laws.regression slope can represented linear combination (varying positive negative combining weights) random variables, variance SD sampling distribution slope gioverned fundamental laws.Although main focus Variances SDs, along way sub-sections, saw Central Limit Theorem (CLT) trying exert . Although narrowness/width sampling distribution measured variance SD, CLT focuses shape. possible give general rule \\(n\\) CTL ensure sufficiently Gaussian shape sampling distribution. ‘close’ Gaussian particular sampling distribution depends ‘parent’ RV \\(n\\), also consider ‘close enough Government work’.Although main focus Variances SDs, along way sub-sections, saw Central Limit Theorem (CLT) trying exert . Although narrowness/width sampling distribution measured variance SD, CLT focuses shape. possible give general rule \\(n\\) CTL ensure sufficiently Gaussian shape sampling distribution. ‘close’ Gaussian particular sampling distribution depends ‘parent’ RV \\(n\\), also consider ‘close enough Government work’.critical ‘’ several exercises theory (laws) chapter. , seldom manually SE calculations based laws – statistical software . , need understand factors make SE’s big small, concepts involved propagation – reduction – errors. several exercises computing session allow ‘see’ laws action, can adopt guiding principles study design, appreciating ‘precision’ can chase (take shot ) statistical parameters.critical ‘’ several exercises theory (laws) chapter. , seldom manually SE calculations based laws – statistical software . , need understand factors make SE’s big small, concepts involved propagation – reduction – errors. several exercises computing session allow ‘see’ laws action, can adopt guiding principles study design, appreciating ‘precision’ can chase (take shot ) statistical parameters.","code":""},{"path":"ChapNormal.html","id":"ChapNormal","chapter":"9 The Normal Random Variable","heading":"9 The Normal Random Variable","text":"","code":""},{"path":"ChapNormal.html","id":"student-learning-objective-1","chapter":"9 The Normal Random Variable","heading":"9.1 Student Learning Objective","text":"chapter introduces important bell-shaped distribution known\nNormal distribution. Computations associated \ndistribution discussed, including percentiles \ndistribution identification intervals subscribed\nprobability. Normal distribution may serve approximation \ndistributions. demonstrate property showing \nappropriate conditions Binomial distribution can approximated \nNormal distribution. property Normal distribution \npicked next chapter mathematical theory \nestablishes Normal approximation demonstrated. end \nchapter, student able :Understand pnorm qnorm functionsUnderstand pnorm qnorm functionsRecognize Normal density apply R functions computing\nNormal probabilities percentiles.Recognize Normal density apply R functions computing\nNormal probabilities percentiles.Associate distribution Normal random variable \nstandardized counterpart, obtained centering \nre-scaling.Associate distribution Normal random variable \nstandardized counterpart, obtained centering \nre-scaling.Use Normal distribution approximate Binomial\ndistribution.Use Normal distribution approximate Binomial\ndistribution.","code":""},{"path":"ChapNormal.html","id":"the-normal-random-variable","chapter":"9 The Normal Random Variable","heading":"9.2 The Normal Random Variable","text":"Normal distribution important distributions \nused statistics. many cases serves generic model \ndistribution measurement. Moreover, even cases \nmeasurement modeled distributions (.e. Binomial, Poisson,\nUniform, Exponential, etc.) Normal distribution emerges \napproximation distribution numerical characteristics \ndata produced measurements.","code":""},{"path":"ChapNormal.html","id":"the-normal-distribution","chapter":"9 The Normal Random Variable","heading":"9.2.1 The Normal Distribution","text":"Normal random variable continuous distribution sample\nspace numbers, negative positive. denote Normal\ndistribution via \\(Y \\sim \\mathrm{Normal}(\\mu, \\sigma)\\), \n\\(\\mu = \\operatorname{E}(Y)\\) expectation random variable \n\\(\\sigma = \\sqrt{\\operatorname{Var}(Y)}\\) ’s standard deviation55.Consider, example, \\(Y \\sim \\mathrm{Normal}(\\mu=2,\\sigma=3)\\). density \ndistribution presented Figure 9.1. Observe \ndistribution symmetric expectation 2. random variable\nlikely obtain value vicinity expectation.\nValues much larger much smaller expectation \nsubstantially less likely.\nFigure 9.1: Normal(2,3) Distribution\ndensity Normal distribution can computed aid \nfunction dnorm. cumulative probability can computed \nfunction pnorm. illustrating use latter function,\nassume \\(Y \\sim \\mathrm{Normal}(2,3)\\). Say one interested \ncomputation probability \\(\\operatorname{P}(0 < Y \\leq 5)\\) random\nvariable obtains value belongs interval \\((0,5]\\). \nrequired probability indicated marked area \nFigure 9.1. area can computed difference\nprobability \\(\\operatorname{P}(Y \\leq 5)\\), area left 5,\nprobability \\(\\operatorname{P}(Y \\leq 0)\\), area left 0:difference indicated area corresponds probability\ninside interval, turns approximately equal\n0.589. Notice expectation \\(\\mu\\) Normal distribution\nentered second argument function. third argument \nfunction standard deviation, .e. square root \nvariance.can alternatively use mosaic R package compute probability, also outputs graph default:Figure 9.2 displays densities Normal\ndistribution combinations \\(\\mu= 0\\), \\(\\sigma = 1\\) (red\nline); \\(\\mu = 2\\), \\(\\sigma = 3\\) (black line); \\(\\mu = -3\\),\n\\(\\sigma = 1/2\\) (green line). Observe smaller \nvariance concentrated distribution random\nvariable expectation.\nFigure 9.2: Normal Distribution Various Values \\(\\mu\\) \\(\\sigma\\)\n","code":"\npnorm(q = 5, mean = 2, sd = 3) - pnorm(q = 0, mean = 2, sd = 3)\n#> [1] 0.5888522\nmosaic::xpnorm(q = c(0,5), mean = 2, sd = 3)#> [1] 0.2524925 0.8413447"},{"path":"ChapNormal.html","id":"the-standard-normal-distribution","chapter":"9 The Normal Random Variable","heading":"9.2.2 The Standard Normal Distribution","text":"standard normal distribution normal distribution \nstandardized values, called \\(z\\)-scores. \\(z\\)-score \noriginal measurement measured units standard deviation \nexpectation. example, expectation Normal\ndistribution 2 standard deviation \\(3 = \\sqrt{9}\\), \nvalue 0 2/3 standard deviations smaller (left )\nexpectation. Hence, \\(z\\)-score value 0 -2/3.standard Normal distribution distribution standardized\nNormal measurement. expectation standard Normal distribution\n0 variance 1. \\(Y \\sim N(\\mu,\\sigma)\\) Normal\ndistribution expectation \\(\\mu\\) standard deviation \\(\\sigma\\) \ntransformed random variable \\(Z = (Y-\\mu)/\\sigma\\) produces standard\nNormal distribution \\(Z\\sim N(0,1)\\). transformation corresponds \nre-expression original measurement terms new “zero”\nnew unit measurement. new “zero” expectation \noriginal measurement new unit standard deviation \noriginal measurement.Computation probabilities associated Normal random variable\n\\(Y\\) can carried aid standard Normal distribution.\nexample, consider computation probability\n\\(\\operatorname{P}(0 < Y \\leq 5)\\) \\(Y \\sim N(2, 3)\\), expectation \\(\\mu=2\\)\nstandard deviation \\(\\sigma = 3\\). Consider \\(Y\\)’s standardized values:\n\\(Z = (Y-2)/3\\). boundaries interval \\([0,5]\\), namely \\(0\\) \n\\(5\\), standardized \\(z\\)-scores \\((0-2)/3=-2/3\\) \\((5-2)/3 =1\\),\nrespectively. Clearly, original measurement \\(Y\\) falls \noriginal boundaries (\\(0 < Y \\leq 5\\)) , , standardized\nmeasurement \\(Z\\) falls standardized boundaries\n(\\(-2/3 < Z \\leq 1\\)). Therefore, probability \\(Y\\) obtains value\nrange \\([0,5]\\) equal probability \\(Y\\) obtains \nvalue range \\([-2/3,1]\\).\nFigure 9.3: Standard Normal Distribution\nfunction “pnorm” used previous subsection order \ncompute probability \\(X\\) obtains values 0 5. \ncomputation produced probability 0.5888522. can repeat \ncomputation application function standardized\nvalues:value computed, area graph \nstandard Normal distribution, presented Figure 9.3.\nRecall 3 arguments specified previous application \nfunction pnorm: \\(x\\) value, expectation, standard\ndeviation. given application specify last two\narguments, first one. (Notice output expression\n(5-2)/3 single number , likewise, output \nexpression (0-2)/3 also single number.) R function many arguments enables flexible application wide range settings. convenience, however, default values set arguments. default values used unless alternative value argument set function called. default value second argument function\npnorm specifies expectation mean=0, default\nvalue third argument specifies standard deviation \nsd=1. Therefore, value set arguments \nfunction computes cumulative distribution function standard\nNormal distribution.","code":"\npnorm(q = (5-2)/3) - pnorm(q = (0-2)/3)\n#> [1] 0.5888522"},{"path":"ChapNormal.html","id":"computing-percentiles","chapter":"9 The Normal Random Variable","heading":"9.2.3 Computing Percentiles","text":"Consider issue determining range contains 95% \nprobability Normal random variable. start standard\nNormal distribution. Consult Figure 9.4. figure\ndisplays standard Normal distribution central region\nshaded. area shaded region 0.95.may find \\(z\\)-values boundaries region, denoted \nfigure \\(z_0\\) \\(z_1\\) investigation cumulative\ndistribution function. Indeed, order 95% distribution\ncentral region one leave 2.5% distribution \ntwo tails. , 0.025 area unshaded\nregion right \\(z_1\\) , likewise, 0.025 area \nunshaded region left \\(z_0\\). words, cumulative\nprobability \\(z_0\\) 0.025 cumulative distribution\n\\(z_1\\) 0.975.general, given random variable \\(Y\\) given percent \\(p\\), \\(y\\)\nvalue property cumulative distribution \\(y\\) \nequal probability \\(p\\) called \\(p\\)-percentile \ndistribution. seek 2.5%-percentile 97.5%-percentile\nstandard Normal distribution.\nFigure 9.4: Central 95\\(\\%\\) Standard Normal Distribution\npercentiles Normal distribution computed function\nqnorm. first argument function probability (\nsequence probabilities), second third arguments \nexpectation standard deviations normal distribution. \ndefault values arguments set 0 1, respectively.\nHence arguments provided function computes \npercentiles standard Normal distribution. Let us apply \nfunction order compute \\(z_1\\) \\(z_0\\):Observe \\(z_1\\) practically equal 1.96 \n\\(z_0 = -1.96 = -z_1\\). fact \\(z_0\\) negative \\(z_1\\)\nresults symmetry standard Normal distribution 0.\nconclusion get standard Normal distribution 95% \nprobability concentrated range \\([-1.96, 1.96]\\).\nFigure 9.5: Central 95\\(\\%\\) Normal(2,3) Distribution\nproblem determining central range contains 95% \ndistribution can addresses context original measurement\n\\(Y\\) (See Figure 9.5). seek case interval\ncentered expectation 2, center distribution\n\\(Y\\), unlike 0 center standardized values \\(Z\\).\nOne way solving problem via application function\nqnorm appropriate values expectation \nstandard deviation:Hence, get \\(y_0 = -3.88\\) property total\nprobability left 0.025 \\(y_1 = 7.88\\) property \ntotal probability right 0.025. total probability \nrange \\([-3.88, 7.88]\\) 0.95.alternative approach obtaining given interval exploits \ninterval obtained standardized values. interval\n\\([-1.96,1.96]\\) standardized \\(z\\)-values corresponds interval\n\\([2 - 1.96 \\cdot 3, 2 + 1.96\\cdot 3]\\) original \\(x\\)-values:Hence, produce interval \\([-3.88,7.88]\\), interval \nobtained central interval contains 95% \ndistribution \\(\\mathrm{Normal}(2,3)\\) random variable.general, \\(Y \\sim N(\\mu,\\sigma)\\) Normal random variable \ninterval \\([\\mu - 1.96 \\cdot \\sigma, \\mu + 1.96 \\cdot \\sigma]\\)\ncontains 95% distribution random variable. Frequently one\nuses notation \\(\\mu \\pm 1.96 \\cdot \\sigma\\) describe \ninterval.","code":"\nqnorm(0.975)\n#> [1] 1.959964\nqnorm(0.025)\n#> [1] -1.959964\nqnorm(0.975,2,3)\n#> [1] 7.879892\nqnorm(0.025,2,3)\n#> [1] -3.879892\n2 + qnorm(0.975)*3\n#> [1] 7.879892\n2 + qnorm(0.025)*3\n#> [1] -3.879892"},{"path":"ChapNormal.html","id":"the-p-and-q-functions-a-summary","chapter":"9 The Normal Random Variable","heading":"9.2.4 The p and q functions: a summary","text":"‘p’ function tells us, given value characteristic, proportion distribution lies left specified value.‘q’ (quantile) function tells us, given proportion p, value characteristic specified proportion p distribution lies left ‘q’ value.Figure 9.6, values p function shown vertical axis, red, (case, equally-spaced) values characteristic, shown horizontal axis. enter horizontal axis, exit answer vertical axis.q function (blue) goes opposite direction. enter proportion vertical axis, exit value characteristic (quantile) horizontal axis. plot, proportions vertical axis equally-spaced. Percentiles quartiles specific sets quantiles: obtained finding values divide distribution 100 4.\nFigure 9.6: p q functions visual representation.\n","code":""},{"path":"ChapNormal.html","id":"outliers-and-the-normal-distribution","chapter":"9 The Normal Random Variable","heading":"9.2.5 Outliers and the Normal Distribution","text":"Consider, next, computation interquartile range Normal\ndistribution. Recall interquartile range length \ncentral interval contains 50% distribution. interval\nstarts first quartile (Q1), value splits \ndistribution 25% distribution left value\n75% right . interval ends third quartile\n(Q3) 75% distribution left 25% \nright.standard Normal third first quartiles can computed\naid function “qnorm”:Observe standard Normal distribution one 75% \ndistribution left value 0.6744898, \nthird quartile distribution. Likewise, 25% standard\nNormal distribution left value -0.6744898, \nfirst quartile. interquartile range length \ninterval third first quartiles. case \nstandard Normal distribution length equal \n\\(0.6744898 - (-0.6744898) = 1.348980\\).previously considered box plots mean \ngraphical display numerical data. box plot includes \nvertical rectangle initiates first quartile ends \nthird quartile, median marked within box. rectangle\ncontains 50% data. Whiskers extends ends \nrectangle smallest largest data values \noutliers. Outliers values lie outside normal range \ndata. Outliers identified values 1.5 times\ninterquartile range away ends central rectangle.\nHence, value outlier larger third quartile\nplus 1.5 times interquartile range less first\nquartile minus 1.5 times interquartile range.likely obtain outlier value measurement \nstandard Normal distribution? obtained third quartile \nstandard Normal distribution equal 0.6744898 first\nquartile minus value. interquartile range difference\nthird first quartiles. upper lower thresholds\ndefining outliers :Hence, value larger 2.697959 smaller -2.697959 \nidentified outlier.probability less upper threshold 2.697959 \nstandard Normal distribution computed expression\n“pnorm(2.697959)”. probability threshold 1\nminus probability, outcome expression\n“1-pnorm(2.697959)”.symmetry standard Normal distribution get \nprobability lower threshold -2.697959 equal \nprobability upper threshold. Consequently, \nprobability obtaining outlier equal twice probability \nupper threshold:get standard Normal distribution probability \noutlier approximately 0.7%.","code":"\nqnorm(0.75)\n#> [1] 0.6744898\nqnorm(0.25)\n#> [1] -0.6744898\nqnorm(0.75) + 1.5*(qnorm(0.75)-qnorm(0.25))\n#> [1] 2.697959\nqnorm(0.25) - 1.5*(qnorm(0.75)-qnorm(0.25))\n#> [1] -2.697959\n2*(1-pnorm(2.697959))\n#> [1] 0.006976603"},{"path":"ChapNormal.html","id":"approximation-of-the-binomial-distribution","chapter":"9 The Normal Random Variable","heading":"9.3 Approximation of the Binomial Distribution","text":"Normal distribution emerges frequently approximation \ndistribution data characteristics. probability theory \nmathematically establishes approximation called Central\nLimit Theorem subject subsequent chapters. section \ndemonstrate Normal approximation context Binomial\ndistribution.","code":""},{"path":"ChapNormal.html","id":"approximate-binomial-probabilities-and-percentiles","chapter":"9 The Normal Random Variable","heading":"9.3.1 Approximate Binomial Probabilities and Percentiles","text":"Consider, example, probability obtaining 1940 \n2060 heads tossing 4,000 fair coins. Let \\(Y\\) total number \nheads. tossing coin trial two possible outcomes:\n“Head” “Tail.” probability “Head” 0.5 \n4,000 trials. Let us call obtaining “Head” trial “Success”.\nObserve random variable \\(Y\\) counts total number \nsuccesses. Hence, \\(Y \\sim \\mathrm{Binomial}(4000,0.5)\\).probability \\(\\operatorname{P}(1940 \\leq Y \\leq 2060)\\) can computed \ndifference probability \\(\\operatorname{P}(Y \\leq 2060)\\) less \nequal 2060 probability \\(\\operatorname{P}(Y < 1940)\\) strictly\nless 1940. However, 1939 largest integer still\nstrictly less integer 1940. result get \n\\(\\operatorname{P}(Y < 1940) = \\operatorname{P}(Y \\leq 1939)\\). Consequently,\n\\(\\operatorname{P}(1940 \\leq Y \\leq 2060) = \\operatorname{P}(Y \\leq 2060) - \\operatorname{P}(Y \\leq 1939)\\).Applying function “pbinom” computation Binomial\ncumulative probability, namely probability less equal \ngiven value, get probability range 1940 \n2060 equal toThis exact computation. Normal approximation produces \napproximate evaluation, exact computation. Normal\napproximation replaces Binomial computations computations carried \nNormal distribution. computation probability \nBinomial random variable replaced computation probability \nNormal random variable expectation standard\ndeviation Binomial random variable.Notice \\(Y \\sim \\mathrm{Binomial}(4000,0.5)\\) expectation\n\\(\\operatorname{E}(Y) = 4,000 \\times 0.5 = 2,000\\) variance \n\\(\\operatorname{Var}(Y) = 4,000 \\times 0.5 \\times 0.5 = 1,000\\), standard\ndeviation square root variance. Repeating \ncomputation conducted Binomial random variable, \ntime function pnorm used computation \nNormal cumulative probability, get:Observe example Normal approximation probability\n(0.9442441) agrees Binomial computation probability\n(0.9442883) 3 significant digits.Normal computations may also applied order find approximate\npercentiles Binomial distribution. example, let us identify\ncentral region contains \\(\\mathrm{Binomial}(4000,0.5)\\)\nrandom variable (approximately) 95% distribution. Towards \nend can identify boundaries region Normal\ndistribution expectation standard deviation \ntarget Binomial distribution:rounding nearest integer get interval \\([1938,2062]\\)\nproposed central region.order validate proposed region may repeat computation\nactual Binomial distribution:, get interval \\([1938,2062]\\) central region, \nagreement one proposed Normal approximation. Notice \nfunction “qbinom” produces percentiles Binomial\ndistribution. may come surprise learn “qpois”,\n“qunif”, “qexp” compute percentiles Poisson, Uniform \nExponential distributions, respectively.ability approximate one distribution , \ncomputation tools distributions handy, seems \nquestionable importance. Indeed, significance Normal\napproximation much ability approximate Binomial\ndistribution . Rather, important point Normal\ndistribution may serve approximation wide class \ndistributions, Binomial distribution one example.\nComputations based Normal approximation valid\nmembers class distributions, including cases \ndon’t computational tools disposal even cases\nknow exact distribution member ! \npromised, detailed discussion Normal approximation \nwider context presented Central Limit Theorem chapter.hand, one need assume distribution well\napproximated Normal distribution. example, distribution\nwealth population tends skewed, 50% \npeople possessing less 50% wealth small percentage\npeople possessing majority wealth. Normal\ndistribution good model distribution. Exponential\ndistribution, distributions similar , may appropriate.","code":"\npbinom(q = 2060, size = 4000, prob = 0.5) - pbinom(q = 1939, size = 4000, prob = 0.5)\n#> [1] 0.9442883\nmu <- 4000*0.5\nsig <- sqrt(4000*0.5*0.5)\npnorm(2060,mu,sig) - pnorm(1939,mu,sig)\n#> [1] 0.9442441\nqnorm(0.975,mu,sig)\n#> [1] 2061.98\nqnorm(0.025,mu,sig)\n#> [1] 1938.02\nqbinom(0.975,4000,0.5)\n#> [1] 2062\nqbinom(0.025,4000,0.5)\n#> [1] 1938"},{"path":"ChapNormal.html","id":"continuity-corrections","chapter":"9 The Normal Random Variable","heading":"9.3.2 Continuity Corrections","text":"order complete section let us look carefully \nNormal approximations Binomial distribution.\nFigure 9.7: Normal Approximation Binomial Distribution\nprinciple, Normal approximation valid \\(n\\), number \nindependent trials Binomial distribution, large. \\(n\\) \nrelatively small approximation may good. Indeed, take\n\\(Y \\sim \\mathrm{Binomial}(30,0.3)\\) consider probability\n\\(\\operatorname{P}(Y \\leq 6)\\). Compare actual probability Normal\napproximation:Normal approximation, equal 0.1159989, close\nactual probability, equal 0.1595230.naïve application Normal approximation \n\\(\\mathrm{Binomial}(n,p)\\) distribution may good number\ntrials \\(n\\) small. Yet, small modification approximation\nmay produce much better results. order explain modification\nconsult Figure 9.7 find bar plot \nBinomial distribution density approximating Normal\ndistribution superimposed top . target probability \nsum heights bars painted red. naïve\napplication Normal approximation used area \nnormal density left bar associated value\n\\(y=6\\).Alternatively, may associate bar located \\(y\\) area\nnormal density interval \\([y-0.5, y+0.5]\\). \nresulting correction approximation use Normal\nprobability event \\(\\{Y \\leq 6.5\\}\\), area shaded \nred. application approximation, called\ncontinuity correction produces:Observe corrected approximation much closer target\nprobability, 0.1595230, substantially better \nuncorrected approximation 0.1159989. Generally, \nrecommended apply continuity correction Normal\napproximation discrete distribution.Consider \\(\\mathrm{Binomial}(n,p)\\) distribution. Another situation\nNormal approximation may fail \\(p\\), probability \n“Success” Binomial distribution, close 0 (close\n1). Recall, large \\(n\\) Poisson distribution emerged \napproximation Binomial distribution setting. One may\nexpect \\(n\\) large \\(p\\) small Poisson\ndistribution may produce better approximation Binomial\nprobability. Poisson distribution used approximation\ncall Poisson Approximation.Let us consider example. Let us analyze 3 Binomial distributions. \nexpectation distributions equal 2 number \ntrials, \\(n\\), vary. first case \\(n=20\\) (hence \\(p=0.1\\)), \nsecond \\(n=200\\) (\\(p=0.01\\)), third \\(n=2,000\\) (\n\\(p=0.001\\). three cases interested probability\nobtaining value less equal 3.Poisson approximation replaces computations conducted \nBinomial distribution Poisson computations, Poisson\ndistribution expectation Binomial. Since \nthree cases expectation equal 2 get Poisson\napproximation used three probabilities:actual Binomial probability first case (\\(n=20\\), \\(p=0.1\\)) \nNormal approximation thereof :Observe Normal approximation (continuity correction) \nbetter Poisson approximation case.second case (\\(n=200\\), \\(p=0.01\\)) actual Binomial probability\nNormal approximation probability :Observe Poisson approximation produces 0.8571235 \nslightly closer target Normal approximation. greater\naccuracy Poisson approximation case \\(n\\) large\n\\(p\\) small pronounced final case (\\(n=2000\\),\n\\(p=0.001\\)) target probability Normal approximation\n:Compare actual Binomial probability, equal 0.8572138, \nPoisson approximation produced 0.8571235. Normal\napproximation, 0.8556984, slightly , still acceptable.","code":"\npbinom(6,30,0.3)\n#> [1] 0.159523\npnorm(6,30*0.3,sqrt(30*0.3*0.7))\n#> [1] 0.1159989\npnorm(6.5,30*0.3,sqrt(30*0.3*0.7))\n#> [1] 0.1596193\nppois(3,2)\n#> [1] 0.8571235\npbinom(3,20,0.1)\n#> [1] 0.8670467\npnorm(3.5,2,sqrt(20*0.1*0.9))\n#> [1] 0.8682238\npbinom(3,200,0.01)\n#> [1] 0.858034\npnorm(3.5,2,sqrt(200*0.01*0.99))\n#> [1] 0.856789\npbinom(3,2000,0.001)\n#> [1] 0.8572138\npnorm(3.5,2,sqrt(2000*0.001*0.999))\n#> [1] 0.8556984"},{"path":"ChapNormal.html","id":"Normal4","chapter":"9 The Normal Random Variable","heading":"9.4 Exercises","text":"Consider problem establishing regulations\nconcerning maximum number people can occupy lift. \nparticular, like assess probability exceeding maximal\nweight 8 people allowed use lift simultaneously \ncompare probability allowing 9 people lift.Assume total weight 8 people chosen random follows \nnormal distribution mean 560kg standard deviation \n57kg. Assume total weight 9 people chosen random follows\nnormal distribution mean 630kg standard deviation \n61kg.probability total weight 8 people exceeds\n650kg?56What probability total weight 9 people exceeds\n650kg?57What central region contains 80% distribution \ntotal weight 8 people?58What central region contains 80% distribution \ntotal weight 9 people?59Assume \\(Y \\sim \\mbox{Binomial}(27,0.32)\\). interested probability \\(\\operatorname{P}(Y > 11)\\).Compute (exact) value probability.60Compute Normal approximation probability, without \ncontinuity correction.61Compute Normal approximation probability, \ncontinuity correction.62Compute Poisson approximation probability.63","code":""},{"path":"ChapNormal.html","id":"summary-1","chapter":"9 The Normal Random Variable","heading":"9.5 Summary","text":"","code":""},{"path":"ChapNormal.html","id":"glossary","chapter":"9 The Normal Random Variable","heading":"Glossary","text":"Normal Random Variable:bell-shaped distribution frequently used model \nmeasurement. distribution marked \n\\(\\mathrm{Normal}(\\mu,\\sigma)\\). Note R uses \\(\\sigma\\) also, \\(\\sigma^2\\) p q functions .\nbell-shaped distribution frequently used model \nmeasurement. distribution marked \n\\(\\mathrm{Normal}(\\mu,\\sigma)\\). Note R uses \\(\\sigma\\) also, \\(\\sigma^2\\) p q functions .Standard Normal Distribution:\\(\\mathrm{Normal}(0,1)\\). distribution standardized Normal\nmeasurement.\n\\(\\mathrm{Normal}(0,1)\\). distribution standardized Normal\nmeasurement.Percentile:Given percent \\(p \\cdot 100\\%\\) (probability \\(p\\)), value\n\\(y\\) percentile random variable \\(Y\\) satisfies \nequation \\(\\operatorname{P}(Y \\leq y) = p\\).\nGiven percent \\(p \\cdot 100\\%\\) (probability \\(p\\)), value\n\\(y\\) percentile random variable \\(Y\\) satisfies \nequation \\(\\operatorname{P}(Y \\leq y) = p\\).Normal Approximation Binomial:Approximate computations associated Binomial distribution\nparallel computations use Normal distribution \nexpectation standard deviation Binomial.\nApproximate computations associated Binomial distribution\nparallel computations use Normal distribution \nexpectation standard deviation Binomial.Poisson Approximation Binomial:Approximate computations associated Binomial distribution\nparallel computations use Poisson distribution \nexpectation Binomial.\nApproximate computations associated Binomial distribution\nparallel computations use Poisson distribution \nexpectation Binomial.","code":""},{"path":"ChapSampDist.html","id":"ChapSampDist","chapter":"10 The Sampling Distribution","heading":"10 The Sampling Distribution","text":"","code":""},{"path":"ChapSampDist.html","id":"student-learning-objective-2","chapter":"10 The Sampling Distribution","heading":"10.1 Student Learning Objective","text":"section integrate concept data extracted\nsample concept random variable. new element\nconnects two concepts notion sampling\ndistribution. data observe results specific sample \nselected. sampling distribution, similar way random\nvariables, corresponds samples selected.\n(, stated different tense, sample selected\nprior selection .) Summaries distribution \ndata, sample mean sample standard deviation, become\nrandom variables considered context sampling\ndistribution. section investigate sampling distribution\ndata summaries. particular, demonstrated (\nlarge samples) sampling distribution sample average may \napproximated Normal distribution. mathematical theorem \nproves approximation called Central Limit Theory. \nend chapter, student able :Comprehend notion sampling distribution simulate \nsampling distribution sample average.Comprehend notion sampling distribution simulate \nsampling distribution sample average.Relate expectation standard deviation measurement \nexpectation standard deviation sample average.Relate expectation standard deviation measurement \nexpectation standard deviation sample average.Apply Central Limit Theorem sample averages.Apply Central Limit Theorem sample averages.","code":""},{"path":"ChapSampDist.html","id":"the-sampling-distribution","chapter":"10 The Sampling Distribution","heading":"10.2 The Sampling Distribution","text":"Chapter 7 concept random variable \nintroduced. part introduction used example involved\nselection random person population measuring\n/height. Prior action selection, height \nperson random variable. potential obtaining \nheights present population, sample\nspace example, distribution reflects relative\nfrequencies heights population: \nprobabilities values. selection person \nmeasuring height get particular value. observed\nvalue longer random variable. section extend \nconcept random variable define concept random\nsample.","code":""},{"path":"ChapSampDist.html","id":"a-random-sample","chapter":"10 The Sampling Distribution","heading":"10.2.1 A Random Sample","text":"relation random sample data similar \nrelation random variable observed value. data \nobserved values sample taken population. content \ndata known. random sample, similarly random variable, \ndata selected taking sample, prior \nselection . content random sample unknown, since \nsample yet taken. Still, just like case \nrandom variable, one able say possible evaluations \nsample may , depending mechanism selecting sample,\nprobabilities different potential evaluations. \ncollection possible evaluations sample sample\nspace random sample probabilities different\nevaluations produce distribution random sample.(Alternatively, one prefers speak past tense, one can define\nsample space random sample evaluations sample\ntaken place, distribution random sample\nprobabilities evaluations.)statistic function data. Example statistics \naverage data, sample variance standard deviation, \nmedian data, etc. case given formula applied \ndata. type statistic different formula applied.formula applied observed data may, principle,\napplied random samples. Hence, example, one may talk \nsample average, average elements data. \naverage, considered context observed data, number \nvalue known. However, think average context\nrandom sample becomes random variable. Prior \nselection actual sample know values \ninclude. Hence, tell outcome average \nvalues . However, due identification possible\nevaluations sample can possess may say advance \ncollection values sample average can . sample\nspace sample average. Moreover, sampling distribution \nrandom sample one may identify probability value \nsample average, thus obtaining sampling distribution sample\naverage.line argumentation applies statistic. Computed \ncontext observed data, statistic known number may,\nexample, used characterize variation data. \nthinking statistic context random sample becomes \nrandom variable. distribution statistic called \nsampling distribution statistic. Consequently, may talk \nsampling distribution median, sample distribution \nsample variance, etc.Random variables also applied models uncertainty future\nmeasurements abstract settings need involve specific\npopulation. Specifically, introduced Binomial Poisson random\nvariables settings involve counting Uniform,\nExponential, Normal random variables settings \nmeasurement continuous.notion sampling distribution may extended situation\none taking several measurements, measurement taken\nindependently others. result one obtains sequence \nmeasurements. use term “sample” denote sequence. \ndistribution sequence also called sampling distribution.\nmeasurements sequence Binomial call \nBinomial sample. measurements Exponential call \nExponential sample forth., one may apply formula (average) content \nrandom sequence produce random variable. term sampling\ndistribution describes distribution random variable\nproduced formula inherits sample.next subsection examine example sample taken \npopulation. Subsequently, discuss examples involves sequence\nmeasurements theoretical model.","code":""},{"path":"ChapSampDist.html","id":"sampling-from-a-population-1","chapter":"10 The Sampling Distribution","heading":"10.2.2 Sampling From a Population","text":"Consider taking sample population. Let us use \nillustration heights_population.csv file contains population data can\nobtained like \nChapter 7. data frame produced file\ncontains sex hight 100,000 members imaginary\npopulation. Recall Chapter 7 applied \nfunction sample randomly sample height single subject\npopulation. Let us apply function , time\norder sample heights 100 subjects:Typically, researcher get examine entire population.\nInstead, measurements sample population made. \nrelation imaginary setting simulate example, \ntypical situation research complete list\npotential measurement evaluations, .e. complete list 100,000\nheights, sample measurements, namely\nlist 100 rows stored Y.samp presented\n. role statistics make inference parameters \nunobserved population based information obtained \nsample.example, may interested estimating mean value \nheights population. reasonable proposal use sample\naverage serve estimate:artificial example can actually compute true population\nmean:Hence, may see although match estimated value\nactual value perfect still close enough.actual estimate obtained resulted specific\nsample collected. collected different subset 100\nindividuals obtained different numerical value \nestimate. Consequently, one may wonder: pure luck got\ngood estimates? likely get estimates close \ntarget parameter?Notice realistic settings know actual value \ntarget population parameters. Nonetheless, still want \nleast probabilistic assessment distance \nestimates parameters try estimate. sampling\ndistribution vehicle may enable us address \nquestions.order illustrate concept sampling distribution let us\nselect another sample compute average::case got different value sample average. \nfirst last two iterations result 1 centimeter\naway population average, equal 170.035, \nsecond within range 1 centimeter. Can say, prior \ntaking sample, probability falling within 1\ncentimeter population mean?Chapter 7 discussed random variable emerges \nrandomly sampling single number population presented \nsequence heights_population$height. distribution random variable\nresulted assignment probability 1/100,000 one \n100,000 possible outcomes. principle applies \nrandomly sample 100 individuals. possible outcome collection\n100 numbers collection assigned equal probability. \nresulting distribution called sampling distribution.distribution average sample emerges \ndistribution: sample one may associate average \nsample. probability assigned average outcome \nprobability sample. Hence, one may assess probability \nfalling within 1 centimeter population mean using sampling\ndistribution. sample produces average either falls within\ngiven range . probability sample average falling\nwithin given range proportion samples event\nhappens among entire collection samples.However, face technical difficulty attempt assess \nsampling distribution average probability falling\nwithin 1 centimeter population mean. Examination \ndistribution sample single individual easy enough. \ntotal number outcomes, 100,000 given example, can \nhandled effort computer. However, consider\nsamples size 100 get total number ways select 100\nnumber 100,000 numbers order \\(10^{342}\\) (1 followed\n342 zeros) handled computer. Thus, \nprobability computed.compromise approximate distribution selecting \nlarge number samples, say 100,000, represent entire\ncollection, use resulting distribution approximation \nsampling distribution. Indeed, larger number samples \ncreate accurate approximation distribution .\nStill, taking 100,000 repeats produce approximations \ngood enough purposes.Consider sampling distribution sample average. simulated\nexamples average. Now like simulate\n100,000 examples. creating first sequence \nlength number evaluations seek (100,000) write \nsmall program produces time new random sample size 100\nassigns value average sample appropriate\nposition sequence. first explain later64:first line produce sequence length 100,000 contains\nzeros. function “rep” creates sequence contains repeats \nfirst argument number times specified second\nargument. example, numerical value 0 repeated 100,000\ntimes produce sequence zeros length seek.main part program “” loop. argument \nfunction “” takes special form: “index.name \nindex.values”, index.name name running index \nindex.values collection values running index\nevaluated. iteration loop running index \nassigned value collection expression follows \nbrackets “” function evaluated given value \nrunning index.given example collection values produced \nexpression “1:n”. Recall expression “1:n” produces \ncollection integers 1 n. , n = 100,000. Hence,\ngiven application collection values sequence \ncontains integers 1 100,000. running index called\n“”. expression evaluated 100,000 times, time \ndifferent integer value running index “”.\nFigure 10.1: Distribution Height Sampling Distribution Averages\nR system treats collection expressions enclosed within curly\nbrackets one entity. Therefore, iteration “”\nloop, lines within curly brackets evaluated. \nfirst line random sample size 100 produced second\nline average sample computed stored \\(\\)-th\nposition sequence “Y.bar”. Observe specific position\nsequence referred using square brackets.program changes original components sequence, 0 \naverage random sample, one one. loop ends \nvalues changed sequence “Y.bar” contains 100,000\nevaluations sample average. last line, outside \ncurly brackets evaluated “” loop ends, produces \nhistogram averages simulated. histogram \npresented lower panel Figure 10.1.Compare distribution sample average distribution \nheights population presented first \nFigure 7.2 currently presented upper\npanel Figure 10.1. Observe distributions \ncentered 170 centimeters. Notice, however, range \nvalues sample average lies essentially 166 174\ncentimeters, whereas range distribution heights \n127 217 centimeter. Broadly speaking, sample average\noriginal measurement centered around location \nsample average less spread.Specifically, let us compare expectation standard deviation \nsample average expectation standard deviation \noriginal measurement:Observe expectation population expectation \nsample average, practically , standard deviation \nsample average 10 times smaller standard deviation\npopulation. result accidental actually reflects \ngeneral phenomena seen examples.may use simulated sampling distribution order compute \napproximation probability sample average falling within 1\ncentimeter population mean. Let us first compute relevant\nprobability explain details computation:Hence get probability given event 62.6%.object “Y.bar” sequence length 100,000 contains \nsimulated sample averages. sequence represents distribution \nsample average. expression\n“abs(Y.bar - mean(heights_population$height)) <= 1” produces sequence logical\n“TRUE” “FALSE” values, depending value sample\naverage less one unit away population mean.\napplication function “mean” output last\nexpression results computation relative frequency \nTRUEs, corresponds probability event interest.","code":"\nheights_population <- rio::import(here::here(\"inst/data/heights_population.csv\"))\nheights_population <- heights_population %>% \n  dplyr::mutate(sex = factor(sex))\nY.samp <- heights_population %>%\n  dplyr::slice_sample(n = 100, replace = FALSE)\nY.samp %>% \n  dplyr::glimpse()\n#> Rows: 100\n#> Columns: 3\n#> $ id     <int> 3776094, 8205435, 2010906, 6929553, 9381841…\n#> $ sex    <fct> MALE, MALE, FEMALE, FEMALE, MALE, FEMALE, F…\n#> $ height <int> 177, 173, 163, 167, 174, 163, 160, 150, 167…\nmean(Y.samp$height)\n#> [1] 168.32\nmean(heights_population$height)\n#> [1] 170.035\nY.samp <- heights_population %>%\n  dplyr::slice_sample(n = 100, replace = FALSE)\nY.samp <- heights_population %>%\n  dplyr::slice_sample(n = 100, replace = FALSE)\nmean(Y.samp$height)\n#> [1] 171.04\nY.bar <- rep(0,10^5)\nfor(i in 1:10^5) {\n  Y.samp <- sample(heights_population$height,100)\n  Y.bar[i] <- mean(Y.samp)\n}\nmean(heights_population$height)\n#> [1] 170.035\nsd(heights_population$height)\n#> [1] 11.23205\nmean(Y.bar)\n#> [1] 170.0276\nsd(Y.bar)\n#> [1] 1.125938\nmean(abs(Y.bar - mean(heights_population$height)) <= 1)\n#> [1] 0.62556"},{"path":"ChapSampDist.html","id":"subsec:theoreticalmdls","chapter":"10 The Sampling Distribution","heading":"10.2.3 Theoretical Models","text":"Sampling distribution can also considered context \ntheoretical distribution models. example, take measurement\n\\(Y \\sim \\mathrm{Binomial}(10,0.5)\\) Binomial distribution.\nAssume 64 independent measurements produced distribution:\n\\(Y_1, Y_2, \\ldots, Y_{64}\\). sample average case corresponds\ndistribution random variable produced averaging \n64 random variables:\\[\\bar Y = \\frac{Y_1 + Y_2 + \\cdots + Y_{64}} {64} = \\frac{1}{64}\\sum_{=1}^{64} Y_i\\;.\\]\n, one may wonder distribution sample average\n\\(\\bar Y\\) case?can approximate distribution sample average simulation.\nfunction “rbinom” produces random sample Binomial\ndistribution. first argument function sample size,\ntake example equal 64. second third\narguments parameters Binomial distribution, 10 0.5 \ncase. can use function simulation:Observe code created sequence length 100,000 \nevaluations sample average 64 Binomial random variables. \nstart sequence zeros iteration “” loop\nzero replaced average random sample 64 Binomial\nrandom variables.\nFigure 10.2: Distributions Average Single Binomial(10,0.5)\nExamine sampling distribution Binomial average:histogram sample average presented lower panel \nFigure 10.2. Compare distribution single\nBinomial random variable appears upper panel. Notice, \n, center two distributions coincide spread\nsample average smaller. sample space single Binomial\nrandom variable composed integers. sample space average\n64 Binomial random variables, hand, contains many \nvalues closer sample space random variable \ncontinuous distribution.Recall expectation \\(\\mathrm{Binomial}(10,0.5)\\) random\nvariable \\(\\operatorname{E}(Y) = 10 \\cdot 0.5 = 5\\) variance \n\\(\\operatorname{Var}(Y) = 10 \\cdot 0.5 \\cdot 0.5 = 2.5\\) (thus, standard deviation\n\\(\\sqrt{2.5} = 1.581139\\)). Observe expectation sample\naverage got simulation essentially equal 5 \nstandard deviation 0.1982219.Section 8, proved mathematically expectation sample mean \nequal theoretical expectation components:\\[\\operatorname{E}(\\bar Y) = \\operatorname{E}(Y)\\;.\\] results simulation \nexpectation sample average consistent mathematical\nstatement. Section 8, also proved variance sample average equal variance components, divided sample size:\\[\\operatorname{Var}(\\bar Y) = \\operatorname{Var}(Y)/n\\;,\\] \\(n\\) number observations\nsample. Specifically, Binomial example get \n\\(\\operatorname{Var}(\\bar Y) = 2.5/64\\), since variance Binomial component \n2.5 64 observations. Consequently, standard deviation\n\\(\\sqrt{2.5/64} = 0.1976424\\), agreement, less, \nresults simulation (produced 0.1982219 standard\ndeviation).Consider problem identifying central interval contains\n95% distribution. Normal distribution able use\nfunction “qnorm” order compute percentiles \ntheoretical distribution. function can used \npurpose simulated distribution function “quantile”. \nfirst argument function sequence simulated values \nstatistic, “Y.bar” current case. second argument \nnumber 0 1, sequence numbers:used sequence “c(0.025,0.975)” input second\nargument. result obtained output 4.609375, \n2.5%-percentile sampling distribution average, \n5.390625, 97.5%-percentile sampling distribution \naverage.interest compare percentiles parallel percentiles\nNormal distribution expectation \nstandard deviation average Binomials:Observe similarity percentiles distribution \naverage percentiles Normal distribution. \nsimilarity reflection Normal approximation sampling\ndistribution average, formulated next section\ntitle: Central Limit Theorem.","code":"\nY.bar <- rep(0,10^5)\nfor(i in 1:10^5) {\n  Y.samp <- rbinom(64,10,0.5)\n  Y.bar[i] <- mean(Y.samp)\n}\nmean(Y.bar)\n#> [1] 5.000628\nsd(Y.bar)\n#> [1] 0.1976092\nquantile(x = Y.bar, probs = c(0.025,0.975))\n#>     2.5%    97.5% \n#> 4.609375 5.390625\nqnorm(c(0.025,0.975),mean(Y.bar),sd(Y.bar))\n#> [1] 4.613321 5.387935"},{"path":"ChapSampDist.html","id":"law-of-large-numbers-and-central-limit-theorem","chapter":"10 The Sampling Distribution","heading":"10.3 Law of Large Numbers and Central Limit Theorem","text":"Law Large Numbers Central Limit Theorem mathematical\ntheorems describe sampling distribution average \nlarge samples.","code":""},{"path":"ChapSampDist.html","id":"the-law-of-large-numbers","chapter":"10 The Sampling Distribution","heading":"10.3.1 The Law of Large Numbers","text":"Law Large Numbers states , sample size becomes larger,\nsampling distribution sample average becomes \nconcentrated expectation.Let us demonstrate Law Large Numbers context \nUniform distribution. Let distribution measurement \\(Y\\) \n\\(\\mathrm{Uniform}(3,7)\\). Consider three different sample sizes \\(n\\):\n\\(n=10\\), \\(n=100\\), \\(n=1000\\). Let us carry simulation similar \nsimulations previous section. However, time run \nsimulation three sample sizes parallel:Observe produced 3 sequences length 100,000 :\n“unif.10”, “unif.100”, “unif.1000”. first sequence \napproximation sampling distribution average 10\nindependent Uniform measurements, second approximates sampling\ndistribution average 100 measurements third \ndistribution average 1000 measurements. distribution \nsingle measurement examples \\(\\mathrm{Uniform}(3,7)\\).Consider expectation sample average three sample sizes:sample size expectation sample average equal 5,\nexpectation \\(\\mathrm{Uniform}(3,7)\\) distribution.Recall variance \\(\\mathrm{Uniform}(,b)\\) distribution \n\\((b-)^2/12\\). Hence, variance given Uniform distribution \n\\(\\operatorname{Var}(X) = (7-3)^2/12 = 16/12 \\approx 1.3333\\). variances \nsample averages :Notice variances decrease increase sample\nsizes. decrease according formula\n\\(\\operatorname{Var}(\\bar Y) = \\operatorname{Var}(Y)/n\\).variance measure spread distribution \nexpectation. smaller variance concentrated \ndistribution around expectation. Consequently, agreement \nLaw Large Numbers, larger sample size concentrated\nsampling distribution sample average \nexpectation.","code":"\nunif.10 <- rep(0,10^5)\nunif.100 <- rep(0,10^5)\nunif.1000 <- rep(0,10^5)\nfor(i in 1:10^5) {\n  Y.samp.10 <- runif(10,3,7)\n  unif.10[i] <- mean(Y.samp.10)\n  Y.samp.100 <- runif(100,3,7)\n  unif.100[i] <- mean(Y.samp.100)\n  Y.samp.1000 <- runif(1000,3,7)\n  unif.1000[i] <- mean(Y.samp.1000)\n}\nmean(unif.10)\n#> [1] 4.999444\nmean(unif.100)\n#> [1] 5.000308\nmean(unif.1000)\n#> [1] 5.000169\nvar(unif.10)\n#> [1] 0.1332605\nvar(unif.100)\n#> [1] 0.01330791\nvar(unif.1000)\n#> [1] 0.001339139"},{"path":"ChapSampDist.html","id":"the-central-limit-theorem-clt","chapter":"10 The Sampling Distribution","heading":"10.3.2 The Central Limit Theorem (CLT)","text":"Law Large Numbers states distribution sample\naverage tends concentrated sample size increases. \nCentral Limit Theorem (CLT short) provides approximation \ndistribution.\nFigure 10.3: CLT Uniform(3,7) Distribution\ndeviation sample average expectation \nmeasurement tend decreases increase sample size. order\nobtain refined assessment deviation one needs magnify\n. appropriate way obtain magnification consider \nstandardized sample average, deviation sample\naverage expectation divided standard deviation \nsample average:\\[Z = \\frac{\\bar Y - \\operatorname{E}(\\bar Y)}{\\sqrt{\\operatorname{Var}(\\bar Y)}}\\;.\\]Recall expectation sample average equal \nexpectation single random variable (\\(\\operatorname{E}(\\bar Y) = \\operatorname{E}(Y)\\))\nvariance sample average equal variance \nsingle observation, divided sample size\n(\\(\\operatorname{Var}(\\bar Y) = \\operatorname{Var}(Y)/n\\)). Consequently, one may rewrite \nstandardized sample average form:\\[Z = \\frac{\\bar Y - \\operatorname{E}(Y)}{\\sqrt{\\operatorname{Var}(Y)/n}}= \\frac{\\sqrt{n}(\\bar Y - \\operatorname{E}(Y))}{\\sqrt{\\operatorname{Var}(Y)}}\\;.\\]\nsecond equality follows placing numerator square\nroot \\(n\\) divides term denominator. Observe \nincrease sample size decreasing difference \naverage expectation magnified square root \\(n\\).Central Limit Theorem states , increase sample size,\nsample average converges (standardization) standard\nNormal distribution.Let us examine Central Normal Theorem context example\nUniform measurement. Figure 10.3 may find\n(approximated) density standardized average three\nsample sizes based simulation carried previously (\nred, green, blue lines). Along side densities \nmay also find theoretical density standard Normal\ndistribution (black line). Observe four curves \nalmost one top , proposing approximation \ndistribution average Normal distribution good even \nsample size small \\(n=10\\).However, jumping conclusion Central Limit Theorem\napplies sample size, let us consider another example. \nexample repeat simulation Uniform\ndistribution, time take \\(\\mathrm{Exponential}(0.5)\\)\nmeasurements instead:\nFigure 10.4: CLT Exponential(0.5) Distribution\nexpectation \\(\\mathrm{Exponential}(0.5)\\) random variable \n\\(\\operatorname{E}(X) = 1/\\lambda = 1/0.5 = 2\\) variance \n\\(\\operatorname{Var}(X) = 1/\\lambda^2 = 1/(0.5)^2 = 4\\). Observe \nexpectations sample averages equal expectation \nmeasurement variances sample averages follow relation\n\\(\\operatorname{Var}(\\bar Y) = \\operatorname{Var}(Y)/n\\):expectations sample average equal 2. \nvariance get:agreement decrease proposed theory,However, one examines densities sample averages \nFigure 10.4 one may see clear distinction \nsampling distribution average sample size 10 \nnormal distribution (compare red curve black curve. \nmatch green curve corresponds sample size\n\\(n=100\\) black line better, perfect. sample\nsize large \\(n=1000\\) (blue curve) agreement \nnormal curve good.","code":"\nexp.10 <- rep(0,10^5)\nexp.100 <- rep(0,10^5)\nexp.1000 <- rep(0,10^5)\nfor(i in 1:10^5) {\n  X.samp.10 <- rexp(10,0.5)\n  exp.10[i] <- mean(X.samp.10)\n  X.samp.100 <- rexp(100,0.5)\n  exp.100[i] <- mean(X.samp.100)\n  X.samp.1000 <- rexp(1000,0.5)\n  exp.1000[i] <- mean(X.samp.1000)\n}\nmean(exp.10)\n#> [1] 1.9974\nmean(exp.100)\n#> [1] 1.999448\nmean(exp.1000)\n#> [1] 2.000381\nvar(exp.10)\n#> [1] 0.4020298\nvar(exp.100)\n#> [1] 0.04006796\nvar(exp.1000)\n#> [1] 0.003989475"},{"path":"ChapSampDist.html","id":"applying-the-central-limit-theorem","chapter":"10 The Sampling Distribution","heading":"10.3.3 Applying the Central Limit Theorem","text":"conclusion Central Limit Theorem sampling\ndistribution sample average can approximated Normal\ndistribution, regardless distribution original\nmeasurement, provided sample size large enough. \nstatement important, since allows us, context \nsample average, carry probabilistic computations using Normal\ndistribution even know actual distribution \nmeasurement. need know computation expectation\nmeasurement, variance (standard deviation) sample\nsize.theorem can applied whenever probability computations associated\nsampling distribution average required. \ncomputation approximation carried using Normal\ndistribution expectation standard deviation\nsample average.example computation conducted \nSubsection 10.2.3 central interval \ncontains 95% sampling distribution Binomial average \nrequired. 2.5%- 97.5%-percentiles Normal distribution\nexpectation variance sample average produced\nboundaries interval. boundaries good agreement\nboundaries produced simulation. examples \nprovided Solved Exercises chapter next one.usefulness, one treat Central Limit Theorem \ngrain salt. approximation may valid large samples, \nmay bad samples large enough. sample \nsmall careless application Central Limit Theorem may produce\nmisleading conclusions.","code":""},{"path":"ChapSampDist.html","id":"exercises-3","chapter":"10 The Sampling Distribution","heading":"10.4 Exercises","text":"subatomic particle hits linear detector random\nlocations. length detector 10 nm hits \nuniformly distributed. location 25 random hits, measured \nspecified endpoint interval, marked average \nlocation computed.expectation average location?66What standard deviation average location?67Use Central Limit Theorem order approximate \nprobability average location left-third \nlinear detector.68The central region contains 99% distribution \naverage form \\(5 \\pm c\\). Use Central Limit Theorem \norder approximate value c.69","code":""},{"path":"ChapSampDist.html","id":"summary-2","chapter":"10 The Sampling Distribution","heading":"10.5 Summary","text":"","code":""},{"path":"ChapSampDist.html","id":"glossary","chapter":"10 The Sampling Distribution","heading":"Glossary","text":"Random Sample:probabilistic model values measurements \nsample, measurement taken.\nprobabilistic model values measurements \nsample, measurement taken.Sampling Distribution:distribution random sample.\ndistribution random sample.Sampling Distribution Statistic:statistic function data; .e. formula applied \ndata. statistic becomes random variable formula \napplied random sample. distribution random\nvariable, inherited distribution sample, \nsampling distribution.\nstatistic function data; .e. formula applied \ndata. statistic becomes random variable formula \napplied random sample. distribution random\nvariable, inherited distribution sample, \nsampling distribution.Sampling Distribution Sample Average:distribution sample average, considered random\nvariable.\ndistribution sample average, considered random\nvariable.Law Large Numbers:mathematical result regarding sampling distribution \nsample average. States distribution average \nmeasurements highly concentrated vicinity \nexpectation measurement sample size large.\nmathematical result regarding sampling distribution \nsample average. States distribution average \nmeasurements highly concentrated vicinity \nexpectation measurement sample size large.Central Limit Theorem:mathematical result regarding sampling distribution \nsample average. States distribution average \napproximately Normal sample size large.\nmathematical result regarding sampling distribution \nsample average. States distribution average \napproximately Normal sample size large.","code":""},{"path":"ChapSampDist.html","id":"summary-of-formulas","chapter":"10 The Sampling Distribution","heading":"Summary of Formulas","text":"Expectation sample average:\\(\\operatorname{E}(\\bar Y) = \\operatorname{E}(Y)\\)\n\\(\\operatorname{E}(\\bar Y) = \\operatorname{E}(Y)\\)Variance sample average:\\(\\operatorname{Var}(\\bar Y) = \\operatorname{Var}(Y)/n\\)\n\\(\\operatorname{Var}(\\bar Y) = \\operatorname{Var}(Y)/n\\)","code":""},{"path":"CI.html","id":"CI","chapter":"11 Parameter Intervals","heading":"11 Parameter Intervals","text":"objective section provide simple examples reverse engineering show logic behind statistical ‘confidence intervals’ parameters. begin ‘100% confidence’ intervals, , following subsections, explain move ‘less--100% confidence’ intervals, things get bit nuanced.\nsections, emphasize reverse engineering, .e, using limits worst-case almost-worst-case scenarios involving (unknown) values parameter estimated.","code":""},{"path":"CI.html","id":"confidence-intervals","chapter":"11 Parameter Intervals","heading":"11.1 ‘100% confidence’ intervals","text":"","code":""},{"path":"CI.html","id":"example-1","chapter":"11 Parameter Intervals","heading":"11.1.1 Example 1","text":"Consider ‘particularistic’ parameter, height particular building. nothing ‘scientific’ parameter, except maybe use tools mathematical science (trignometry) measure . Nevertheless, sometimes refer one generic symbols parameter, namely \\(\\theta.\\)Suppose measure height building standing known horizontal distance (e.g. 100 metres) bottom building using instrument measure angle horizontal top building. Suppose, shown left panel Figure 11.1 , instrument gives reading 70 degrees.Remembering trigonometry tangent 70 degree angles 2.75, angle 70 degrees suggests height building \\(\\hat{\\theta}\\) = 275 metres. ‘hat’ statistical shorthand ‘estimate .’ Since sometimes referred ‘point estimate’ \\(\\theta\\), display value using dot point.calculating , learn measuring instrument displays angle nearest 10 degrees. means true angle somewhere 65 75 degrees.70So say true height exactly 275 metres. can say? certainty?can put limits true height asking minimum maximum heights produced observed reading 70 degrees?need take limits one time. minimum angle given (rounded) readout 70 degrees 65 degrees, corresponds minimum height (lower limit) height \\(\\theta_L\\) = 214 metres. maximum angle given readout 70 degrees 75 degrees, corresponds maximum height (upper limit) \\(\\theta_U\\) = 373 metres. Thus, assuming instrument measuring angle correctly, told , 100% confident true height lies interval (214, 373). clear graph, typical 275 \\(\\pm\\) single-number (sybols, \\(\\hat{\\theta} \\pm\\) [‘margin error’]) typically see reports.\nFigure 11.1: Estimating height building measuring subtended angles. ‘70’ left panel signifies real angle somewhere 65 75 degrees; thus real height lies L U limits 214 373 metres. righ panel, interval shown thicker black segment right 3 individual intervals set parameter values common 3.\ndataThe panel right shows , obtaining 3 measurements 3 different distances, finding interval common (overlap), can narrow interval within true height must lie.allows us 100% confident parameter intervalThe reason limited error range. wide error range , many measurements one makes, determine wide parameter interval .","code":""},{"path":"CI.html","id":"example-2","chapter":"11 Parameter Intervals","heading":"11.1.2 Example 2","text":"one less artificial, indeed motivated real court case late 1990s Quebec, defendant’s age (determine whether tried adult juvenile court) doubt. adopted, still young child, another country. Official birth records available, adoptive parents able get cheaper airfare claiming age 2 time. Bone age, Tanner Staging, also known Sexual Maturity Rating (SMR), objective classification system used track development sequence secondary sex characteristics children puberty, pieces information used judge.topic determining chronological age, see article, entitled Many applications medical statistics thos one, entitled People smugglers, statistics bone age, UCL statistics professor child growth expert, Tim Cole., person’s correct chronological age particularistic parameter, one nothing science, universal laws Nature. can estimated using laws mathematics statistics.didactic purposes, simplify matters, assume ‘’ indirect method gives estimates many averaged, give correct chronological age person (statistical lingo, statisticians say method/estimator ‘unbiased’). However, seen Figure (fig:ciage), individual measurements vary quite bit around correct age. can much 25% (1/4th) either direction.71 Another unrealistic feature ‘measurement model’ ‘error distribution’ finite range. shape error distribution doesn’t come 100% ‘confidence intervals’ , matter little bit – whole lot unless sample size small – later cut corners.Consider first single indirect measurement chronological age, yielded value 17.6 years.Given know sizes possible errors, say true age exactly 17.6 years can say? certainty?can put limits true age asking minimum maximum ages produced observed reading \n17.6 years.need consider limits one scenario time. minimum age given estimate 17.6 years /\n1.125 =\n15.7 years. maximum age produced reading \n17.6 / 0.875 = 20.1 years. Thus (assuming error model correct!) 100% confident true age lies interval (15.7 , 20.1) years. , clear graph, typical\n17.6 \\(\\pm\\) single-number margin error typically see reports. Rather, 17.6 - 2.6 17.6 + 4.4 !, can’t arrive directly; get way. try various limits, \\[ LowerLimit + margin  = 17.6 \\ = \\   UpperLimit - margin  \\]\nFigure 11.2: 100% Confidence Intervals person’s chronological age error distributions (example wider older ages) 100% confined within shaded ranges. Left: based n = 1 measurement; right: based n = 4 independent measurements.\ndataThe panel right shows , obtaining 4 independent measurements, finding interval common, can narrow interval containing true age.Can narrow interval , maybe first averaging 4 measurements? mean 4 measurements give us information, ie., tighter interval, one based overlap? sad fact , long insist 100% confidence interval (procedure), can : mean 3 measurements can still – theoretically – anywhere 0.75 \\(\\times\\) True Age 1.25 \\(\\times\\) True Age range – just single measurement can.way narrow interval take chance, cut corners, accept lower confidence level. , need know bit pattern (shape) error distribution (**now didn’t use shape, just boundaries). words, need know much error distribution corners, can cut !next section, stick now Daniel Bernoulli’s error distribution, cut corners. (Later , cut corners Laplace’s Gauss’s error distributions, standard deviation Bernoulli’s error curve.)","code":""},{"path":"CI.html","id":"more-nuanced-intervals","chapter":"11 Parameter Intervals","heading":"11.2 More-nuanced intervals","text":"cut 5% corner distribution, focus middlemost 90%. formula mathematical shape, can calculate measurement range \n-1 \\(\\times\\) radius semi-ellipse \n+1 \\(\\times\\) radius. 5% probability observing measurement (left ) interval, 5% probability observing measurement (right ) interval. observe single measurement, ‘try ’ possible true-age-scenarios. retain true-age-scenarios observed measurement fall within central (90%) range. discard (‘rule ’) age scenarios measurement one extreme extreme, one two excluded ‘cut’ corners.left panel Figure 11.3 shows (now narrower, nuanced) range true-ages (rahe parameter values) compatible observed measurement 13.1 years. age-scenarios, 13.1 extreme, scenarios discarded. can think ‘ruled ’ range (nuanced, compromise) parameter interval.Note method constructing non-symmetric parameter interval, namely one boundary time. fit \\(\\pm\\) mold., however, give way talk interval:observed measurement (point-estimate) may underestimate parameter: might fallen short true parameter value. , may overestimate: might overshot true parameter value. plus minus amounts almost-maximal amounts shot might -target. (see later, maximal error can infinite, put probalistic limits error narrow interval).Q: procedure constructing intervals 90% success rate, used ages, say 10 30 years? try people known ages.72You discover simulations might matter whether simulate number 16 year olds 10 years, .e., mix real ages . matter 100% intervals, might nuanced. example,instead estimating age indirect method, pretend estimating person’s height indirectly, just measuring arm span (height, mean armspan close height, spread armspans (pardon pun!)). (just like example 2 spread increases mean) spread armspans larger people 6 feet tall people 5 feet tall.\n, aren’t many people 5 feet 6 feet tall people 5 feet 6 inches. , distribution heights people span 5 feet 11 might different shape people span 5 feet 6, 5 feet. Simulations (even diagrams) settle issue whether height-mix (, example 2, age mix) matters. intuition whether affects perfornace nuanced parameter estimates? point method needs claimed performance (say 90%) age throw .\nFigure 11.3: 90% Confidence intervals Chronological Age 90% error distributions lie within shaded ranges.\n\\(n = 3\\) observations (right panel), easy say confident overlap 3 intervals.\nInstead, bettter taking mean 3 measurements, ‘trying ’ single mean various sampling distributions means 3 independent measurements semi-circular error distribution. , since range remains , cut corners.","code":""},{"path":"CI.html","id":"summary-3","chapter":"11 Parameter Intervals","heading":"11.3 Summary","text":"error distribution bounded, can 100% confident parameter interval, can narrow taking measurements. Moreover, don’t need specify exact shape error distribution. matters bounds.error distribution bounded, can 100% confident parameter interval, can narrow taking measurements. Moreover, don’t need specify exact shape error distribution. matters bounds.unbounded error distributions, 100% parameter interval may unacceptably wide, even take many measurements. Thus, ‘give something’ (certainty) order ‘get something’ (narrower interval). Moreover, need either () specify model shape error distribution, (b) use data-intensive techniques, re-sampling, able ‘cut corners.’unbounded error distributions, 100% parameter interval may unacceptably wide, even take many measurements. Thus, ‘give something’ (certainty) order ‘get something’ (narrower interval). Moreover, need either () specify model shape error distribution, (b) use data-intensive techniques, re-sampling, able ‘cut corners.’Either way, logical way determine parameter intervals consist parameter-value scenarios observed measurement (summary measurement) ‘plausible’. upper limit parameter scenario measurement probalistically near bottom corresponding sampling distribution; lower limit scenario measurement near top corresponding sampling distribution.Either way, logical way determine parameter intervals consist parameter-value scenarios observed measurement (summary measurement) ‘plausible’. upper limit parameter scenario measurement probalistically near bottom corresponding sampling distribution; lower limit scenario measurement near top corresponding sampling distribution.error (sampling) distributions differing spreads different parameter values, parameter interval symmetric point estimate. error (sampling) distributions spreads different parameter values, parameter interval symmetric point estimate, thus, easier calculate.error (sampling) distributions differing spreads different parameter values, parameter interval symmetric point estimate. error (sampling) distributions spreads different parameter values, parameter interval symmetric point estimate, thus, easier calculate.correct view parameter ‘falling’ one side measurement. true parameter values fixed, isn’t moving falling anywhere. Rather, observed measurement (point-estimate) may fallen left (fallen short ), thus provided underestimate , true parameter value: , may overshot true parameter value, thus overestimated . point also explains +/- formula fails usIt correct view parameter ‘falling’ one side measurement. true parameter values fixed, isn’t moving falling anywhere. Rather, observed measurement (point-estimate) may fallen left (fallen short ), thus provided underestimate , true parameter value: , may overshot true parameter value, thus overestimated . point also explains +/- formula fails us","code":""},{"path":"foundations-bootstrapping.html","id":"foundations-bootstrapping","chapter":"12 Confidence intervals with bootstrapping","heading":"12 Confidence intervals with bootstrapping","text":"section adapted book Introduction Modern Statistics73","code":""},{"path":"foundations-bootstrapping.html","id":"introduction","chapter":"12 Confidence intervals with bootstrapping","heading":"12.1 Introduction","text":"chapter, create confidence interval using data driven approach known bootstrap. process creating confidence interval based understanding statistic (sample proportion) varies around parameter (population proportion) many different statistics calculated many different samples., measure variability statistics repeatedly taking sample data population compute sample proportion.\n.\n.\ngood sense variability original estimate.variability across samples large, assume original statistic possibly far true population parameter interest (interval estimate wide).\nvariability across samples small, expect sample statistic close true parameter interest (interval estimate narrow).ideal world sampling data free extremely cheap almost never case, taking repeated samples population usually impossible., instead using “resample population” approach, bootstrapping uses “resample sample” approach.\nchapter provide examples details bootstrapping process.Bootstrapping best suited modeling studies data generated random sampling population.goal bootstrapping understand variability statistic.bootstrap model statistic varies one sample another taken population.\nprovide information different statistic parameter interest.Quantifying variability statistic sample sample hard problem.Fortunately, sometimes mathematical theory statistic varies (across different samples) known (e.g. CLT).However, statistics don’t simple theory vary, bootstrapping provides computational approach providing interval estimates almost population parameter.goal bootstrapping produce interval estimate (range plausible values) population parameter.","code":""},{"path":"foundations-bootstrapping.html","id":"case-study-med-consult","chapter":"12 Confidence intervals with bootstrapping","heading":"12.2 Medical consultant case study","text":"People providing organ donation sometimes seek help special medical consultant.\nconsultants assist patient aspects surgery, goal reducing possibility complications medical procedure recovery.\nPatients might choose consultant based part historical complication rate consultant’s clients.","code":""},{"path":"foundations-bootstrapping.html","id":"observed-data","chapter":"12 Confidence intervals with bootstrapping","heading":"12.2.1 Observed data","text":"One consultant tried attract patients noting average complication rate liver donor surgeries US 10%, clients 3 complications 62 liver donor surgeries facilitated.\nclaims strong evidence work meaningfully contributes reducing complications (therefore hired!).let \\(p\\) represent true complication rate liver donors working consultant.\n(“true” complication rate referred parameter.) estimate \\(p\\) using data, label estimate \\(\\hat{p}.\\)sample proportion complication rate 3 complications divided 62 surgeries consultant worked : \\(\\hat{p} = 3/62 = 0.048.\\)Parameter.parameter “true” value interest.typically estimate parameter using point estimate sample data.\npoint estimate also known statistic.example, estimate probability \\(p\\) complication client medical consultant examining past complications rates clients:\\[\\hat{p} = 3 / 62 = 0.048~\\text{used estimate}~p\\]","code":""},{"path":"foundations-bootstrapping.html","id":"variability-of-the-statistic","chapter":"12 Confidence intervals with bootstrapping","heading":"12.2.2 Variability of the statistic","text":"medical consultant case study, parameter \\(p,\\) true probability complication client medical consultant.\nreason believe \\(p\\) exactly \\(\\hat{p} = 3/62,\\) also reason believe \\(p\\) particularly far \\(\\hat{p} = 3/62.\\)\nsampling replacement dataset (process called bootstrapping), variability possible \\(\\hat{p}\\) values can approximated.inferential procedures covered text grounded quantifying one dataset differ another taken population.\ndoesn’t make sense take repeated samples population means take samples, larger sample size benefit separately evaluating two sample exact size.\nInstead, measure samples behave estimate population.Figure 12.1 shows unknown original population can estimated using sample approximate proportion successes failures (case, proportion complications complications medical consultant).\nFigure 12.1: unknown population estimated using observed sample data. Note can use sample create estimated bootstrapped population sample. observed data include three red four white marbles, estimated population contains 3/7 red marbles 4/7 white marbles.\ntaking repeated samples estimated population, variability sample sample can observed.\nFigure 12.2 repeated bootstrap samples obviously different original population.\nRecall bootstrap samples taken (estimated) population, differences due entirely natural variability sampling procedure.\nFigure 12.2: Bootstrap sampling provides measure sample sample variability. Note taking samples estimated population created observed data.\nsummarizing bootstrap samples (, using sample proportion), see, directly, variability sample proportion, \\(\\hat{p},\\) sample sample.\ndistribution \\(\\hat{p}_{boot}\\) example scenario shown Figure 12.3, full bootstrap distribution medical consultant data shown Figure 12.6.\nFigure 12.3: bootstrapped proportion estimated bootstrap sample. resulting bootstrap distribution (dotplot) provides measure proportions vary sample sample\nturns practice, difficult computers work infinite population (proportional breakdown sample).\nHowever, physical computational method produces equivalent bootstrap distribution sample proportion computationally efficient manner.Consider observed data bag marbles 3 success (red) 4 failures (white).\ndrawing marbles bag replacement, depict exact sampling process done infinitely large estimated population.\nFigure 12.4: Taking repeated resamples sample data process creating infinitely large estimate population. computationally feasible take resamples directly sample. Note resampling now done replacement (, original sample ever change) original sample estimated hypothetical population equivalent.\n\nFigure 12.5: comparison process sampling estimate infinite population resampling replacement original sample. Note dotplot bootstrapped proportions process statistics estimated equivalent.\napply bootstrap sampling process medical consultant example, consider client one marbles bag.\n59 white marbles (complication) 3 red marbles (complication).\nchoose 62 marbles bag (one time replacement) compute proportion simulated patients complications, \\(\\hat{p}_{boot},\\) “bootstrap” proportion represents single simulated proportion “resample sample” approach.simulation 62 patients, many expect complication?74One simulation isn’t enough get sense variability one bootstrap proportion another bootstrap proportion, repeat simulation 10,000 times using computer.Figure 12.6 shows distribution 10,000 bootstrap simulations.\nbootstrapped proportions vary zero 11.3%.\nvariability bootstrapped proportions leads us believe true probability complication (parameter, \\(p\\)) likely fall somewhere 0 11.3%, numbers capture 95% bootstrap resampled values.range values true proportion called bootstrap percentile confidence interval, see throughout next sections chapters.\nFigure 12.6: original medical consultant data bootstrapped 10,000 times. simulation creates sample original data probability complication \\(\\hat{p} = 3/62.\\) bootstrap 2.5 percentile proportion 0 97.5 percentile 0.113. result : 95% confident , population, true probability complication 0% 11.3%.\noriginal claim consultant’s true rate complication national rate 10%.\ninterval estimate 0 11.3% true probability complication indicate surgical consultant lower rate complications national average?\nExplain..\ninterval overlaps 10%, might consultant’s work associated lower risk complications, might consultant’s work associated higher risk (.e., greater 10%) complications!\nAdditionally, previously mentioned, observational study, even association can measured, evidence consultant’s work cause complication rate (higher lower).","code":""},{"path":"foundations-bootstrapping.html","id":"tapperscasestudy","chapter":"12 Confidence intervals with bootstrapping","heading":"12.3 Tappers and listeners case study","text":"’s game can try friends family: pick simple, well-known song, tap tune desk, see person can guess song.\nsimple game, tapper, person listener.","code":""},{"path":"foundations-bootstrapping.html","id":"observed-data-1","chapter":"12 Confidence intervals with bootstrapping","heading":"12.3.1 Observed data","text":"Stanford University graduate student named Elizabeth Newton conducted experiment using tapper-listener game.75\nstudy, recruited 120 tappers 120 listeners study.\n50% tappers expected listener able guess song.\nNewton wondered, 50% reasonable expectation?Newton’s study, 3 120 listeners (\\(\\hat{p} = 0.025\\)) able guess tune!\nseems like quite low number leads researcher ask: true proportion people can guess tune?","code":""},{"path":"foundations-bootstrapping.html","id":"variability-of-the-statistic-1","chapter":"12 Confidence intervals with bootstrapping","heading":"12.3.2 Variability of the statistic","text":"answer question, use simulation.\nsimulate 120 games, time use bag 120 marbles 3 red (guessed correctly) 117 white (guess song).\nSampling bag 120 times (remembering replace marble back bag time keep constant population proportion red) produces one bootstrap sample.example, can start simulating 5 tapper-listener pairs sampling 5 marbles bag 3 red 117 white marbles.selecting 120 marbles, counted 2 red \\(\\hat{p}_{boot1} = 0.0167.\\) randomization technique, seeing happen one simulation isn’t enough.\norder understand far observed proportion 0.025 might true parameter, generate simulations.\n’ve repeated entire simulation ten times:\\[0.0417 \\quad 0.025 \\quad 0.025 \\quad 0.0083 \\quad 0.05 \\quad 0.0333 \\quad 0.025 \\quad 0 \\quad 0.0083 \\quad 0\\] , ’ll run total 10,000 simulations using computer.\nseen Figure 12.7, range 95% resampled values \\(\\hat{p}_{boot}\\) 0.000 0.0583.\n, expect 0% 5.83% people truly able guess tapper’s tune.\nFigure 12.7: original listener-tapper data bootstrapped 10,000 times. simulation creates sample probability correct \\(\\hat{p} = 3/120.\\) 2.5 percentile proportion 0 97.5 percentile 0.0583. result confident , population, true percent people can guess correctly 0% 5.83%.\ndata provide convincing evidence claim 50% listeners can guess tapper’s tune?76","code":""},{"path":"foundations-bootstrapping.html","id":"ConfidenceIntervals","chapter":"12 Confidence intervals with bootstrapping","heading":"12.4 Confidence intervals","text":"point estimate provides single plausible value parameter.\nHowever, point estimate rarely perfect; usually error estimate.\naddition supplying point estimate parameter, next logical step provide plausible range values parameter.","code":""},{"path":"foundations-bootstrapping.html","id":"plausible-range-of-values-for-the-population-parameter","chapter":"12 Confidence intervals with bootstrapping","heading":"12.4.1 Plausible range of values for the population parameter","text":"plausible range values population parameter called confidence interval.\nUsing single point estimate like fishing murky lake spear, using confidence interval like fishing net.\ncan throw spear saw fish, probably miss.\nhand, toss net area, good chance catching fish.report point estimate, probably hit exact population parameter.\nhand, report range plausible values – confidence interval – good shot capturing parameter.want certain capture population parameter, use wider interval (e.g., 99%) smaller interval (e.g., 80%)?77","code":""},{"path":"foundations-bootstrapping.html","id":"bootstrap-confidence-interval","chapter":"12 Confidence intervals with bootstrapping","heading":"12.4.2 Bootstrap confidence interval","text":"saw , bootstrap sample sample original sample.\ncase medical complications data, proceed follows:Randomly sample one observation 62 patients (replace marble back bag keep population constant).Randomly sample second observation 62 patients. sample replacement (.e., don’t actually remove marbles bag), 1--62 chance second observation one sampled first step!Keep going one sampled observation time …Randomly sample 62nd observation 62 patients.Bootstrap sampling often called sampling replacement.bootstrap sample behaves similarly actual sample population behave, compute point estimate interest (, compute \\(\\hat{p}_{boot}\\)).Due theory beyond text, know bootstrap proportions \\(\\hat{p}_{boot}\\) vary around \\(\\hat{p}\\) similar way different sample proportions (.e., values \\(\\hat{p}\\)) vary around true parameter \\(p.\\)Therefore, interval estimate \\(p\\) can produced using \\(\\hat{p}_{boot}\\) values .95% Bootstrap percentile confidence interval parameter \\(p.\\)95% bootstrap confidence interval parameter \\(p\\) can obtained directly using ordered \\(\\hat{p}_{boot}\\) values.Consider sorted \\(\\hat{p}_{boot}\\) values.\nCall 2.5% bootstrapped proportion value “lower”, call 97.5% bootstrapped proportion value “upper”.95% confidence interval given : (lower, upper)","code":""},{"path":"foundations-bootstrapping.html","id":"summary-4","chapter":"12 Confidence intervals with bootstrapping","heading":"12.5 Summary","text":"Figure 12.8 provides visual summary creating bootstrap confidence intervals.\nFigure 12.8: use sampling replacement measure variability statistic interest (proportion). Sampling replacement computational tool equivalent using sample way estimating infinitely large population sample.\ncan summarize bootstrap process follows:Frame research question terms parameter estimate. Confidence Intervals appropriate research questions aim estimate number population (called parameter).Collect data observational study experiment. research question can formed query parameter, can collect data calculate statistic best guess value parameter. However, know statistic won’t exactly equal parameter due natural variability.Model randomness using data values proxy population. order assess far statistic might parameter, take repeated resamples dataset measure variability bootstrapped statistics. variability bootstrapped statistics around observed statistic (quantity can measured computational technique) approximately variability many observed sample statistics around parameter (quantity difficult measure real life get exactly one sample).Create interval. choosing particular confidence level, use variability bootstrapped statistics create interval estimate hope capture true parameter. interval estimate associated particular sample hand may may capture parameter, researcher knows lifetime, confidence level determine percentage research confidence intervals capture true parameter.Form conclusion. Using confidence interval analysis, report interval estimate parameter interest. Also, sure write conclusion plain language casual readers can understand results.Table 12.1 another look Bootstrap process summary.\nTable 12.1: Summary bootstrapping inferential statistical method.\n","code":""},{"path":"foundations-bootstrapping.html","id":"terms","chapter":"12 Confidence intervals with bootstrapping","heading":"12.5.1 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"inference-one-mean.html","id":"inference-one-mean","chapter":"13 Inference for a single mean","heading":"13 Inference for a single mean","text":"section adapted book Introduction Modern Statistics78","code":""},{"path":"inference-one-mean.html","id":"introduction-1","chapter":"13 Inference for a single mean","heading":"13.1 Introduction","text":"important data structure chapter numeric response variable (, outcome quantitative).introduce new important mathematical model, \\(t\\)-distribution (foundation \\(t\\)-test).chapter, focus sample mean (instead , example, sample median range observations) well-studied mathematical model describes behavior sample mean.\ncover mathematical models describe statistics, bootstrap randomization techniques described immediately extendable function observed data.\nsample mean calculated one group, two paired groups, two independent groups, many groups settings.\ntechniques described setting vary slightly, well served find structural similarities across different settings.sample mean \\(\\bar{y}\\) can also modeled using normal distribution certain conditions met.\n However, ’ll soon learn new distribution, called \\(t\\)-distribution, tends useful working sample mean.\n’ll first learn new distribution, ’ll use construct confidence intervals conduct hypothesis tests mean.","code":""},{"path":"inference-one-mean.html","id":"boot1mean","chapter":"13 Inference for a single mean","heading":"13.2 Bootstrap confidence interval for a mean","text":"Consider situation want know whether buy franchise used car store Awesome Autos.\npart planning, ’d like know much average car Awesome Autos sells.\norder go example clearly, let’s say able randomly sample five cars Awesome Auto.\n(real example, surely able take much larger sample size, possibly even able measure entire population!)","code":""},{"path":"inference-one-mean.html","id":"observed-data-2","chapter":"13 Inference for a single mean","heading":"13.2.1 Observed data","text":"Figure 13.1 shows (small) random sample observations Awesome Auto.\nactual cars well selling price shown.\nFigure 13.1: sample five cars Awesome Auto.\nsample average car price $17140.00 first guess price average car price Awesome Auto.\nHowever, student statistics, understand one sample mean based sample five observations necessarily equal true population average car price cars Awesome Auto.\nIndeed, can see observed car prices vary standard deviation $7170.29, surely average car price different different sample size five taken population.\nFortunately, previous chapters sample proportion, bootstrapping approximate variability sample mean sample sample.","code":""},{"path":"inference-one-mean.html","id":"variability-of-the-statistic-2","chapter":"13 Inference for a single mean","heading":"13.2.2 Variability of the statistic","text":"inferential analysis methods chapter grounded quantifying one dataset differs another taken population.\nrepeat, idea want know datasets differ one another, aren’t ever going take one sample observations.\ndoesn’t make sense take repeated samples population ability take samples, larger sample size benefit taking two samples population.\nInstead taking repeated samples actual population, use bootstrapping measure samples behave estimate population.mentioned previously, get sense cars Awesome Auto, take sample five cars Awesome Auto branch near way gauge price cars sold.\nFigure 13.2 shows unknown original population can estimated using sample approximate distribution car prices population cars Awesome Auto.\nFigure 13.2: seen previously, idea behind bootstrapping consider sample hand estimate population. Sampling sample (5 cars) identical sampling infinite population made cars original sample.\ntaking repeated samples estimated population, variability sample sample can observed.\nFigure 12.2 repeated bootstrap samples seen different original population.\nRecall bootstrap samples taken (estimated) population, differences bootstrap samples due entirely natural variability sampling procedure.\nsituation hand sample mean statistic interest, variability sample sample can seen Figure 13.3.\nFigure 13.3: estimate natural variability sample mean, different bootstrap samples taken original sample. Notice bootstrap resample different well original sample\nsummarizing bootstrap samples (, using sample mean), see, directly, variability sample mean, \\(\\bar{y},\\) sample sample.\ndistribution \\(\\bar{y}_{bs}\\) Awesome Auto cars shown Figure 13.4.\nFigure 13.4: bootstrap resamples respresents different set cars, mean bootstrap resample different value. bootstrapped means calculated, histogram values describes inherent natural variability sample mean due sampling process.\nFigure 13.5 summarizes one thousand bootstrap samples histogram bootstrap sample means.\nbootstrapped average car prices vary $10,000 $25,000.\nbootstrap percentile confidence interval found locating middle 90% (90% confidence interval) 95% (95% confidence interval) bootstrapped statistics.Using Figure 13.5, find 90% 95% bootstrap percentile confidence intervals true average price car Awesome Auto.90% confidence interval given $12,140 $22,007.\nconclusion 90% confident true average car price Awesome Auto lies somewhere $12,140 $22,007.95% confidence interval given $11,778 $22,500.\nconclusion 95% confident true average car price Awesome Auto lies somewhere $11,778 $22,500.\nFigure 13.5: original Awesome Auto data bootstrapped 1,000 times. histogram provides sense variability average car price sample sample.\n","code":""},{"path":"inference-one-mean.html","id":"bootstrap-se-confidence-interval","chapter":"13 Inference for a single mean","heading":"13.2.3 Bootstrap SE confidence interval","text":"Another method creating bootstrap confidence intervals directly uses calculation variability bootstrap statistics (, bootstrap means).\nbootstrap distribution relatively symmetric bell-shaped, 95% bootstrap SE confidence interval can constructed formula familiar mathematical models previous chapters:\\[\\mbox{point estimate} \\pm 2 \\cdot SE_{BS}\\] quantile function. number 2 approximation connected “95%” part confidence interval (remember 68-95-99.7 rule).\nseen Section 13.3, new distribution (\\(t\\)-distribution) applied mathematical inference numerical variables.\nHowever, bootstrapping grounded theory mathematical approach given text, stick standard normal quantiles (R use function qnorm() find normal percentiles 95%) different confidence percentages.79Explain standard error (SE) bootstrapped means calculated measuring.SE bootstrapped means measures variable means resample resample.\nbootstrap SE good approximation SE means taken repeated samples original population (agreed isn’t something wasted resources).Logistically, can find standard deviation bootstrapped means using calculations descriptive statistics section.\n, bootstrapped means individual observations measure variability.turns standard deviation bootstrapped means Figure 13.5 $2,891.87 (value excellent approximation standard error sample means take repeated samples population).\n[Note: R calculation done using function sd().] average observed prices $17,140, ad consider sample average best guess point estimate \\(\\mu.\\) .Find interpret confidence interval \\(\\mu\\) (true average cost car Awesome Auto) using bootstrap SE confidence interval formula.80Compare contrast two different 95% confidence intervals \\(\\mu\\) created finding percentiles bootstrapped means created finding SE bootstrapped means.\nthink intervals identical?Percentile interval: ($11,778, $22,500)SE interval: ($11,356.26, $22,923.74)intervals created using different methods, surprising identical.\nHowever, pleased see two methods provide similar interval approximations.technical details surrounding data structures best percentile intervals best SE intervals beyond scope text.\nHowever, larger samples , better (closer) interval estimates .","code":""},{"path":"inference-one-mean.html","id":"bootstrap-percentile-confidence-interval-for-a-standard-deviation","chapter":"13 Inference for a single mean","heading":"13.2.4 Bootstrap percentile confidence interval for a standard deviation","text":"Suppose research question hand seeks understand variable prices cars Awesome Auto.\n, interest longer average car price standard deviation prices cars Awesome Auto, \\(\\sigma.\\) may already realized sample standard deviation, \\(s,\\) work good point estimate parameter interest: population standard deviation, \\(\\sigma.\\) point estimate five observations calculated \\(s = \\$7,170.286.\\) \\(s = \\$7,170.286\\) might good guess \\(\\sigma,\\) prefer interval estimate parameter interest.\nAlthough mathematical model describes \\(s\\) varies sample sample, mathematical model presented text.\nEven without mathematical model, bootstrapping can used find confidence interval parameter \\(\\sigma.\\) Using technique presented confidence interval \\(\\mu,\\) find bootstrap percentile confidence interval \\(\\sigma.\\)Describe bootstrap distribution standard deviation shown Figure 13.6.distribution skewed left centered near $7,170.286, point estimate original data.\nobservations distribution lie $0 $10,000.Using Figure 13.6, find interpret 90% bootstrap percentile confidence interval population standard deviation car prices Awesome Auto.81\nFigure 13.6: original Awesome Auto data bootstrapped 1,000 times. histogram provides sense variability standard deviation car prices sample sample.\n","code":""},{"path":"inference-one-mean.html","id":"bootstrapping-is-not-a-solution-to-small-sample-sizes","chapter":"13 Inference for a single mean","heading":"13.2.5 Bootstrapping is not a solution to small sample sizes!","text":"example presented done sample five observations.\nanalysis techniques build mathematical models, bootstrapping works best large random sample taken population.\nBootstrapping method capturing variability statistic mathematical model unknown (method navigating small samples).\nmight guess, larger random sample, accurately sample represent population interest.","code":""},{"path":"inference-one-mean.html","id":"one-mean-math","chapter":"13 Inference for a single mean","heading":"13.3 Mathematical model for a mean","text":"sample proportion, variability sample mean well described mathematical theory given Central Limit Theorem.\nHowever, missing information inherent variability population (\\(\\sigma\\)), \\(t\\)-distribution used place standard normal performing hypothesis test confidence interval analyses.","code":""},{"path":"inference-one-mean.html","id":"mathematical-distribution-of-the-sample-mean","chapter":"13 Inference for a single mean","heading":"13.3.1 Mathematical distribution of the sample mean","text":"sample mean tends follow normal distribution centered population mean, \\(\\mu,\\) certain conditions met.\nAdditionally, can compute standard error sample mean using population standard deviation \\(\\sigma\\) sample size \\(n.\\)Central Limit Theorem sample mean.collect sufficiently large sample \\(n\\) independent observations population mean \\(\\mu\\) standard deviation \\(\\sigma,\\) sampling distribution \\(\\bar{y}\\) nearly normal \\[\\text{Mean} = \\mu \\qquad \\text{Standard Error }(SE) = \\frac{\\sigma}{\\sqrt{n}}\\]diving confidence intervals hypothesis tests using \\(\\bar{y},\\) first need cover two topics:modeled \\(\\hat{p}\\) using normal distribution, certain conditions satisfied. conditions working \\(\\bar{y}\\) little complex, , discuss check conditions inference using mathematical model.standard error dependent population standard deviation, \\(\\sigma.\\) However, rarely know \\(\\sigma,\\) instead must estimate . estimation imperfect, use new distribution called \\(t\\)-distribution fix problem, discuss .","code":""},{"path":"inference-one-mean.html","id":"evaluating-the-two-conditions-required-for-modeling-bary","chapter":"13 Inference for a single mean","heading":"13.3.2 Evaluating the two conditions required for modeling \\(\\bar{y}\\)","text":"Two conditions required apply Central Limit Theorem sample mean \\(\\bar{y}:\\)Independence. sample observations must independent. common way satisfy condition sample simple random sample population.\ndata come random process, analogous rolling die, also satisfy independence condition.Independence. sample observations must independent. common way satisfy condition sample simple random sample population.\ndata come random process, analogous rolling die, also satisfy independence condition.Normality. sample small, also require sample observations come normally distributed population.\ncan relax condition larger larger sample sizes.Normality. sample small, also require sample observations come normally distributed population.\ncan relax condition larger larger sample sizes.aren’t expected develop perfect judgment normality condition.\nHowever, expected able handle clear cut cases based rules thumb.82Consider four plots provided Figure 13.7 come simple random samples different populations.\nsample sizes \\(n_1 = 15\\) \\(n_2 = 50.\\)independence normality conditions met case?samples simple random sample respective population, independence condition satisfied.\nLet’s next check normality condition using rule thumb.first sample little observations, watching clear outliers.\nNone present; small gap histogram right, gap small 20% observations small sample represented left gap, can hardly call clear outliers.\nclear outliers, normality condition can reasonably assumed met.second sample sample size larger includes outlier appears roughly 5 times center distribution next furthest observation.\nexample particularly extreme outlier, normality condition satisfied.’s often helpful also visualize data using box plot assess skewness existence outliers.\nbox plots provided underneath histogram confirms conclusions first sample outliers second sample , one outlier particularly extreme others.\nFigure 13.7: Histograms samples two different populations.\npractice, ’s typical also mental check evaluate whether reason believe underlying population moderate skew particularly extreme outliers beyond observe data.\nexample, consider number followers individual account Twitter, imagine distribution.\nlarge majority accounts built couple thousand followers fewer, relatively tiny fraction amassed tens millions followers, meaning distribution extremely skewed.\nknow data come extremely skewed distribution, takes effort understand sample size large enough normality condition satisfied.","code":""},{"path":"inference-one-mean.html","id":"introducing-the-t-distribution","chapter":"13 Inference for a single mean","heading":"13.3.3 Introducing the t-distribution","text":"practice, directly calculate standard error \\(\\bar{y}\\) since know population standard deviation, \\(\\sigma.\\) encountered similar issue computing standard error sample proportion, relied population proportion, \\(p.\\) solution proportion context use sample value place population value computing standard error.\n’ll employ similar strategy computing standard error \\(\\bar{y},\\) using sample standard deviation \\(s\\) place \\(\\sigma:\\)\\[SE = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}\\]strategy tends work well lot data can estimate \\(\\sigma\\) using \\(s\\) accurately.\nHowever, estimate less precise smaller samples, leads problems using normal distribution model \\(\\bar{y}.\\)’ll find useful use new distribution inference calculations called \\(t\\)-distribution.\n\\(t\\)-distribution, shown solid line Figure 13.8, bell shape.\nHowever, tails thicker normal distribution’s, meaning observations likely fall beyond two standard deviations mean normal distribution.extra thick tails \\(t\\)-distribution exactly correction needed resolve problem (due extra variability T score) using \\(s\\) place \\(\\sigma\\) \\(SE\\) calculation.\nFigure 13.8: Comparison \\(t\\)-distribution normal distribution.\n\\(t\\)-distribution always centered zero single parameter: degrees freedom.\ndegrees freedom describes precise form bell-shaped \\(t\\)-distribution.\nSeveral \\(t\\)-distributions shown Figure 13.9 comparison normal distribution.\nSimilar Chi-square distribution, shape \\(t\\)-distribution also depends degrees freedom.general, ’ll use \\(t\\)-distribution \\(df = n - 1\\) model sample mean sample size \\(n.\\) , observations, degrees freedom larger \\(t\\)-distribution look like standard normal distribution; degrees freedom 30 , \\(t\\)-distribution nearly indistinguishable normal distribution.\nFigure 13.9: larger degrees freedom, closely \\(t\\)-distribution resembles standard normal distribution.\nDegrees freedom: df.degrees freedom describes shape \\(t\\)-distribution.\nlarger degrees freedom, closely distribution approximates normal distribution.modeling \\(\\bar{y}\\) using \\(t\\)-distribution, use \\(df = n - 1.\\)\\(t\\)-distribution allows us greater flexibility normal distribution analyzing numerical data.\npractice, ’s common use statistical software, R, Python, SAS analyses.\nR, function used calculating probabilities \\(t\\)-distribution pt() (seem similar previous R functions, pnorm() pchisq()).\nDon’t forget \\(t\\)-distribution, degrees freedom must always specified!examples guided practices , may use table statistical software find answers.\nrecommend trying problems get sense \\(t\\)-distribution can vary width depending degrees freedom.\nmatter approach choose, apply method using examples confirm working understanding \\(t\\)-distribution.proportion \\(t\\)-distribution 18 degrees freedom falls -2.10?Just like normal probability problem, first draw picture Figure 13.10 shade area -2.10.Using statistical software, can obtain precise value: 0.0250.\nFigure 13.10: \\(t\\)-distribution 18 degrees freedom. area -2.10 shaded.\n\\(t\\)-distribution 20 degrees freedom shown Figure 13.11.\nEstimate proportion distribution falling 1.65.Note 20 degrees freedom, \\(t\\)-distribution relatively close normal distribution.\nnormal distribution, correspond 0.05, expect \\(t\\)-distribution give us value neighborhood.\nUsing statistical software: 0.0573.\nFigure 13.11: Top: \\(t\\)-distribution 20 degrees freedom, area 1.65 shaded.\n\\(t\\)-distribution 2 degrees freedom shown Figure 13.12.\nEstimate proportion distribution falling 3 units mean ().degrees freedom, \\(t\\)-distribution give notably different value normal distribution.\nnormal distribution, area 0.003 using 68-95-99.7 rule.\n\\(t\\)-distribution \\(df = 2,\\) area tails beyond 3 units totals 0.0955.\narea dramatically different obtain normal distribution.\nFigure 13.12: \\(t\\)-distribution 2 degrees freedom, area 3 units 0 shaded.\nproportion \\(t\\)-distribution 19 degrees freedom falls -1.79 units?\nUse preferred method finding tail areas.83","code":"\n# use pt() to find probability under the $t$-distribution\npt(-2.10, df = 18)\n#> [1] 0.0250452\n# use pt() to find probability under the $t$-distribution\n1 - pt(1.65, df = 20)\n#> [1] 0.05728041\n# use pt() to find probability under the $t$-distribution\npt(-3, df = 2) + (1 - pt(3, df = 2))\n#> [1] 0.09546597"},{"path":"inference-one-mean.html","id":"one-sample-t-intervals","chapter":"13 Inference for a single mean","heading":"13.3.4 One sample t-intervals","text":"Let’s get first taste applying \\(t\\)-distribution context example mercury content dolphin muscle.\nElevated mercury concentrations important problem dolphins animals, like humans, occasionally eat .\nFigure 13.13: Risso’s dolphin. Photo Mike Baird, www.bairdphotos.com\nidentify confidence interval average mercury content dolphin muscle using sample 19 Risso’s dolphins Taiji area Japan.\ndata summarized Table 13.1.\nminimum maximum observed values can used evaluate whether clear outliers.\nTable 13.1: Summary mercury content muscle 19 Risso’s dolphins Taiji area. Measurements micrograms mercury per wet gram muscle \\((\\mu\\)g/wet g).\nindependence normality conditions satisfied dataset?observations simple random sample, therefore reasonable assume dolphins independent.\nsummary statistics Table 13.1 suggest clear outliers, observations within 2.5 standard deviations mean.\nBased evidence, normality condition seems reasonable.normal model, used \\(z^{\\star}\\) standard error determine width confidence interval.\nrevise confidence interval formula slightly using \\(t\\)-distribution:\\[\n\\begin{aligned}\n\\text{point estimate} \\ &\\pm\\  t^{\\star}_{df} \\times SE \\\\\n\\bar{y} \\ &\\pm\\  t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}}\n\\end{aligned}\n\\]Using summary statistics Table 13.1, compute standard error average mercury content \\(n = 19\\) dolphins.plug \\(s\\) \\(n\\) formula: \\(SE = \\frac{s}{\\sqrt{n}} = \\frac{2.3}{\\sqrt{19}} = 0.528.\\)value \\(t^{\\star}_{df}\\) cutoff obtain based confidence level \\(t\\)-distribution \\(df\\) degrees freedom.\ncutoff found way normal distribution: find \\(t^{\\star}_{df}\\) fraction \\(t\\)-distribution \\(df\\) degrees freedom within distance \\(t^{\\star}_{df}\\) 0 matches confidence level interest.\\(n = 19,\\) appropriate degrees freedom?\nFind \\(t^{\\star}_{df}\\) degrees freedom confidence level 95%degrees freedom easy calculate: \\(df = n - 1 = 18.\\)Using statistical software, find cutoff upper tail equal 2.5%: \\(t^{\\star}_{18} = 2.10.\\) area -2.10 also equal 2.5%.\n, 95% \\(t\\)-distribution \\(df = 18\\) lies within 2.10 units 0.Degrees freedom single sample.sample \\(n\\) observations examining single mean, use \\(t\\)-distribution \\(df=n-1\\) degrees freedom.Compute interpret 95% confidence interval average mercury content Risso’s dolphins.can construct confidence interval \\[\n\\begin{aligned}\n\\bar{y} \\ &\\pm\\  t^{\\star}_{18} \\times SE \\\\\n4.4 \\ &\\pm\\  2.10 \\times 0.528 \\\\\n(3.29 \\  &, \\ 5.51)\n\\end{aligned} \n\\]95% confident average mercury content muscles Risso’s dolphins 3.29 5.51 \\(\\mu\\)g/wet gram, considered extremely high.Calculating \\(t\\)-confidence interval mean, \\(\\mu.\\)Based sample \\(n\\) independent nearly normal observations, confidence interval population mean \\[\n\\begin{aligned}\n\\text{point estimate} \\ &\\pm\\  t^{\\star}_{df} \\times SE \\\\\n\\bar{y} \\ &\\pm\\  t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}}\n\\end{aligned}\n\\]\\(\\bar{y}\\) sample mean, \\(t^{\\star}_{df}\\) corresponds confidence level degrees freedom \\(df,\\) \\(SE\\) standard error estimated sample.FDA’s webpage provides data mercury content fish.\nBased sample 15 croaker white fish (Pacific), sample mean standard deviation computed 0.287 0.069 ppm (parts per million), respectively.\n15 observations ranged 0.18 0.41 ppm.\nassume observations independent.\nBased summary statistics data, objections normality condition individual observations?84Estimate standard error \\(\\bar{y} = 0.287\\) ppm using data summaries previous Guided Practice.\nuse \\(t\\)-distribution create 90% confidence interval actual mean mercury content, identify degrees freedom \\(t^{\\star}_{df}.\\)standard error: \\(SE = \\frac{0.069}{\\sqrt{15}} = 0.0178.\\)Degrees freedom: \\(df = n - 1 = 14.\\)Since goal 90% confidence interval, choose \\(t_{14}^{\\star}\\) two-tail area 0.1: \\(t^{\\star}_{14} = 1.76.\\)Using information results previous Guided Practice Example, compute 90% confidence interval average mercury content croaker white fish (Pacific).85The 90% confidence interval previous Guided Practice 0.256 ppm 0.318 ppm.\nCan say 90% croaker white fish (Pacific) mercury levels 0.256 0.318 ppm?86Recall margin error defined standard error.\nmargin error \\(\\bar{y}\\) can directly obtained \\(SE(\\bar{y}).\\)Margin error \\(\\bar{y}.\\)margin error \\(t^\\star_{df} \\times s/\\sqrt{n}\\) \\(t^\\star_{df}\\) calculated specified percentile t-distribution df degrees freedom.","code":"\n# use qt() to find the t-cutoff (with 95% in the middle)\nqt(0.025, df = 18)\n#> [1] -2.100922\nqt(0.975, df = 18)\n#> [1] 2.100922\n# use qt() to find the t-cutoff (with 90% in the middle)\nqt(0.05, df = 14)\n#> [1] -1.76131\nqt(0.95, df = 14)\n#> [1] 1.76131"},{"path":"inference-one-mean.html","id":"one-sample-t-tests","chapter":"13 Inference for a single mean","heading":"13.3.5 One sample t-tests","text":"Now ’ve used \\(t\\)-distribution making confidence interval mean, let’s speed hypothesis tests mean.test statistic assessing single mean T.T score ratio sample mean differs hypothesized mean compared observations vary.\\[ T = \\frac{\\bar{y} - \\mbox{null value}}{s/\\sqrt{n}} \\]null hypothesis true conditions met, T t-distribution \\(df = n - 1.\\)Conditions:Independent observations.Large samples extreme outliers.typical US runner getting faster slower time?\nconsider question context Cherry Blossom Race, 10-mile race Washington, DC spring.\naverage time runners finished Cherry Blossom Race 2006 93.29 minutes (93 minutes 17 seconds).\nwant determine using data 100 participants 2017 Cherry Blossom Race whether runners race getting faster slower, versus possibility change.run17 data can found cherryblossom R package.appropriate hypotheses context?87The data come simple random sample participants, observations independent.histogram race times given evaluate can move forward t-test.\nnormality condition met?88When completing hypothesis test one-sample mean, process nearly identical completing hypothesis test single proportion.\nFirst, find Z score using observed value, null value, standard error; however, call T score since use \\(t\\)-distribution calculating tail area.\nfind p-value using ideas used previously: find one-tail area sampling distribution, double .independence normality conditions satisfied, can proceed hypothesis test using \\(t\\)-distribution.\nsample mean sample standard deviation sample 100 runners 2017 Cherry Blossom Race 98.78 16.59 minutes, respectively.\nRecall average run time 2006 93.29 minutes.\nFind test statistic p-value.\nconclusion?find test statistic (T score), first must determine standard error:\\[  SE = 16.6 / \\sqrt{100} = 1.66 \\]Now can compute T score using sample mean (98.78), null value (93.29), \\(SE:\\)\\[ T = \\frac{98.8 - 93.29}{1.66} = 3.32 \\]\\(df = 100 - 1 = 99,\\) can determine using statistical software (\\(t\\)-table) one-tail area 0.000631, double get p-value: 0.00126.p-value smaller 0.05, reject null hypothesis.\n, data provide convincing evidence average run time Cherry Blossom Run 2017 different 2006 average.using \\(t\\)-distribution, use T score (similar Z score).help us remember use \\(t\\)-distribution, use \\(T\\) represent test statistic, often call T score.\nZ score T score computed exact way conceptually identical: represents many standard errors observed value null value.","code":"\n# using pt() to find the left tail and multiply by 2 to get both tails\n(1 - pt(3.32, df = 99)) * 2 \n#> [1] 0.001261064"},{"path":"inference-one-mean.html","id":"chp19-review","chapter":"13 Inference for a single mean","heading":"13.4 Chapter review","text":"","code":""},{"path":"inference-one-mean.html","id":"summary-5","chapter":"13 Inference for a single mean","heading":"13.4.1 Summary","text":"chapter extended randomization / bootstrap / mathematical model paradigm questions involving quantitative variables interest.\none variable interest, often hypothesizing finding confidence intervals population mean.\nNote, however, bootstrap method can used statistics like population median population IQR.\ncomparing quantitative variable across two groups, question often focuses difference population means (sometimes paired difference means).\nquestions revolving around one, two, paired samples means addressed using t-distribution; therefore called “t-tests” “t-intervals.” considering quantitative variable across 3 groups, method called ANOVA applied.\n, almost research questions can approached using computational methods (e.g., randomization tests bootstrapping) using mathematical models.\ncontinue emphasize importance experimental design making conclusions research claims.\nparticular, recall variability can come different sources (e.g., random sampling vs. random allocation).","code":""},{"path":"inference-one-mean.html","id":"terms-1","chapter":"13 Inference for a single mean","heading":"13.4.2 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"ChapBinom.html","id":"ChapBinom","chapter":"14 Binomial Random Variable","heading":"14 Binomial Random Variable","text":"","code":""},{"path":"ChapBinom.html","id":"objectives-2","chapter":"14 Binomial Random Variable","heading":"14.1 Objectives","text":"Distributions individual values take shapes spreads features setting, thus follow general laws. shapes spreads distributions statistical summaries parameter estimates made aggregates individual observations tend regular predictable, thus law-abiding., specific objectives chapter truly understandthe distinction natural investigator-made distributions, observable conceptual ones.distinction natural investigator-made distributions, observable conceptual ones.automatically associate certain distributions certain types random variableswhy automatically associate certain distributions certain types random variableswhy need understand pre-requisites random variables following distributions dowhy need understand pre-requisites random variables following distributions dowhy rely much Normal distribution, ‘Central’ statistical inference concerning parameters.rely much Normal distribution, ‘Central’ statistical inference concerning parameters.pre-occupation checking ‘Normality’ (Gaussian-ness) misguidedwhy pre-occupation checking ‘Normality’ (Gaussian-ness) misguidedwhy Normality even relevant ‘variable’ ‘random’, appears right hand side regression model.Normality even relevant ‘variable’ ‘random’, appears right hand side regression model.contexts shape matterthe contexts shape matterTo get full list named distributions available R can use following command: help(Distributions)","code":""},{"path":"ChapBinom.html","id":"bernoulli","chapter":"14 Binomial Random Variable","heading":"14.2 Bernoulli","text":"simplest random variable one take just 2 possible values, YES/, MALE/FEMALE, 0/1, /, POSITIVE/NEGATIVE, PRESENT/ABSENT, EXISTS/, etc.random variable \\(Y\\) governed just one parameter, namely probability, \\(\\pi\\), takes YES (‘1’) value. course can reverse scale, speak probability (‘0’) result.bad Wikipedia, unified way showing main features statistical distributions, follow principles show graph various versions Bernoulli distribution. graph.\nFigure 11.2: Various Bernoulli random variables/distributions. continue convention using letter Y (instead X) generic name random variable. Moreover, keeping view, selected Bernoulli distributions plotted 2 possible values shown vertical axis.\nPlease, reading Wikipedia entry, replace instances \\(X\\) \\(x\\) \\(Y\\) \\(y\\). Note also use \\(\\pi\\) Wikipedia, textbooks, use \\(p\\). much can, use Greek letters parameters Roman letters empirical (sample) counterparts. Also, consistent, random variable itslef called \\(Y\\), makes sense use \\(y\\) possible relaizations , rather illogical \\(k\\) Wikipedia uses.]sidebar, Wikipedia shows probability mass function (pmf, probabilities go possible \\(Y\\) values) two separate rows, text pmf also shown concisely, (notation)\\[f(y) = \\pi^y (1-\\pi)^{1-y}, \\ \\ y = 0, 1.\\]\nwish align R software names features distributions, might want switch \\(f\\) \\(d\\). R uses \\(d\\) harmonizes probability \\(d\\)ensity function (pdf) notation uses random variables interval scale, even though statistical ‘purists’ see mixing terminology: use term probablility mass function discrete random variables, probablity density function ones interval scale.\\[d_{Bernoulli}(y) = \\pi^y (1-\\pi)^{1-y}.\\]Sadly, Bernoulli get entry R’s list named distributions, presumably special case binomial distribution, one \\(n\\) = 1.\ncall dbinom(x,size,prob) get \ndensity (probability mass) function binomial distribution parameters size (\\(n\\)) prob (\\(\\pi\\)), set \\(n\\) 1.3 arguments dbinom(x,size,prob) :x: vector quantiles (just 0 /1),size: number ‘trials’ (‘\\(n\\)’, 1 Bernoulli),\nandprob: probability ‘success’ ‘trial’. think probability realization $Y4, .e, \\(y\\) equal 1, \\(\\pi.\\))Thus, dbinom(x=0,size=1,prob=0.3) yields 0.7, dbinom(x=1,size=1,prob=0.3) yields 0.3 dbinom(x=c(0,1),size=1,prob=0.3) yields vector 0.7, 0.3.Incidentally, please adopt convention \\(x\\) (\\(y\\)) number ‘successes’ \\(n\\) trials. number ‘positives’ sample \\(n\\) independent draws population proportion \\(\\pi\\) positive.","code":""},{"path":"ChapBinom.html","id":"expectation-and-variance-of-a-bernoulli-random-variable","chapter":"14 Binomial Random Variable","heading":"14.2.1 Expectation and Variance of a Bernoulli random variable","text":"Shortening \\(Prob(Y=y)\\) \\(P_y\\), haveFrom first principles,\n\\[E[Y] = 0 \\times P_0 + 1 \\times P_1 = 0 \\times (1-\\pi) + 1 \\times \\pi = \\pi,\\]\n\n\\[V[Y] = (0-\\pi)^2 \\times P_0 + (1-\\pi)^2 \\times P_1  = \\pi^2(1-\\pi) + (1-\\pi)^2\\pi =  \\underline{\\pi(1-\\pi)}.\\]functional form (‘unit’) variance entirely surprising: obvious selected distributions whon concentrated Bernoulli distributions ones proportion \\((\\pi)\\) Y = 1 values either close 1 zero, spread Bernoulli distributions ones \\(\\pi\\) close 1/2. , function \\(\\pi\\), Variance must symmetric \\(\\pi\\) = 1/2.fact greatest uncertainty (‘entropy’, lack order predictability) \\(\\pi\\) = 1/2 one factors makes sports contests engaging teams players well matched. Later, come study influences imprecision sample surveys, see given sample size, imprecision largest \\(\\pi\\) closer 1/2.focus variance Bernoulli random variable? , later, use intesting binomial distribution, can call first prionciples recall expectation variance . Binomial random variable sum \\(n\\) independently distributed Bernoulli random variables, expectation \\(\\pi\\) unit variance \\(\\pi(1-\\pi).\\) Thus expectation (\\(E\\)) variance (\\(V\\)) sums ‘unit’ versions, .e., \\(E[binom.sum] = n \\times \\pi\\) \\(V[binom.sum] = n \\times \\pi(1-\\pi).\\) Moreover, first principles, can deduce instead sample sum, interested sample mean (mean 0’s 1’s sample proportion), expected value \n\\[\\boxed{\\boxed{E[binom.prop'n] = \\frac{n \\pi}{n} = \\pi; \\   V[.] = \\frac{n  \\pi(1-\\pi))}{n^2} = \\frac{\\pi(1-\\pi)}{n}; \\ SD[.] = \\frac{\\sqrt{\\pi(1-\\pi)}}{ \\sqrt{n}} = \\frac{\\sigma_{0,1}}{\\sqrt{n}} } }  \\]Note generic way write SD sampling distribution sample proportion, way write SD sampling distribution sample mean, \\(\\sigma_u/\\sqrt{n},\\) \\(\\sigma_u\\) ‘unit’ SD, standard deviation values individuals. individual values case Bernoulli randomn variable just 0s 1s, SD \\(\\sqrt{\\pi(1-\\pi)}.\\) call SD SD 0’1 1’s, \\(\\sigma_{0,1}\\) short.Notice , even though might look nicer simpler compute, involves just 1 square root calculation, write SD binomial proportion \\[SD[binom.proportion] = \\sqrt{\\frac{\\pi(1-\\pi)}{ n} }.\\]\nchoose instead use \\(\\sigma/\\sqrt{n}\\) version, show form SD sampling distribution sample mean. Now longer need savw keystrokes hand caloculator, move away computational forms focus instead intuitive form. Sadly, many textbooks re-use concept disjoint chapters without telling readers cases SD formula.lot gained thinking proportions means, \\(Y\\) values just 0’s 1’s. can use R code simulate large number 0’s 1’s, calculate variance. sd function R doesn’t know care values supply limited just 0s 1s, spread along interval. Better still don’t use rbinom function; instead use sample function, replacement.Try code larger \\(n\\) different \\(\\pi\\) convince variance (thus SD) individual 0 1 values () nothing many everything proportion type (b) larger proportions close , smaller .answer ‘applications, yes.’ reason cases, able use Gaussian (Normal) approximation binomial distribution. Thus, need expectation variance (standard deviation): don’t need dbinom() probability mass function, pbinom() gives cumulative distribution function thus tail areas, qbinom() function gives quantiles. sometimes deal situations binomial distributions symmetric close-enough--Gaussian.recount , 1738, almost 4 decades Gauss born, summing probabilities binomial distribution large \\(n\\), deMoivre effectively used -yet unrecognized ‘Gaussian’ distribution accurate approximation. Without calling , relied standard deviation binomial distribution.","code":"\n\nn = 750\n\nzeros.and.ones = sample(0:1, n , \n   prob=c(0.2, 0.8),replace=TRUE )\n\nm = matrix(zeros.and.ones,n/75,75)\nnoquote(apply(m,1,paste,collapse=\"\"))\n#>  [1] 100111011111100011011111111011111010111011101111111111101001111111111101101\n#>  [2] 011111111111111111111111111101111111011100011011111111111111111101111111110\n#>  [3] 111110111111111110110110111011111011111010111111111110111110111111101101110\n#>  [4] 111000111011111111110111111111111101011111111000010111010111100110111100111\n#>  [5] 111111011111001111111011101111101011011011011110111111111111111111101110011\n#>  [6] 011101101001011100111110101111111111011010111110101000101011111101111011111\n#>  [7] 111001001111101111110111111110001101111111110111111111001111100111011010111\n#>  [8] 111111111101101111111011111011011111101111111111110111111111101101011111111\n#>  [9] 111111111010110111110010110011110101111110110011111111011011111111110111101\n#> [10] 011111100111111011101110011111101111111111101110111111111110011111111111111\n\nsum(zeros.and.ones)/n\n#> [1] 0.7986667\nround( sd(zeros.and.ones),4)\n#> [1] 0.4013"},{"path":"ChapBinom.html","id":"binomial","chapter":"14 Binomial Random Variable","heading":"14.3 Binomial","text":"Binomial Distribution model (sampling) variability proportion count randomly selected sampleThe Binomial Distribution: isThe \\(n+1\\) probabilities \\(p_{0}, p_{1}, ..., p_{y}, ..., p_{n}\\) observing \\(y\\) = \\(0, 1, 2, \\dots , n\\) ‘positives’ \\(n\\) independent realizations Bernoulli random variable \\(Y\\)probability, \\(\\pi,\\) Y=1, (1-\\(\\pi\\)) 0. number sum \\(n\\) independen Bernoulli random variables probability, s.r.s \\(n\\) individuals.\\(n+1\\) probabilities \\(p_{0}, p_{1}, ..., p_{y}, ..., p_{n}\\) observing \\(y\\) = \\(0, 1, 2, \\dots , n\\) ‘positives’ \\(n\\) independent realizations Bernoulli random variable \\(Y\\)probability, \\(\\pi,\\) Y=1, (1-\\(\\pi\\)) 0. number sum \\(n\\) independen Bernoulli random variables probability, s.r.s \\(n\\) individuals.\\(n\\) observed elements binary (0 1)\\(n\\) observed elements binary (0 1)\\(2^{n}\\) possible sequences … \\(n+1\\) possible values, .e. \\(0/n,\\;1/n,\\;\\dots ,\\;n/n\\) can think \\(y\\) sum \\(n\\) Bernoulli r. v.’s. [Later , ptractive, work scale parameter. .e., (0,1). (0,n) ‘count’ scale.]\\(2^{n}\\) possible sequences … \\(n+1\\) possible values, .e. \\(0/n,\\;1/n,\\;\\dots ,\\;n/n\\) can think \\(y\\) sum \\(n\\) Bernoulli r. v.’s. [Later , ptractive, work scale parameter. .e., (0,1). (0,n) ‘count’ scale.]Apart \\(n\\), probabilities \\(p_{0}\\) \\(p_{n}\\) depend 1 parameter:\nprobability selected individual ‘positive’ .e., trait interest\nproportion ‘positive’ individuals sampled population\nApart \\(n\\), probabilities \\(p_{0}\\) \\(p_{n}\\) depend 1 parameter:probability selected individual ‘positive’ .e., trait interestthe proportion ‘positive’ individuals sampled populationUsually denote (un-knowable) proportion \\(\\pi\\) (sometimes generic \\(\\theta\\))\nTextbooks consistent (see ); try use Greek letters parameters,\nNote Miettinen’s use UPPER-CASE letters, [e.g. \\(P\\), \\(M\\)] PARAMETERS lower-case letters [e.g., \\(p\\), \\(m\\)] statistics (estimates} parameters).\nUsually denote (un-knowable) proportion \\(\\pi\\) (sometimes generic \\(\\theta\\))Textbooks consistent (see ); try use Greek letters parameters,Note Miettinen’s use UPPER-CASE letters, [e.g. \\(P\\), \\(M\\)] PARAMETERS lower-case letters [e.g., \\(p\\), \\(m\\)] statistics (estimates} parameters).Shorthand: \\(Y \\sim Binomial(n, \\pi)\\) \\(y \\sim Binomial(n, \\pi)\\)arisesSample SurveysClinical TrialsPilot studiesGeneticsEpidemiologyUsesto make inferences \\(\\pi\\) observed proportion \\(p = y/n\\).make inferences \\(\\pi\\) observed proportion \\(p = y/n\\).make inferences complex situations, e.g. …\nPrevalence Difference: \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\)\nRisk Difference (RD): \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\)\nRisk Ratio, synonym Relative Risk (RR): \\(\\frac{\\pi_{index.category}}{\\pi_{reference.category}}\\)\nOdds Ratio (): \\(\\frac{\\pi_{index.category}/(1-\\pi_{index.category})}{  \\pi_{reference.category}/(1-\\pi_{reference.category}) }\\)\nTrend several \\(\\pi\\)’s\nmake inferences complex situations, e.g. …Prevalence Difference: \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\)Prevalence Difference: \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\)Risk Difference (RD): \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\)Risk Difference (RD): \\(\\pi_{index.category}\\) - \\(\\pi_{reference.category}\\)Risk Ratio, synonym Relative Risk (RR): \\(\\frac{\\pi_{index.category}}{\\pi_{reference.category}}\\)Risk Ratio, synonym Relative Risk (RR): \\(\\frac{\\pi_{index.category}}{\\pi_{reference.category}}\\)Odds Ratio (): \\(\\frac{\\pi_{index.category}/(1-\\pi_{index.category})}{  \\pi_{reference.category}/(1-\\pi_{reference.category}) }\\)Odds Ratio (): \\(\\frac{\\pi_{index.category}/(1-\\pi_{index.category})}{  \\pi_{reference.category}/(1-\\pi_{reference.category}) }\\)Trend several \\(\\pi\\)’sTrend several \\(\\pi\\)’sRequirements \\(Y\\) Binomial\\((n, \\pi)\\) distributionEach element ‘population’ 0 1, interested estimating proportion (\\(\\pi\\)) 1’s; interested individuals.element ‘population’ 0 1, interested estimating proportion (\\(\\pi\\)) 1’s; interested individuals.Fixed sample size \\(n\\).Fixed sample size \\(n\\).Elements selected random independent ; element population probability sampled: .e., \\(n\\) independent Bernoulli random variables expectation (statisticians say ‘..d’ ‘independent identically distributed’).Elements selected random independent ; element population probability sampled: .e., \\(n\\) independent Bernoulli random variables expectation (statisticians say ‘..d’ ‘independent identically distributed’).helps distinguish population values, say \\(Y_1\\) \\(Y_N\\), \\(n\\) sampled values \\(y_1\\) \\(y_n\\).\nDenote \\(y_i\\) value \\(\\)-th sampled element. Prob[\\(y_i\\) = 1] constant (\\(\\pi\\)) across \\(\\).\nproportion time spend indoors? example, random/blind sampling temporal spatial patterns 0s 1s makes \\(y_1\\) \\(y_n\\) independent . \\(Y\\)’s, elements population can related [e.g. can peculiar spatial distribution persons] elements chosen random, chance value \\(\\)-th element chosen 1 depend value \\(y_{−1}\\) \\(y\\): sampling ‘blind’ spatial location 1’s 0s.helps distinguish population values, say \\(Y_1\\) \\(Y_N\\), \\(n\\) sampled values \\(y_1\\) \\(y_n\\).\nDenote \\(y_i\\) value \\(\\)-th sampled element. Prob[\\(y_i\\) = 1] constant (\\(\\pi\\)) across \\(\\).\nproportion time spend indoors? example, random/blind sampling temporal spatial patterns 0s 1s makes \\(y_1\\) \\(y_n\\) independent . \\(Y\\)’s, elements population can related [e.g. can peculiar spatial distribution persons] elements chosen random, chance value \\(\\)-th element chosen 1 depend value \\(y_{−1}\\) \\(y\\): sampling ‘blind’ spatial location 1’s 0s.","code":""},{"path":"ChapBinom.html","id":"binomial-probabilities-illustrated-using-a-binomial-tree","chapter":"14 Binomial Random Variable","heading":"14.3.1 Binomial probabilities, illustrated using a Binomial Tree","text":"\nFigure 14.1: 5 (independent identically distributed) Bernoulli observations Binomial(n=5), Bernoulli probability left unspecified. 2 power n possible (distinct) sequences 0’s 1’s, probability. interested 2 power n probabilities, probability sample contains y 1’s (n-y) 0’s. (n+1) possibilities y, namely 0 n. Fortunately, n.choose.y sequences lead sum count y, probability. group 2..power.n sequences (n+1) sets, according sum count. sequence set y 1’s (n-y) 0’s probability, namely prob...power.y times (1-prob)...power.(n-y). Thus, lieu adding probabilities, simply multiply probability number, n.choose-y – shown black – unique sequences set. Check: frequencies black add 2..power.n. Nowadays, (n+1) probabilities easily obtained supplying value ‘prob’ argument R function dbinom(), instead computing binomial coefficient n.choose-y hand.\nrotate binomial tree right 90 degrees, use imagination, can see resembles quincunx constructed Francis Galton. used show Central Linit Theorem, applied sum several ‘Bernoulli deflections right left’, makes Binomial distribution approach Gaussian one. Several games game shows built pinball machine, example, Plinko , recently, Wall.\nGalton’s quincunx cottage industry, versions often displayed Science Museums. present authors inherited low tech version Galton Board, ‘shot’ turnip seeds, former McGill Professor – early teacher course 607 – FDK Liddell.","code":""},{"path":"ChapBinom.html","id":"calculating-binomial-probabilities","chapter":"14 Binomial Random Variable","heading":"14.3.2 Calculating Binomial probabilities","text":"Exactlyprobability mass function (p.m.f.) :\nformula: \\(Prob[y] = \\  ^n C _y \\ \\pi^y \\  (1 − \\pi)^{n−y}\\).\nrecursively: \\(Prob[0] = (1−\\pi)^n\\);    \\(Prob[y] = \\frac{n−y+1}{y} \\times \\frac{\\pi}{1-\\pi} \\times Prob[y−1]\\).\nprobability mass function (p.m.f.) :formula: \\(Prob[y] = \\  ^n C _y \\ \\pi^y \\  (1 − \\pi)^{n−y}\\).formula: \\(Prob[y] = \\  ^n C _y \\ \\pi^y \\  (1 − \\pi)^{n−y}\\).recursively: \\(Prob[0] = (1−\\pi)^n\\);    \\(Prob[y] = \\frac{n−y+1}{y} \\times \\frac{\\pi}{1-\\pi} \\times Prob[y−1]\\).recursively: \\(Prob[0] = (1−\\pi)^n\\);    \\(Prob[y] = \\frac{n−y+1}{y} \\times \\frac{\\pi}{1-\\pi} \\times Prob[y−1]\\).Statistical Packages:\nR functions dbinom(), pbinom(), qbinom()\nprobability mass, distribution/cdf, quantile functions.\nStata function Binomial(n,k,p)\nSAS PROBBNML(p, n, y) function\nStatistical Packages:R functions dbinom(), pbinom(), qbinom()\nprobability mass, distribution/cdf, quantile functions.R functions dbinom(), pbinom(), qbinom()\nprobability mass, distribution/cdf, quantile functions.Stata function Binomial(n,k,p)Stata function Binomial(n,k,p)SAS PROBBNML(p, n, y) functionSAS PROBBNML(p, n, y) functionSpreadsheet — Excel function BINOMDIST(y, n, π, cumulative)Spreadsheet — Excel function BINOMDIST(y, n, π, cumulative)Tables: CRC; Fisher Yates; Biometrika Tables; Documenta GeigyTables: CRC; Fisher Yates; Biometrika Tables; Documenta GeigyUsing approximationPoisson Distribution (\\(n\\) large; small \\(\\pi\\))Poisson Distribution (\\(n\\) large; small \\(\\pi\\))Normal (Gaussian) Distribution (\\(n\\) large midrange \\(\\pi\\), expected value, \\(n \\times \\pi\\), sufficiently far ‘’ ‘edges’ scale, .e., sufficiently far 0 \\(n\\), Gaussian distribution doesn’t flow past one edges. Normal approximation good don’t access software Tables, e.g, plane, internet , battery phone laptop run , takes long boot Windows!).\nuse Normal approximatiom, aware scale working , .e.g., say \\(n = 10\\), whether summary count proportion percentage.Normal (Gaussian) Distribution (\\(n\\) large midrange \\(\\pi\\), expected value, \\(n \\times \\pi\\), sufficiently far ‘’ ‘edges’ scale, .e., sufficiently far 0 \\(n\\), Gaussian distribution doesn’t flow past one edges. Normal approximation good don’t access software Tables, e.g, plane, internet , battery phone laptop run , takes long boot Windows!).\nuse Normal approximatiom, aware scale working , .e.g., say \\(n = 10\\), whether summary count proportion percentage.first person suggest approximation, using now call ‘Normal’ ‘Gaussian’ ‘Laplace-Gaussian’ distribution, \ndeMoivre, 1738. debate among historians whether marks first description Normal distribution: piece explicitly point probability density function \\(\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\times exp[-z^2/2\\sigma^2],\\) highlight role quantity \\((1/2) \\times \\sqrt{n}\\), standard deviation sum \\(n\\) independent Bernoulli random variables, expectation 1/2 thus ‘unit’ standard deviation 1/2, also SD quantity \\(\\sqrt{\\pi(1-\\pi)}\\) \\(\\times\\) \\(\\sqrt{n}\\) general case. DeMoivre arrived familiar ‘68-95-99.7 rule’ : percentages normal distribution lie within 1, 2 3 SD’s mean.Factors modulate shapes Binomial distributionssize \\(n\\): larger n, symmetricsize \\(n\\): larger n, symmetricvalue \\(\\pi\\): closer 1/2, symmetricvalue \\(\\pi\\): closer 1/2, symmetricIn small-\\(n\\) contexts, distribtions \\(\\pi\\) close 0.5 reasonably symmetric.larger-\\(n\\) contexts (see ), long ‘room’ , binomial distribtions expected value \\(E = n \\times \\pi\\) least 5-10 ‘edges’ (.e. right 0, left \\(n\\), reasonably symmetric.\nFigure 14.2: Binomial random variables/distributions, n = 5, Bernoulli expectation (probability) smaller (left panels) larger (right panels).\n\nFigure 8.5: Various Binomial random variables/distributions, n = 20. dotted horizontal lines light blue 5 10 units (0,n) boundaries. distributions expected value E mean, mu ( = n * Bernoulli Probability) least 5 units (0,n) boundaries shown blue.\n\nFigure 8.6: Various Binomial random variables/distributions, n = 50. blue dotted lines 5 10 units (0,n) boundaries. distributions expected value E mean, mu ( = n * Bernoulli Probability) least 5 units (0,n) boundaries shown blue\nBack binomial computations difficult, normal approximation accurate, another approximation saved labour, particular, avoided deal large numbers involved \\(^nC_y\\) binomial coefficients (even built hand calculator, can problematic \\(n\\) large).Poisson, 1837, shown use Normal distribution binomial (closely-related negative binomial) calculations, devoted 1 small section (81) less 2 pages, case (notation) one two chances \\(\\pi\\) \\((1-\\pi)\\) ‘est très petite’ [Poisson’s \\(q\\) \\(\\pi\\), \\(\\mu\\) \\(n\\), \\(\\omega\\) \\(\\mu\\), \\(n\\) \\(y\\)]. notation, arrived \n\\[Prob[y \\ \\ fewer \\ events] = \\bigg(1 + \\frac{\\mu}{1!} + \\frac{\\mu^2}{2!} + \\dots + \\frac{\\mu^y}{y!} \\bigg) \\  e^{-\\mu}.\\]last just three paragraphs digression, notes probability \\(e^{-\\mu}\\) event, \\(1 - e^{-\\mu}\\) least 1.\nalso calculates , \\(\\mu\\) = 1, chance close 1 100 million \\(y\\) = 10 events occur large series trials (length \\(n\\)), probability 1/\\(n\\) trial. Although give little emphasis formula, real application, Poisson’s name now undividely associated formula.examples well approximates Binomial ‘corner’ \\(\\pi\\) small \\(n\\) large. , course, product, \\(\\mu = n \\times \\pi\\) , reaches double digits, Normal approximation distribution provides accurate approximation Binomial Poisson distributions, – one ready acccess tail areas Normal distribution – less effort. Today, course, unless limited hand calculator internet R paper tables available, need use approximation, Normal Poisson, binomial.\nFigure 14.3: Various Binomial random variables/distributions, large n’s small Bernoulli probabilities, together Poisson distributions corresponding means. Poisson distrubution provides good approximation ’lowee corner Bimonial distribution large n’s small probabilities. , product, mu = n * probability, double digits, Normal approximation distribution provides accurate approximation Binomial Poisson distributions.\n","code":""},{"path":"ChapBinom.html","id":"when-the-binomial-does-not-apply","chapter":"14 Binomial Random Variable","heading":"14.4 When the Binomial does not apply","text":"apply one () ‘..d.’ (independent identical) conditions hold. first conditions often one absent.Two nice examples can found Cochran’s (old still excellent) textbook sampling. re-visited decades years ago connection () new technique dealing \ncorrelated responses.table shows, 30 randomly sampled households, number household members visited physician previous year. Can base precision observed proportion, 30/104 28%, binomial distribution \\(n\\) = 104?True, ‘n’ household household household, can segregate households size, carry separate binomial calculations , time assuming common binomial proportion across houdeholds.\nFigure 14.4: 30 randomly sampled households, (O)bserved numbers households, shown grey, 0, 1, .. household visited physician previous year. 30 households contained 104 persons, 28 visited physician previous year. Also shown, blue (E)xpected numbers households, assuming data generated 104 independent Bernoulli random variables, probability 28/104. observed variance considerably LARGER predicted binomial distribution. see even individuals house family. example, occupants students, proportion history different (?lower) occupants older: ‘non-identical probabilities’ aspect. possibility, ‘non-independence’ aspect, health status seeking medical care affected shared family factors, behaviours, attitudes, lifestyle, insurance coverage.\nCochran provides explanation greater binomial variation proportionsThe variance given ratio method, 0.00520, much larger given binomial formula, 0.00197. various reasons, families differ frequency members consult doctor. sample whole, proportion consult doctor little one four, several families every member seen doctor. Similar results obtained characteristic members family tend act way.Clearly, dealing infectious disease, need careful statistical model numbers family household care facility affected.\nFigure 8.9: 30 randomly sampled households, (O)bserved numbers households, shown grey, 0, 1, .. household male. 30 households contained 104 persons, 53 male. Also shown, blue (E)xpected numbers households, assuming data generated 104 independent Bernoulli random variables, probability 53/104. observed variance considerably SMALLER predicted binomial distribution. see even individuals house family. example, occupants students, proportion history different (?lower) occupants older: ‘non-identical probabilities’ aspect. possibility, ‘non-independence’ aspect, health status seeking medical care affected shared family factors, behaviours, attitudes, lifestyle, insurance coverage.\nCochran provides explanation (unsually) less binomial variation seen response variable.estimating proportion males population, results different. type calculation, find binomial formula: v(p) = 0.00240; ratio formula v(p) = 0.00114\nbinomial formula overestimates variance. reason interesting. households set result marriage, hence contain least one male one female. Consequently proportion males per family varies less one half expected binomial formula. None 30 families, except one one member, composed entirely males, entirely females. binomial distribution applicable, true P approximately one half, households members sex constitute one quarter households size 3 one eighth households size 4. property sex ratio discussed Hansen Hurwitz (1942). illustrations error committed improper use binomial formula sociological investigations given Kish (1957).Binomial Distribution apply … ?","code":"\n\nXLAB = \"y: number in household who has visited an MD in last year\"\nYLAB = \"Number of Households\\nfor which y persons visited an MD\"\n\nshow.household.survey.data(n.who.visited.md,XLAB,YLAB)\n#> [1] No. persons with history/trait of interest: 30 / 104 ; Prop'n: 0.29\n#> [1]  \n#> Below, the row labels are the household sizes.\n#> Each column label is the number in a household with history/trait of interest.\n#> The entries are how many households had the indicated configuration.\n#> \n#>           0    1    2    3    4    5    6    7    Total\n#> 1         1    .    .    .    .    .    .    .        1\n#> 2         3    .    1    .    .    .    .    .        4\n#> 3         7    2    1    2    .    .    .    .       12\n#> 4         4    1    3    .    1    .    .    .        9\n#> 5         .    .    1    .    .    1    .    .        2\n#> 6         1    .    .    .    .    .    .    .        1\n#> 7         1    .    .    .    .    .    .    .        1\n#> Total    17    3    6    2    1    1    0    0       30\n#> \n#> Same row/col meanings, but theoretical, binomial-based,\n#> EXPECTED frequencies of households having these many persons\n#>  who would have -- if all 128 persons were independently sampled\n#> from a population where the proportion with \n#> the history/trait of interest was as above.\n#> \n#>           0     1     2     3     4     5     6     7 Total\n#> 1     0.712 0.288     .     .     .     .     .     .     1\n#> 2     1.013 0.821 0.166     .     .     .     .     .     4\n#> 3     1.081 1.314 0.533 0.072     .     .     .     .    12\n#> 4     1.025 1.663 1.011 0.273 0.028     .     .     .     9\n#> 5     0.912 1.849 1.499 0.608 0.123 0.010     .     .     2\n#> 6     0.779 1.894 1.920 1.038 0.315 0.051 0.003     .     1\n#> 7     0.646 1.834 2.231 1.507 0.611 0.149 0.020 0.001     1\n#> Total 6.167 9.663 7.360 3.498 1.077 0.210 0.024 0.001    30\n#> \n#> We will just assess the fit on the 'totals'row:\n#>  the numbers in the individual rows are too small to judge.\n#> \n#>         0   1   2   3   4   5 6 7 Total\n#> [1,] 17.0 3.0 6.0 2.0 1.0 1.0 0 0    30\n#> [2,]  6.2 9.7 7.4 3.5 1.1 0.2 0 0    30\n#> \n#> Better still, here is a picture:\n\n\nXLAB = \"y: number of males in household\"\nYLAB=  \"Number of Households\\n where y persons were male\"\nshow.household.survey.data(n.males,XLAB,YLAB)\n#> [1] No. persons with history/trait of interest: 53 / 104 ; Prop'n: 0.51\n#> [1]  \n#> Below, the row labels are the household sizes.\n#> Each column label is the number in a household with history/trait of interest.\n#> The entries are how many households had the indicated configuration.\n#> \n#>          0     1    2    3    4    5    6    7    Total\n#> 1        1     .    .    .    .    .    .    .        1\n#> 2        .     4    .    .    .    .    .    .        4\n#> 3        .     7    5    .    .    .    .    .       12\n#> 4        .     1    3    5    .    .    .    .        9\n#> 5        .     1    .    1    .    .    .    .        2\n#> 6        .     .    .    1    .    .    .    .        1\n#> 7        .     .    .    1    .    .    .    .        1\n#> Total    1    13    8    8    0    0    0    0       30\n#> \n#> Same row/col meanings, but theoretical, binomial-based,\n#> EXPECTED frequencies of households having these many persons\n#>  who would have -- if all 128 persons were independently sampled\n#> from a population where the proportion with \n#> the history/trait of interest was as above.\n#> \n#>           0     1     2     3     4     5     6     7 Total\n#> 1     0.490 0.510     .     .     .     .     .     .     1\n#> 2     0.481 1.000 0.519     .     .     .     .     .     4\n#> 3     0.354 1.103 1.146 0.397     .     .     .     .    12\n#> 4     0.231 0.962 1.499 1.038 0.270     .     .     .     9\n#> 5     0.142 0.737 1.531 1.591 0.827 0.172     .     .     2\n#> 6     0.083 0.520 1.352 1.873 1.460 0.607 0.105     .     1\n#> 7     0.048 0.347 1.083 1.875 1.949 1.215 0.421 0.062     1\n#> Total 1.829 5.178 7.130 6.775 4.505 1.994 0.526 0.062    30\n#> \n#> We will just assess the fit on the 'totals'row:\n#>  the numbers in the individual rows are too small to judge.\n#> \n#>        0    1   2   3   4 5   6   7 Total\n#> [1,] 1.0 13.0 8.0 8.0 0.0 0 0.0 0.0    30\n#> [2,] 1.8  5.2 7.1 6.8 4.5 2 0.5 0.1    30\n#> \n#> Better still, here is a picture:"},{"path":"ChapBinom.html","id":"more-on-the-approximation-of-the-binomial-distribution","chapter":"14 Binomial Random Variable","heading":"14.5 More on the Approximation of the Binomial Distribution","text":"Normal distribution emerges frequently approximation \ndistribution data characteristics. probability theory \nmathematically establishes approximation called Central\nLimit Theorem. section \ndemonstrate Normal approximation context Binomial\ndistribution.","code":""},{"path":"ChapBinom.html","id":"approximate-binomial-probabilities-and-percentiles-1","chapter":"14 Binomial Random Variable","heading":"14.5.1 Approximate Binomial Probabilities and Percentiles","text":"Consider, example, probability obtaining 1940 \n2060 heads tossing 4,000 fair coins. Let \\(Y\\) total number \nheads. tossing coin trial two possible outcomes:\n“Head” “Tail.” probability “Head” 0.5 \n4,000 trials. Let us call obtaining “Head” trial “Success”.\nObserve random variable \\(Y\\) counts total number \nsuccesses. Hence, \\(Y \\sim \\mathrm{Binomial}(4000,0.5)\\).probability \\(\\operatorname{P}(1940 \\leq Y \\leq 2060)\\) can computed \ndifference probability \\(\\operatorname{P}(Y \\leq 2060)\\) less \nequal 2060 probability \\(\\operatorname{P}(Y < 1940)\\) strictly\nless 1940. However, 1939 largest integer still\nstrictly less integer 1940. result get \n\\(\\operatorname{P}(Y < 1940) = \\operatorname{P}(Y \\leq 1939)\\). Consequently,\n\\(\\operatorname{P}(1940 \\leq Y \\leq 2060) = \\operatorname{P}(Y \\leq 2060) - \\operatorname{P}(Y \\leq 1939)\\).Applying function “pbinom” computation Binomial\ncumulative probability, namely probability less equal \ngiven value, get probability range 1940 \n2060 equal toThis exact computation. Normal approximation produces \napproximate evaluation, exact computation. Normal\napproximation replaces Binomial computations computations carried \nNormal distribution. computation probability \nBinomial random variable replaced computation probability \nNormal random variable expectation standard\ndeviation Binomial random variable.Notice \\(Y \\sim \\mathrm{Binomial}(4000,0.5)\\) expectation\n\\(\\operatorname{E}(Y) = 4,000 \\times 0.5 = 2,000\\) variance \n\\(\\operatorname{Var}(Y) = 4,000 \\times 0.5 \\times 0.5 = 1,000\\), standard\ndeviation square root variance. Repeating \ncomputation conducted Binomial random variable, \ntime function “pnorm” used computation \nNormal cumulative probability, get:Observe example Normal approximation probability\n(0.9442441) agrees Binomial computation probability\n(0.9442883) 3 significant digits.Normal computations may also applied order find approximate\npercentiles Binomial distribution. example, let us identify\ncentral region contains \\(\\mathrm{Binomial}(4000,0.5)\\)\nrandom variable (approximately) 95% distribution. Towards \nend can identify boundaries region Normal\ndistribution expectation standard deviation \ntarget Binomial distribution:rounding nearest integer get interval \\([1938,2062]\\)\nproposed central region.order validate proposed region may repeat computation\nactual Binomial distribution:, get interval \\([1938,2062]\\) central region, \nagreement one proposed Normal approximation. Notice \nfunction “qbinom” produces percentiles Binomial\ndistribution.ability approximate one distribution , \ncomputation tools distributions handy, seems \nquestionable importance. Indeed, significance Normal\napproximation much ability approximate Binomial\ndistribution . Rather, important point Normal\ndistribution may serve approximation wide class \ndistributions, Binomial distribution one example.\nComputations based Normal approximation valid\nmembers class distributions, including cases \ndon’t computational tools disposal even cases\nknow exact distribution member (aka CLT).hand, one need assume distribution well\napproximated Normal distribution. example, distribution\nwealth population tends skewed, 50% \npeople possessing less 50% wealth small percentage\npeople possessing majority wealth. Normal\ndistribution good model distribution. Exponential\ndistribution, distributions similar , may appropriate.","code":"\npbinom(2060,4000,0.5) - pbinom(1939,4000,0.5)\n#> [1] 0.9442883\nmu <- 4000*0.5\nsig <- sqrt(4000*0.5*0.5)\npnorm(2060,mu,sig) - pnorm(1939,mu,sig)\n#> [1] 0.9442441\nqnorm(0.975,mu,sig)\n#> [1] 2061.98\nqnorm(0.025,mu,sig)\n#> [1] 1938.02\nqbinom(0.975,4000,0.5)\n#> [1] 2062\nqbinom(0.025,4000,0.5)\n#> [1] 1938"},{"path":"ChapBinom.html","id":"continuity-corrections-1","chapter":"14 Binomial Random Variable","heading":"14.5.2 Continuity Corrections","text":"order complete section let us look carefully \nNormal approximations Binomial distribution.\nFigure 9.7: Normal Approximation Binomial Distribution\nprinciple, Normal approximation valid \\(n\\), number \nindependent trials Binomial distribution, large. \\(n\\) \nrelatively small approximation may good. Indeed, take\n\\(Y \\sim \\mathrm{Binomial}(30,0.3)\\) consider probability\n\\(\\operatorname{P}(Y \\leq 6)\\). Compare actual probability Normal\napproximation:Normal approximation, equal 0.1159989, close\nactual probability, equal 0.1595230.naïve application Normal approximation \n\\(\\mathrm{Binomial}(n,p)\\) distribution may good number\ntrials \\(n\\) small. Yet, small modification approximation\nmay produce much better results. order explain modification\nconsult Figure 9.7 find bar plot \nBinomial distribution density approximating Normal\ndistribution superimposed top . target probability \nsum heights bars painted red. naïve\napplication Normal approximation used area \nnormal density left bar associated value\n\\(y=6\\).Alternatively, may associate bar located \\(y\\) area\nnormal density interval \\([y-0.5, y+0.5]\\). \nresulting correction approximation use Normal\nprobability event \\(\\{Y \\leq 6.5\\}\\), area shaded \nred. application approximation, called\ncontinuity correction produces:Observe corrected approximation much closer target\nprobability, 0.1595230, substantially better \nuncorrected approximation 0.1159989. Generally, \nrecommended apply continuity correction Normal\napproximation discrete distribution.Consider \\(\\mathrm{Binomial}(n,p)\\) distribution. Another situation\nNormal approximation may fail \\(p\\), probability \n“Success” Binomial distribution, close 0 (close\n1).","code":"\npbinom(6,30,0.3)\n#> [1] 0.159523\npnorm(6,30*0.3,sqrt(30*0.3*0.7))\n#> [1] 0.1159989\npnorm(6.5,30*0.3,sqrt(30*0.3*0.7))\n#> [1] 0.1596193"},{"path":"ChapBinom.html","id":"sec:RVarExercises","chapter":"14 Binomial Random Variable","heading":"14.6 Exercises","text":"particular measles vaccine produces reaction (\nfever higher 102 Fahrenheit) vaccinee probability \n0.09. clinic vaccinates 500 people day.expected number people develop reaction\nday?expected number people develop reaction\nday?standard deviation number people \ndevelop reaction day?standard deviation number people \ndevelop reaction day?given day, probability 40 people\ndevelop reaction?given day, probability 40 people\ndevelop reaction?given day, probability number people\ndevelop reaction 50 45 (inclusive)?given day, probability number people\ndevelop reaction 50 45 (inclusive)?\nFigure 14.5: Bar Plots Negative-Binomial Distribution\n","code":""},{"path":"ChapBinom.html","id":"summary-6","chapter":"14 Binomial Random Variable","heading":"14.7 Summary","text":"","code":""},{"path":"ChapBinom.html","id":"glossary","chapter":"14 Binomial Random Variable","heading":"Glossary","text":"Binomial Random Variable:number successes among \\(n\\) repeats independent trials \nprobability \\(p\\) success trial. distribution \nmarked \\(\\mathrm{Binomial}(n,p)\\).\nnumber successes among \\(n\\) repeats independent trials \nprobability \\(p\\) success trial. distribution \nmarked \\(\\mathrm{Binomial}(n,p)\\).Density:Histogram describes distribution continuous random\nvariable. area curve corresponds probability.\nHistogram describes distribution continuous random\nvariable. area curve corresponds probability.","code":""},{"path":"ChapBinom.html","id":"summary-of-formulas","chapter":"14 Binomial Random Variable","heading":"Summary of Formulas","text":"Discrete Random Variable:\n\\[\\begin{aligned}\n        \\operatorname{E}(Y) &= \\sum_y \\big(y \\times \\operatorname{P}(y)\\big) \\\\\n        \\operatorname{Var}(Y) &= \\sum_y\\big( (y-\\operatorname{E}(Y))^2 \\times \\operatorname{P}(y)\\big) \\end{aligned}\\]Binomial:\\(\\operatorname{E}(Y) = n p \\;, \\quad \\operatorname{Var}(Y) = n p(1-p)\\)\n\\(\\operatorname{E}(Y) = n p \\;, \\quad \\operatorname{Var}(Y) = n p(1-p)\\)","code":""},{"path":"inference-one-prop.html","id":"inference-one-prop","chapter":"15 Inference for a single proportion","heading":"15 Inference for a single proportion","text":"section adapted book Introduction Modern Statistics89There one variable measured study focuses one proportion.\nobservational unit, single variable measured either event non-event (e.g., “surgical complication” vs. “surgical complication”).\nnature research question hand focuses single variable, way randomize variable across different (explanatory) variable.\nreason, use randomization analysis tool focusing single proportion.\nInstead, apply bootstrapping techniques test given hypothesis, also revisit associated mathematical models.","code":""},{"path":"inference-one-prop.html","id":"one-prop-null-boot","chapter":"15 Inference for a single proportion","heading":"15.1 Bootstrap test for a proportion","text":"testing hypothesized value \\(p\\) (referred \\(p_0),\\) using bootstrap.","code":""},{"path":"inference-one-prop.html","id":"observed-data-3","chapter":"15 Inference for a single proportion","heading":"15.1.1 Observed data","text":"People providing organ donation sometimes seek help special “medical consultant”.\nconsultants assist patient aspects surgery, goal reducing possibility complications medical procedure recovery.\nPatients might choose consultant based part historical complication rate consultant’s clients.\nOne consultant tried attract patients noting average complication rate liver donor surgeries US 10%, clients 3 complications 62 liver donor surgeries facilitated.\nclaims strong evidence work meaningfully contributes reducing complications (therefore hired!).Using data, possible assess consultant’s claim complication rate less 10%?.\nclaim causal connection, data observational.\nPatients hire medical consultant may lower complication rates reasons.possible assess causal claim, still possible test association using data.\nquestion ask, low complication rate \\(\\hat{p} = 0.048\\) simply occurred chance, complication rate differ US standard rate?Write hypotheses plain statistical language test association consultant’s work true complication rate, \\(p,\\) consultant’s clients.90Because, turns , conditions working normal distribution met, uncertainty associated sample proportion modeled using normal distribution, underestimate uncertainty associated sample statistic.\nHowever, still like assess hypotheses previous Guided Practice absence normal framework.\n, need evaluate possibility sample value \\((\\hat{p})\\) far null value, \\(p_0 = 0.10\\) observed.\ndeviation sample value hypothesized parameter usually quantified p-value.p-value computed based null distribution, distribution test statistic null hypothesis true.\nSupposing null hypothesis true, can compute p-value identifying probability observing test statistic favors alternative hypothesis least strongly observed test statistic.\nuse bootstrap simulation calculate p-value.","code":""},{"path":"inference-one-prop.html","id":"variability-of-the-statistic-3","chapter":"15 Inference for a single proportion","heading":"15.1.2 Variability of the statistic","text":"want identify sampling distribution test statistic \\((\\hat{p})\\) null hypothesis true.\nwords, want see variability can expect sample proportions null hypothesis true.\nplan use information decide whether enough evidence reject null hypothesis.null hypothesis, 10% liver donors complications surgery.\nSuppose rate really different consultant’s clients (consultant’s clients, just 62 previously measured).\ncase, simulate 62 clients get sample proportion complication rate null distribution.\nSimulating observations using hypothesized null parameter value often called parametric bootstrap simulation.Similar previous applications bootstrap, client can simulated using bag marbles 10% red marbles 90% white marbles.\nSampling marble bag (10% red marbles) one way simulating whether patient complication true complication rate 10%.\nselect 62 marbles compute proportion patients complications simulation, \\(\\hat{p}_{sim1},\\) resulting sample proportion sample null distribution.5 simulated cases complication 57 simulated cases without complication, .e., \\(\\hat{p}_{sim1} = 5/62 = 0.081.\\)one simulation enough determine whether reject null hypothesis?.\nassess hypotheses, need see distribution many values \\(\\hat{p}_{sim},\\) just single draw sampling distribution.","code":""},{"path":"inference-one-prop.html","id":"observed-statistic-vs.-null-statistics","chapter":"15 Inference for a single proportion","heading":"15.1.3 Observed statistic vs. null statistics","text":"One simulation isn’t enough get sense null distribution; many simulation studies needed.\nRoughly 10,000 seems sufficient.\nHowever, paying someone simulate 10,000 studies hand waste time money.\nInstead, simulations typically programmed computer, much efficient.Figure 15.1 shows results 10,000 simulated studies.\nproportions equal less \\(\\hat{p} = 0.048\\) shaded.\nshaded areas represent sample proportions null distribution provide least much evidence \\(\\hat{p}\\) favoring alternative hypothesis.\n420 simulated sample proportions \\(\\hat{p}_{sim} \\leq 0.048.\\) use construct null distribution’s left-tail area find p-value:\\[\\text{left tail area} = \\frac{\\text{Number observed simulations }\\hat{p}_{sim} \\leq \\text{ 0.048}}{10000}\\]10,000 simulated \\(\\hat{p}_{sim},\\) 420 equal smaller \\(\\hat{p}.\\) Since hypothesis test one-sided, estimated p-value equal tail area: 0.042.\nFigure 15.1: null distribution \\(\\hat{p},\\) created 10,000 simulated studies. left tail, representing p-value hypothesis test, contains 4.2% simulations.\nestimated p-value 0.042, smaller significance level 0.05, reject null hypothesis.\nExplain means plain language context problem.91Does conclusion previous Guided Practice imply consultant good job?\nExplain.92Null distribution \\(\\hat{p}\\) bootstrap simulation.Regardless statistical method chosen, p-value always derived analyzing null distribution test statistic.\nnormal model poorly approximates null distribution \\(\\hat{p}\\) success-failure condition satisfied.\nsubstitute, can generate null distribution using simulated sample proportions use distribution compute tail area, .e., p-value.previous Guided Practice, p-value estimated.\nexact simulated null distribution close approximation sampling distribution sample statistic.\nexact p-value can generated using binomial distribution, method covered text.","code":""},{"path":"inference-one-prop.html","id":"one-prop-norm","chapter":"15 Inference for a single proportion","heading":"15.2 Mathematical model for a proportion","text":"","code":""},{"path":"inference-one-prop.html","id":"conditions","chapter":"15 Inference for a single proportion","heading":"15.2.1 Conditions","text":"previously introduced normal distribution showed can used mathematical model describe variability statistic.\nconditions sample proportion \\(\\hat{p}\\) well modeled using normal distribution.\nsample observations independent sample size sufficiently large, normal model describe sampling distribution sample proportion quite well; observations violate conditions, normal model can inaccurate.\nParticularly, can underestimate variability sample proportion.Sampling distribution \\(\\hat{p}.\\)sampling distribution \\(\\hat{p}\\) based sample size \\(n\\) population true proportion \\(p\\) nearly normal :sample’s observations independent, e.g., simple random sample.expected see least 10 successes 10 failures sample, .e., \\(np\\geq10\\) \\(n(1-p)\\geq10.\\) called success-failure condition.conditions met, sampling distribution \\(\\hat{p}\\) nearly normal mean \\(p\\) standard error \\(\\hat{p}\\) \\(SE = \\sqrt{\\frac{\\ \\hat{p}(1-\\hat{p})\\ }{n}}.\\)Recall margin error defined standard error.\nmargin error \\(\\hat{p}\\) can directly obtained \\(SE(\\hat{p}).\\)Margin error \\(\\hat{p}.\\)margin error \\(z^\\star \\times \\sqrt{\\frac{\\ \\hat{p}(1-\\hat{p})\\ }{n}}\\) \\(z^\\star\\) calculated specified percentile normal distribution. Typically don’t know true proportion \\(p,\\) substitute value check conditions estimate standard error.\nconfidence intervals, sample proportion \\(\\hat{p}\\) used check success-failure condition compute standard error.\nhypothesis tests, typically null value – , proportion claimed null hypothesis – used place \\(p.\\)independence condition nuanced requirement.\nisn’t met, important understand violated.\nexample, exist statistical methods available truly correct inherent biases data convenience sample.\nhand, took cluster sample, observations wouldn’t independent, suitable statistical methods available analyzing data (beyond scope even second third courses statistics).examples based large sample theory, modeled \\(\\hat{p}\\) using normal distribution.\nappropriate case study medical consultant?independence assumption may reasonable surgeries different surgical team.\nHowever, success-failure condition satisfied.\nnull hypothesis, anticipate seeing \\(62 \\times 0.10 = 6.2\\) complications, 10 required normal approximation.book scoped well-constrained statistical problems, remember just first book large library statistical methods suitable wide range data contexts.","code":""},{"path":"inference-one-prop.html","id":"confidence-interval-for-a-proportion","chapter":"15 Inference for a single proportion","heading":"15.2.2 Confidence interval for a proportion","text":"confidence interval provides range plausible values parameter \\(p,\\) \\(\\hat{p}\\) can modeled using normal distribution, confidence interval \\(p\\) takes form \\(\\hat{p} \\pm z^{\\star} \\times SE.\\) seen \\(\\hat{p}\\) sample proportion.\nvalue \\(z^{\\star}\\) determines confidence level (previously set 1.96) discussed detail examples following.\nvalue standard error, \\(SE,\\) depends heavily sample size.Standard error one proportion, \\(\\hat{p}.\\)conditions met distribution \\(\\hat{p}\\) nearly normal, variability single proportion, \\(\\hat{p}\\) well described :\\[SE(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\]Note almost never know true value \\(p.\\) helpful formula use :\\[SE(\\hat{p}) \\approx \\sqrt{\\frac{(\\mbox{best guess }p)(1 - \\mbox{best guess }p)}{n}}\\]hypothesis testing, often use \\(p_0\\) best guess \\(p.\\) confidence intervals, typically use \\(\\hat{p}\\) best guess \\(p.\\)Consider taking many polls registered voters (.e., random samples) size 300 asking support legalized marijuana.\nsuspected 2/3 voters support legalized marijuana.\nunderstand sample proportion \\((\\hat{p})\\) vary across samples, calculate standard error \\(\\hat{p}.\\)93","code":""},{"path":"inference-one-prop.html","id":"variability-of-the-sample-proportion","chapter":"15 Inference for a single proportion","heading":"15.2.3 Variability of the sample proportion","text":"simple random sample 826 payday loan borrowers surveyed better understand interests around regulation costs.\n70% responses supported new regulations payday lenders.reasonable model variability \\(\\hat{p}\\) sample sample using normal distribution?reasonable model variability \\(\\hat{p}\\) sample sample using normal distribution?Estimate standard error \\(\\hat{p}.\\)Estimate standard error \\(\\hat{p}.\\)Construct 95% confidence interval \\(p,\\) proportion payday borrowers support increased regulation payday lenders.Construct 95% confidence interval \\(p,\\) proportion payday borrowers support increased regulation payday lenders.data random sample, reasonable assume observations independent representative population interest.also must check success-failure condition, using \\(\\hat{p}\\) place \\(p\\) computing confidence interval:\\[\n\\begin{aligned}\n  \\text{Support: }\n      n p &\n          \\approx 826 \\times 0.70\n      = 578\\\\\n  \\text{: }\n      n (1 - p) &\n        \\approx 826 \\times (1 - 0.70)\n      = 248\n\\end{aligned}\n\\]Since values least 10, can use normal distribution model \\(\\hat{p}.\\)\\(p\\) unknown standard error confidence interval, use \\(\\hat{p}\\) place \\(p\\) formula.\\[SE = \\sqrt{\\frac{p(1-p)}{n}} \\approx \\sqrt{\\frac{0.70 (1 - 0.70)} {826}} = 0.016.\\]Using point estimate 0.70, \\(z^{\\star} = 1.96\\) 95% confidence interval, standard error \\(SE = 0.016\\) previous Guided Practice, confidence interval \\[ \n\\begin{aligned}\n\\text{point estimate} \\ &\\pm \\ z^{\\star} \\times \\ SE \\\\\n0.70 \\ &\\pm \\ 1.96 \\ \\times \\ 0.016 \\\\ \n(0.669 \\ &, \\ 0.731)\n\\end{aligned}\n\\]95% confident true proportion payday borrowers supported regulation time poll 0.669 0.731.Constructing confidence interval single proportion.three steps constructing confidence interval \\(p.\\)Check seems reasonable assume observations independent check success-failure condition using \\(\\hat{p}.\\) conditions met, sampling distribution \\(\\hat{p}\\) may well-approximated normal model.Construct standard error using \\(\\hat{p}\\) place \\(p\\) standard error formula.Apply general confidence interval formula.additional one-proportion confidence interval examples, see Section 12.4.","code":""},{"path":"inference-one-prop.html","id":"changing-the-confidence-level","chapter":"15 Inference for a single proportion","heading":"15.2.4 Changing the confidence level","text":"Suppose want consider confidence intervals confidence level somewhat higher 95%: perhaps like confidence level 99%.\nThink back analogy trying catch fish: want sure catch fish, use wider net.\ncreate 99% confidence level, must also widen 95% interval.\nhand, want interval lower confidence, 90%, make original 95% interval slightly slimmer.95% confidence interval structure provides guidance make intervals new confidence levels.\ngeneral 95% confidence interval point estimate comes nearly normal distribution:\\[\\text{point estimate} \\ \\pm \\ 1.96 \\ \\times \\ SE\\]three components interval: point estimate, “1.96”, standard error.\nchoice \\(1.96 \\times SE\\) based capturing 95% data since estimate within 1.96 standard errors true value 95% time.\nchoice 1.96 corresponds 95% confidence level.\\(X\\) normally distributed random variable, often \\(X\\) within 2.58 standard deviations mean?94\nFigure 15.2: area -\\(z^{\\star}\\) \\(z^{\\star}\\) increases \\(|z^{\\star}|\\) becomes larger. confidence level 99%, choose \\(z^{\\star}\\) 99% normal curve -\\(z^{\\star}\\) \\(z^{\\star},\\) corresponds 0.5% lower tail 0.5% upper tail: \\(z^{\\star}=2.58.\\)\ncreate 99% confidence interval, change 1.96 95% confidence interval formula \\(2.58.\\) previous Guided Practice highlights 99% time normal random variable within 2.58 standard deviations mean.\napproach – using Z scores normal model compute confidence levels – appropriate point estimate associated normal distribution can properly compute standard error.\nThus, formula 99% confidence interval :\\[\\text{point estimate} \\ \\pm \\ 2.58 \\ \\times \\ SE\\]normal approximation crucial precision \\(z^\\star\\) confidence intervals (contrast bootstrap percentile confidence intervals).\nnormal model good fit, use alternative distributions better characterize sampling distribution use bootstrapping procedures.Create 99% confidence interval impact stent risk stroke using data Section ??.\npoint estimate 0.090, standard error \\(SE = 0.028.\\) verified point estimate can reasonably modeled normal distribution.95Mathematical model confidence interval confidence level.point estimate follows normal model standard error \\(SE,\\) confidence interval population parameter \\[\\text{point estimate} \\ \\pm \\ z^{\\star} \\ \\times \\ SE\\]\\(z^{\\star}\\) corresponds confidence level selected.Figure 15.2 provides picture identify \\(z^{\\star}\\) based confidence level.\nselect \\(z^{\\star}\\) area -\\(z^{\\star}\\) \\(z^{\\star}\\) normal model corresponds confidence level.Previously, found implanting stent brain patient risk stroke increased risk stroke.\nstudy estimated 9% increase number patients stroke, standard error estimate \\(SE = 2.8%.\\) Compute 90% confidence interval effect.96","code":""},{"path":"inference-one-prop.html","id":"hypothesis-test-for-a-proportion","chapter":"15 Inference for a single proportion","heading":"15.2.5 Hypothesis test for a proportion","text":"test statistic assessing single proportion Z.Z score ratio sample proportion differs hypothesized proportion compared expected variability \\(\\hat{p}\\) values.\\[Z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1 - p_0)/n}}\\]null hypothesis true conditions met, Z standard normal distribution.Conditions:independent observationslarge samples \\((n p_0 \\geq 10\\) \\(n (1-p_0) \\geq 10)\\)One possible regulation payday lenders required credit check evaluate debt payments borrower’s finances.\nlike know: borrowers support form regulation?Set hypotheses evaluate whether borrowers majority support type regulation.97To apply normal distribution framework context hypothesis test proportion, independence success-failure conditions must satisfied.\nhypothesis test, success-failure condition checked using null proportion: verify \\(np_0\\) \\(n(1-p_0)\\) least 10, \\(p_0\\) null value.payday loan borrowers support regulation require lenders pull credit report evaluate debt payments?\nrandom sample 826 borrowers, 51% said support regulation.\nreasonable use normal distribution model \\(\\hat{p}\\) hypothesis test ?98Using hypotheses data previous Guided Practices, evaluate whether poll lending regulations provides convincing evidence majority payday loan borrowers support new regulation require lenders pull credit reports evaluate debt payments.hypotheses already set conditions checked, can move onto calculations.\nstandard error context one-proportion hypothesis test computed using null value, \\(p_0:\\)\\[SE = \\sqrt{\\frac{p_0 (1 - p_0)}{n}} = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}} = 0.017\\]picture normal model shown p-value represented shaded region.Based normal model, test statistic can computed Z score point estimate:\\[\n\\begin{aligned}\nZ &= \\frac{\\text{point estimate} - \\text{null value}}{SE} \\\\\n  &= \\frac{0.51 - 0.50}{0.017} \\\\\n  &= 0.59\n\\end{aligned} \n\\]single tail area represents p-value 0.2776.\np-value larger 0.05, reject \\(H_0.\\) poll provide convincing evidence majority payday loan borrowers support regulations around credit checks evaluation debt payments.might wanted ask whether borrows support oppose regulations (study opinion either direction away 50% benchmark), .e., two-sided test.\ncase, p-value doubled 0.5552 (, reject \\(H_0).\\) two-sided hypothesis setting, appropriate conclusion claim poll provide convincing evidence majority payday loan borrowers support oppose regulations around credit checks evaluation debt payments.one-sided two-sided setting, conclusion somewhat unsatisfactory conclusion.\n, resolution one way public opinion.\nclaim exactly 50% people support regulation, claim majority either direction.Mathematical model hypothesis test proportion.Set hypotheses verify conditions using null value, \\(p_0,\\) ensure \\(\\hat{p}\\) nearly normal \\(H_0.\\) conditions hold, construct standard error, using \\(p_0,\\) show p-value drawing.\nLastly, compute p-value evaluate hypotheses.","code":""},{"path":"inference-one-prop.html","id":"violating-conditions","chapter":"15 Inference for a single proportion","heading":"15.2.6 Violating conditions","text":"’ve spent lot time discussing conditions \\(\\hat{p}\\) can reasonably modeled normal distribution.\nhappens success-failure condition fails?\nindependence condition fails?\neither case, general ideas confidence intervals hypothesis tests remain , strategy technique used generate interval p-value change.success-failure condition isn’t met hypothesis test, can simulate null distribution \\(\\hat{p}\\) using null value, \\(p_0,\\) seen Section 15.1.\nUnfortunately, methods dealing observations independent (e.g., repeated measurements subject, studies measurements subjects taken pre post study) outside scope course.","code":""},{"path":"inference-one-prop.html","id":"chp16-review","chapter":"15 Inference for a single proportion","heading":"15.3 Chapter review","text":"","code":""},{"path":"inference-one-prop.html","id":"summary-7","chapter":"15 Inference for a single proportion","heading":"15.3.1 Summary","text":"Building foundational ideas previous ideas, chapter focused exclusively single population proportion parameter interest.\nNote possible randomization test one variable, computational hypothesis testing, applied bootstrapping framework.\nbootstrap confidence interval mathematical framework hypothesis testing confidence intervals similar applied data structures parameters.\nusing mathematical model, keep mind success-failure conditions.\nAdditionally, know bootstrapping always accurate larger samples.","code":""},{"path":"inference-one-prop.html","id":"terms-2","chapter":"15 Inference for a single proportion","heading":"15.3.2 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"inference-one-prop.html","id":"chp16-exercises","chapter":"15 Inference for a single proportion","heading":"15.4 Exercises","text":"aliens exist?\nMay 2021, YouGov asked 4,839 adult Great Britain residents whether think aliens exist, , visited Earth.\nwant evaluate quarter (25%) Great Britain adults think aliens don’t exist.\nsurvey 22% responded “think exist, visited Earth”, 28% responded “think exist, visited Earth”, 29% responded “don’t think exist”, 22% responded “Don’t know”.\nfriend offers help setting hypothesis test comes following hypotheses.\nIndicate errors see.\n\\(H_0: \\hat{p} = 0.29 \\quad \\quad H_A: \\hat{p} > 0.29\\)\naliens exist?\nMay 2021, YouGov asked 4,839 adult Great Britain residents whether think aliens exist, , visited Earth.\nwant evaluate quarter (25%) Great Britain adults think aliens don’t exist.\nsurvey 22% responded “think exist, visited Earth”, 28% responded “think exist, visited Earth”, 29% responded “don’t think exist”, 22% responded “Don’t know”.\nfriend offers help setting hypothesis test comes following hypotheses.\nIndicate errors see.\\(H_0: \\hat{p} = 0.29 \\quad \\quad H_A: \\hat{p} > 0.29\\)Married 25.\nstudy suggests 25% 25 year olds gotten married.\nbelieve incorrect decide collect sample hypothesis test.\nrandom sample 25 year olds census data size 776, find 24% married.\nfriend offers help setting hypothesis test comes following hypotheses. Indicate errors see.\n\\(H_0: \\hat{p} = 0.24 \\quad \\quad H_A: \\hat{p} \\neq 0.24\\)\nMarried 25.\nstudy suggests 25% 25 year olds gotten married.\nbelieve incorrect decide collect sample hypothesis test.\nrandom sample 25 year olds census data size 776, find 24% married.\nfriend offers help setting hypothesis test comes following hypotheses. Indicate errors see.\\(H_0: \\hat{p} = 0.24 \\quad \\quad H_A: \\hat{p} \\neq 0.24\\)Defund police.\nSurvey USA poll conducted Seattle, WA May 2021 reports 650 respondents (adults living area), 159 support proposals defund police departments.99\njournalist writing news story poll results wants use headline “1 5 adults living Seattle support proposals defund police departments.” caution journalist first conduct hypothesis test see poll data provide convincing evidence claim. Write hypotheses test.\nCalculate proportion Seattle adults sample support proposals defund police departments.\nDescribe setup simulation appropriate situation p-value can calculated using simulation results.\nhistogram showing distribution \\(\\hat{p}_{sim}\\) 1,000 simulations null hypothesis. Estimate p-value using plot use evaluate hypotheses.\n\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\n\ndefund_police <- tibble(opinion = c(rep(\"support\", 159), rep(\"support\", 650-159)))\n\nset.seed(47)\nnull_dist <- defund_police %>%\n  specify(response = opinion, success = \"support\") %>%\n  hypothesize(null = \"point\", p = 0.20) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\")\n\nnull_dist %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(bins = 20, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.20\",\n    x = \"Bootstrapped proportion \\nthose support proposals defund police departments\",\n    y = \"Count\"\n    ) +\n  scale_x_continuous(breaks = seq(0.15, 0.25, 0.02))\n\nDefund police.\nSurvey USA poll conducted Seattle, WA May 2021 reports 650 respondents (adults living area), 159 support proposals defund police departments.99A journalist writing news story poll results wants use headline “1 5 adults living Seattle support proposals defund police departments.” caution journalist first conduct hypothesis test see poll data provide convincing evidence claim. Write hypotheses test.journalist writing news story poll results wants use headline “1 5 adults living Seattle support proposals defund police departments.” caution journalist first conduct hypothesis test see poll data provide convincing evidence claim. Write hypotheses test.Calculate proportion Seattle adults sample support proposals defund police departments.Calculate proportion Seattle adults sample support proposals defund police departments.Describe setup simulation appropriate situation p-value can calculated using simulation results.Describe setup simulation appropriate situation p-value can calculated using simulation results.histogram showing distribution \\(\\hat{p}_{sim}\\) 1,000 simulations null hypothesis. Estimate p-value using plot use evaluate hypotheses.histogram showing distribution \\(\\hat{p}_{sim}\\) 1,000 simulations null hypothesis. Estimate p-value using plot use evaluate hypotheses.Assisted reproduction.\nAssisted Reproductive Technology (ART) collection techniques help facilitate pregnancy (e.g., vitro fertilization). 2018 ART Fertility Clinic Success Rates Report published Centers Disease Control Prevention reports ART successful leading live birth 48.8% cases patient 35 years old.100 new fertility clinic claims success rate higher average age group. random sample 30 patients yielded success rate 60%. consumer watchdog group like determine provides strong evidence support company’s claim.\nWrite hypotheses test success rate ART clinic significantly higher success rate reported CDC.\nDescribe setup simulation appropriate situation p-value can calculated using simulation results.\n\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\n\nart <- tibble(fertility = c(rep(\"success\", 12), rep(\"failure\", 18)))\n\nset.seed(47)\nnull_dist <- art %>%\n  specify(response = fertility, success = \"success\") %>%\n  hypothesize(null = \"point\", p = 0.488) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") \n\nnull_dist %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.488\",\n    x = \"Proportion successful ART cases\",\n    y = \"Count\"\n    ) +\n  geom_vline(xintercept = 0.6, color = IMSCOL[\"red\", 1], size = 1)\n\nhistogram showing distribution \\(\\hat{p}_{sim}\\) 1,000 simulations null hypothesis. Estimate p-value using plot use evaluate hypotheses.\nperforming analysis, consumer group releases following news headline: “Infertility clinic falsely advertises better success rates”. Comment appropriateness statement.\nAssisted reproduction.\nAssisted Reproductive Technology (ART) collection techniques help facilitate pregnancy (e.g., vitro fertilization). 2018 ART Fertility Clinic Success Rates Report published Centers Disease Control Prevention reports ART successful leading live birth 48.8% cases patient 35 years old.100 new fertility clinic claims success rate higher average age group. random sample 30 patients yielded success rate 60%. consumer watchdog group like determine provides strong evidence support company’s claim.Write hypotheses test success rate ART clinic significantly higher success rate reported CDC.Write hypotheses test success rate ART clinic significantly higher success rate reported CDC.Describe setup simulation appropriate situation p-value can calculated using simulation results.Describe setup simulation appropriate situation p-value can calculated using simulation results.histogram showing distribution \\(\\hat{p}_{sim}\\) 1,000 simulations null hypothesis. Estimate p-value using plot use evaluate hypotheses.histogram showing distribution \\(\\hat{p}_{sim}\\) 1,000 simulations null hypothesis. Estimate p-value using plot use evaluate hypotheses.performing analysis, consumer group releases following news headline: “Infertility clinic falsely advertises better success rates”. Comment appropriateness statement.performing analysis, consumer group releases following news headline: “Infertility clinic falsely advertises better success rates”. Comment appropriateness statement.fits, sits, bootstrap test.\ncitizen science project type enclosed spaces cats likely sit compared (among options) two different spaces taped ground. first square, second shape known Kanizsa square illusion. comparing two options given 7 cats, 5 chose square, 2 chose Kanizsa square illusion. interested know whether data provide convincing evidence cats prefer one shapes .101\nnull alternative hypotheses evaluating whether data provide convincing evidence cats preference one shapes\nparametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram .Find p-value using distribution conclude hypothesis test context problem.\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\ncats <- tibble(choice = c(rep(\"square\", 5), rep(\"Kanizsa\", 2)))\n\nset.seed(47)\ncats %>%\n  specify(response = choice, success = \"square\") %>%\n  hypothesize(null = \"point\", p = 0.5) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.5\",\n    x = \"Proportion cats prefer squares\",\n    y = \"Count\"\n    )\nfits, sits, bootstrap test.\ncitizen science project type enclosed spaces cats likely sit compared (among options) two different spaces taped ground. first square, second shape known Kanizsa square illusion. comparing two options given 7 cats, 5 chose square, 2 chose Kanizsa square illusion. interested know whether data provide convincing evidence cats prefer one shapes .101What null alternative hypotheses evaluating whether data provide convincing evidence cats preference one shapesWhat null alternative hypotheses evaluating whether data provide convincing evidence cats preference one shapesA parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram .Find p-value using distribution conclude hypothesis test context problem.parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram .Find p-value using distribution conclude hypothesis test context problem.Legalization marijuana, bootstrap test.\n2018 General Social Survey asked random sample 1,563 US adults: “think use marijuana made legal, ?” 60% respondents said made legal.102 Consider scenario , order become legal, 55% () voters must approve.\nnull alternative hypotheses evaluating whether data provide convincing evidence , voted , marijuana legalized US.\nparametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . Find p-value using distribution conclude hypothesis test context problem.\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\nmj <- tibble(opinion = c(rep(\"legal\", 938),rep(\"legal\", 625)))\n\nset.seed(47)\nmj %>%\n  specify(response = opinion, success = \"legal\") %>%\n  hypothesize(null = \"point\", p = 0.55) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.005, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.55\",\n    x = \"Proportion US adults support legalizing marijuana\",\n    y = \"Count\"\n    )\n\nLegalization marijuana, bootstrap test.\n2018 General Social Survey asked random sample 1,563 US adults: “think use marijuana made legal, ?” 60% respondents said made legal.102 Consider scenario , order become legal, 55% () voters must approve.null alternative hypotheses evaluating whether data provide convincing evidence , voted , marijuana legalized US.null alternative hypotheses evaluating whether data provide convincing evidence , voted , marijuana legalized US.parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . Find p-value using distribution conclude hypothesis test context problem.parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . Find p-value using distribution conclude hypothesis test context problem.fits, sits, standard errors.\nresults study type enclosed spaces cats likely sit show 5 7 cats chose square taped ground shape known Kanizsa square illusion, preferred remaining 2 cats. evaluate whether data provide convincing evidence cats prefer one shapes , set \\(H_0: p = 0.5\\), \\(p\\) population proportion cats prefer square Kanizsa square illusion \\(H_A: p \\neq 0.5\\), suggests preference, without specifying shape preferred.103\nUsing mathematical model, calculate standard error sample proportion repeated samples size 7.\nparametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . distribution shows variability sample proportion samples size 7 50% cats prefer square shape Kanizsa square illusion. approximate standard error sample proportion based distribution?\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\ncats <- tibble(choice = c(rep(\"square\", 5), rep(\"Kanizsa\", 2)))\n\nset.seed(47)\ncats %>%\n  specify(response = choice, success = \"square\") %>%\n  hypothesize(null = \"point\", p = 0.5) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.5\",\n    x = \"Bootstrapped proportion \\ncats prefer squares\",\n    y = \"Count\"\n    )\n\nmathematical model parametric bootstrap give similar standard errors?\norder approach problem using mathematical model, success-failure condition met study?Explain.\nnull distribution shown (generated using parametric bootstrap) tells us mathematical model probably used?\nfits, sits, standard errors.\nresults study type enclosed spaces cats likely sit show 5 7 cats chose square taped ground shape known Kanizsa square illusion, preferred remaining 2 cats. evaluate whether data provide convincing evidence cats prefer one shapes , set \\(H_0: p = 0.5\\), \\(p\\) population proportion cats prefer square Kanizsa square illusion \\(H_A: p \\neq 0.5\\), suggests preference, without specifying shape preferred.103Using mathematical model, calculate standard error sample proportion repeated samples size 7.Using mathematical model, calculate standard error sample proportion repeated samples size 7.parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . distribution shows variability sample proportion samples size 7 50% cats prefer square shape Kanizsa square illusion. approximate standard error sample proportion based distribution?parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . distribution shows variability sample proportion samples size 7 50% cats prefer square shape Kanizsa square illusion. approximate standard error sample proportion based distribution?mathematical model parametric bootstrap give similar standard errors?mathematical model parametric bootstrap give similar standard errors?order approach problem using mathematical model, success-failure condition met study?Explain.order approach problem using mathematical model, success-failure condition met study?Explain.null distribution shown (generated using parametric bootstrap) tells us mathematical model probably used?null distribution shown (generated using parametric bootstrap) tells us mathematical model probably used?Legalization marijuana, standard errors.\nAccording 2018 General Social Survey, random sample 1,563 US adults, 60% think marijuana made legal.104 Consider scenario , order become legal, 55% () voters must approve.\nCalculate standard error sample proportion using mathematical model.\nparametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . distribution shows variability sample proportion samples size 1,563 55% voters approve legalizing marijuana. approximate standard error sample proportion based distribution?\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\nmj <- tibble(opinion = c(rep(\"legal\", 938),rep(\"legal\", 625)))\n\nset.seed(47)\nmj %>%\n  specify(response = opinion, success = \"legal\") %>%\n  hypothesize(null = \"point\", p = 0.55) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.005, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.55\",\n    x = \"Proportion US adults support legalizing marijuana\",\n    y = \"Count\"\n    )\n\nmathematical model parametric bootstrap give similar standard errors?\nsetting (test whether true underlying population proportion greater 0.55), strong reason choose mathematical model parametric bootstrap (vice versa)?\nLegalization marijuana, standard errors.\nAccording 2018 General Social Survey, random sample 1,563 US adults, 60% think marijuana made legal.104 Consider scenario , order become legal, 55% () voters must approve.Calculate standard error sample proportion using mathematical model.Calculate standard error sample proportion using mathematical model.parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . distribution shows variability sample proportion samples size 1,563 55% voters approve legalizing marijuana. approximate standard error sample proportion based distribution?parametric bootstrap simulation (1,000 bootstrap samples) run resulting null distribution displayed histogram . distribution shows variability sample proportion samples size 1,563 55% voters approve legalizing marijuana. approximate standard error sample proportion based distribution?mathematical model parametric bootstrap give similar standard errors?mathematical model parametric bootstrap give similar standard errors?setting (test whether true underlying population proportion greater 0.55), strong reason choose mathematical model parametric bootstrap (vice versa)?setting (test whether true underlying population proportion greater 0.55), strong reason choose mathematical model parametric bootstrap (vice versa)?Statistics employment, describe bootstrap.\nlarge university knows 70% full-time students employed least 5 hours per week. members Statistics Department wonder proportion students work least 5 hours per week. randomly sample 25 majors find 15 students work 5 hours week.\nTwo bootstrap sampling distributions created describe variability proportion statistics majors work least 5 hours per week. parametric bootstrap imposes true population proportion \\(p = 0.7\\) data bootstrap resamples actual data (60% observations work least 5 hours per week).\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\nlibrary(patchwork)\n\nstudents <- tibble(outside = c(rep(\"work\", 15), rep(\"work\", 10)))\n\nset.seed(47)\np_para <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  hypothesize(null = \"point\", p = 0.7) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Parametric bootstrap\",\n    subtitle = \"p = 0.7\",\n    x = \"Bootstrapped proportion \\nthose work\",\n    y = \"Count\"\n  )\n\nset.seed(47)\np_data <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Data bootstrap\",\n    x = \"Bootstrapped proportion \\nthose work\",\n    y = \"Count\"\n  )\n\np_para + p_data\n\nbootstrap sampling done two different settings generate distributions shown . Describe two different settings.\ntwo distributions centered? centered roughly place?\nEstimate standard error simulated proportions based distribution. two standard errors estimate roughly equal?\nDescribe shapes two distributions. roughly ?\nStatistics employment, describe bootstrap.\nlarge university knows 70% full-time students employed least 5 hours per week. members Statistics Department wonder proportion students work least 5 hours per week. randomly sample 25 majors find 15 students work 5 hours week.Two bootstrap sampling distributions created describe variability proportion statistics majors work least 5 hours per week. parametric bootstrap imposes true population proportion \\(p = 0.7\\) data bootstrap resamples actual data (60% observations work least 5 hours per week).bootstrap sampling done two different settings generate distributions shown . Describe two different settings.bootstrap sampling done two different settings generate distributions shown . Describe two different settings.two distributions centered? centered roughly place?two distributions centered? centered roughly place?Estimate standard error simulated proportions based distribution. two standard errors estimate roughly equal?Estimate standard error simulated proportions based distribution. two standard errors estimate roughly equal?Describe shapes two distributions. roughly ?Describe shapes two distributions. roughly ?National Health Plan, parametric bootstrap.\nKaiser Family Foundation poll random sample US adults 2019 found 79% Democrats, 55% Independents, 24% Republicans supported generic “National Health Plan”.\n347 Democrats, 298 Republicans, 617 Independents surveyed.105\npolitical pundit TV claims majority Independents support National Health Plan. data provide strong evidence support type statement? One approach assessing question whether majority Independents support National Health Plan simulate 1,000 parametric bootstrap samples \\(p = 0.5\\) proportion Independents support.\n\nlibrary(tidyverse)\nlibrary(infer)\n\nnhp <- tibble(opinion = c(rep(\"yes plan\", 339),rep(\"plan\", 278)))\n\nset.seed(47)\nnull_dist <- nhp %>%\n  specify(response = opinion, success = \"yes plan\") %>%\n  hypothesize(null = \"point\", p = 0.50) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\")\n\nnull_dist %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.005, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.50\",\n    x = \"Bootstrapped proportion \\nthose support national health plan\",\n    y = \"Count\"\n    )\n\nhistogram displays 1000 values ?\nobserved proportion Independents consistent parametric bootstrap proportions setting \\(p=0.5?\\)\norder test claim “majority Independents support National Health Plan” null alternative hypotheses?\nUsing parametric bootstrap distribution, find p-value conclude hypothesis test context problem.\nNational Health Plan, parametric bootstrap.\nKaiser Family Foundation poll random sample US adults 2019 found 79% Democrats, 55% Independents, 24% Republicans supported generic “National Health Plan”.\n347 Democrats, 298 Republicans, 617 Independents surveyed.105A political pundit TV claims majority Independents support National Health Plan. data provide strong evidence support type statement? One approach assessing question whether majority Independents support National Health Plan simulate 1,000 parametric bootstrap samples \\(p = 0.5\\) proportion Independents support.histogram displays 1000 values ?histogram displays 1000 values ?observed proportion Independents consistent parametric bootstrap proportions setting \\(p=0.5?\\)observed proportion Independents consistent parametric bootstrap proportions setting \\(p=0.5?\\)order test claim “majority Independents support National Health Plan” null alternative hypotheses?order test claim “majority Independents support National Health Plan” null alternative hypotheses?Using parametric bootstrap distribution, find p-value conclude hypothesis test context problem.Using parametric bootstrap distribution, find p-value conclude hypothesis test context problem.Statistics employment, use bootstrap.\nlarge university 70% full-time students employed least 5 hours per week, members Statistics Department wonder proportion students work least 5 hours per week. randomly sample 25 majors find 15 students work 5 hours week.\nTwo bootstrap sampling distributions created describe variability proportion statistics majors work least 5 hours per week. parametric bootstrap imposes true population proportion \\(p=0.7\\) data bootstrap resamples actual data (60% observations work least 5 hours per week).\n\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\nlibrary(patchwork)\n\nstudents <- tibble(outside = c(rep(\"work\", 15), rep(\"work\", 10)))\n\nset.seed(47)\np_para <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  hypothesize(null = \"point\", p = 0.7) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Parametric bootstrap\",\n    subtitle = \"p = 0.7\",\n    x = \"Bootstrapped proportion \\nthose work\",\n    y = \"Count\"\n  )\n\nset.seed(47)\np_data <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Data bootstrap\",\n    x = \"Bootstrapped proportion \\nthose work\",\n    y = \"Count\"\n  )\n\np_para + p_data\n\nbootstrap distribution used test whether proportion statistics majors work least 5 hours per week 70%? bootstrap distribution used find confidence interval true poportion statistics majors work least 5 hours per week?\nUsing appropriate histogram, test claim 70% statistics majors, like peers, work least 5 hours per week. State null alternative hypotheses, find p-value, conclude test context problem.\nUsing appropriate histogram, find 98% bootstrap percentile confidence interval true proportion statistics majors work least 5 hours per week. Interpret confidence interval context problem.\nUsing appropriate historgram, find 98% bootstrap SE confidence interval true proportion statistics majors work least 5 hours per week. Interpret confidence interval context problem.\nStatistics employment, use bootstrap.\nlarge university 70% full-time students employed least 5 hours per week, members Statistics Department wonder proportion students work least 5 hours per week. randomly sample 25 majors find 15 students work 5 hours week.Two bootstrap sampling distributions created describe variability proportion statistics majors work least 5 hours per week. parametric bootstrap imposes true population proportion \\(p=0.7\\) data bootstrap resamples actual data (60% observations work least 5 hours per week).bootstrap distribution used test whether proportion statistics majors work least 5 hours per week 70%? bootstrap distribution used find confidence interval true poportion statistics majors work least 5 hours per week?bootstrap distribution used test whether proportion statistics majors work least 5 hours per week 70%? bootstrap distribution used find confidence interval true poportion statistics majors work least 5 hours per week?Using appropriate histogram, test claim 70% statistics majors, like peers, work least 5 hours per week. State null alternative hypotheses, find p-value, conclude test context problem.Using appropriate histogram, test claim 70% statistics majors, like peers, work least 5 hours per week. State null alternative hypotheses, find p-value, conclude test context problem.Using appropriate histogram, find 98% bootstrap percentile confidence interval true proportion statistics majors work least 5 hours per week. Interpret confidence interval context problem.Using appropriate histogram, find 98% bootstrap percentile confidence interval true proportion statistics majors work least 5 hours per week. Interpret confidence interval context problem.Using appropriate historgram, find 98% bootstrap SE confidence interval true proportion statistics majors work least 5 hours per week. Interpret confidence interval context problem.Using appropriate historgram, find 98% bootstrap SE confidence interval true proportion statistics majors work least 5 hours per week. Interpret confidence interval context problem.CLT proportions.\nDefine term “sampling distribution” sample proportion, describe shape, center, spread sampling distribution change sample size increases \\(p = 0.1\\).\nCLT proportions.\nDefine term “sampling distribution” sample proportion, describe shape, center, spread sampling distribution change sample size increases \\(p = 0.1\\).Vegetarian college students.\nSuppose 8% college students vegetarians. Determine following statements true false, explain reasoning.\ndistribution sample proportions vegetarians random samples size 60 approximately normal since \\(n \\ge 30\\).\ndistribution sample proportions vegetarian college students random samples size 50 right skewed.\nrandom sample 125 college students 12% vegetarians considered unusual.\nrandom sample 250 college students 12% vegetarians considered unusual.\nstandard error reduced one-half increased sample size 125  250.\nVegetarian college students.\nSuppose 8% college students vegetarians. Determine following statements true false, explain reasoning.distribution sample proportions vegetarians random samples size 60 approximately normal since \\(n \\ge 30\\).distribution sample proportions vegetarians random samples size 60 approximately normal since \\(n \\ge 30\\).distribution sample proportions vegetarian college students random samples size 50 right skewed.distribution sample proportions vegetarian college students random samples size 50 right skewed.random sample 125 college students 12% vegetarians considered unusual.random sample 125 college students 12% vegetarians considered unusual.random sample 250 college students 12% vegetarians considered unusual.random sample 250 college students 12% vegetarians considered unusual.standard error reduced one-half increased sample size 125  250.standard error reduced one-half increased sample size 125  250.Young Americans, American dream.\n77% young adults think can achieve American dream.\nDetermine following statements true false, explain reasoning.106\ndistribution sample proportions young Americans think can achieve American dream random samples size 20 left skewed.\ndistribution sample proportions young Americans think can achieve American dream random samples size 40 approximately normal since \\(n \\ge 30\\).\nrandom sample 60 young Americans 85% think can achieve American dream considered unusual.\nrandom sample 120 young Americans 85% think can achieve American dream considered unusual.\nYoung Americans, American dream.\n77% young adults think can achieve American dream.\nDetermine following statements true false, explain reasoning.106The distribution sample proportions young Americans think can achieve American dream random samples size 20 left skewed.distribution sample proportions young Americans think can achieve American dream random samples size 20 left skewed.distribution sample proportions young Americans think can achieve American dream random samples size 40 approximately normal since \\(n \\ge 30\\).distribution sample proportions young Americans think can achieve American dream random samples size 40 approximately normal since \\(n \\ge 30\\).random sample 60 young Americans 85% think can achieve American dream considered unusual.random sample 60 young Americans 85% think can achieve American dream considered unusual.random sample 120 young Americans 85% think can achieve American dream considered unusual.random sample 120 young Americans 85% think can achieve American dream considered unusual.Orange tabbies.\nSuppose 90% orange tabby cats male.\nDetermine following statements true false, explain reasoning.\ndistribution sample proportions random samples size 30 left skewed.\nUsing sample size 4 times large reduce standard error sample proportion one-half.\ndistribution sample proportions random samples size 140 approximately normal.\ndistribution sample proportions random samples size 280 approximately normal.\nOrange tabbies.\nSuppose 90% orange tabby cats male.\nDetermine following statements true false, explain reasoning.distribution sample proportions random samples size 30 left skewed.distribution sample proportions random samples size 30 left skewed.Using sample size 4 times large reduce standard error sample proportion one-half.Using sample size 4 times large reduce standard error sample proportion one-half.distribution sample proportions random samples size 140 approximately normal.distribution sample proportions random samples size 140 approximately normal.distribution sample proportions random samples size 280 approximately normal.distribution sample proportions random samples size 280 approximately normal.Young Americans, starting family.\n25% young Americans delayed starting family due continued economic slump.\nDetermine following statements true false, explain reasoning.107\ndistribution sample proportions young Americans delayed starting family due continued economic slump random samples size 12 right skewed.\norder distribution sample proportions young Americans delayed starting family due continued economic slump approximately normal, need random samples sample size least 40.\nrandom sample 50 young Americans 20% delayed starting family due continued economic slump considered unusual.\nrandom sample 150 young Americans 20% delayed starting family due continued economic slump considered unusual.\nTripling sample size reduce standard error sample proportion one-third.\nYoung Americans, starting family.\n25% young Americans delayed starting family due continued economic slump.\nDetermine following statements true false, explain reasoning.107The distribution sample proportions young Americans delayed starting family due continued economic slump random samples size 12 right skewed.distribution sample proportions young Americans delayed starting family due continued economic slump random samples size 12 right skewed.order distribution sample proportions young Americans delayed starting family due continued economic slump approximately normal, need random samples sample size least 40.order distribution sample proportions young Americans delayed starting family due continued economic slump approximately normal, need random samples sample size least 40.random sample 50 young Americans 20% delayed starting family due continued economic slump considered unusual.random sample 50 young Americans 20% delayed starting family due continued economic slump considered unusual.random sample 150 young Americans 20% delayed starting family due continued economic slump considered unusual.random sample 150 young Americans 20% delayed starting family due continued economic slump considered unusual.Tripling sample size reduce standard error sample proportion one-third.Tripling sample size reduce standard error sample proportion one-third.Sex equality.\nGeneral Social Survey asked random sample 1,390 Americans following question: “whole, think government’s responsibility promote equality men women?” 82% respondents said “”. 95% confidence level, sample 2% margin error. Based information, determine following statements true false, explain reasoning.108\n95% confident 80% 84% Americans sample think ’s government’s responsibility promote equality men women.\n95% confident 80% 84% Americans think ’s government’s responsibility promote equality men women.\nconsidered many random samples 1,390 Americans, calculated 95% confidence intervals , 95% intervals include true population proportion Americans think ’s government’s responsibility promote equality men women.\norder decrease margin error 1%, need quadruple (multiply 4) sample size.\nBased confidence interval, sufficient evidence conclude majority Americans think ’s government’s responsibility promote equality men women.\nSex equality.\nGeneral Social Survey asked random sample 1,390 Americans following question: “whole, think government’s responsibility promote equality men women?” 82% respondents said “”. 95% confidence level, sample 2% margin error. Based information, determine following statements true false, explain reasoning.108We 95% confident 80% 84% Americans sample think ’s government’s responsibility promote equality men women.95% confident 80% 84% Americans sample think ’s government’s responsibility promote equality men women.95% confident 80% 84% Americans think ’s government’s responsibility promote equality men women.95% confident 80% 84% Americans think ’s government’s responsibility promote equality men women.considered many random samples 1,390 Americans, calculated 95% confidence intervals , 95% intervals include true population proportion Americans think ’s government’s responsibility promote equality men women.considered many random samples 1,390 Americans, calculated 95% confidence intervals , 95% intervals include true population proportion Americans think ’s government’s responsibility promote equality men women.order decrease margin error 1%, need quadruple (multiply 4) sample size.order decrease margin error 1%, need quadruple (multiply 4) sample size.Based confidence interval, sufficient evidence conclude majority Americans think ’s government’s responsibility promote equality men women.Based confidence interval, sufficient evidence conclude majority Americans think ’s government’s responsibility promote equality men women.Elderly drivers.\nMarist Poll published report stating 66% adults nationally think licensed drivers required retake road test reach 65 years age. also reported interviews conducted random sample 1,018 American adults, margin error 3% using 95% confidence level.109\nVerify margin error reported Marist Poll using mathematical model.\nBased 95% confidence interval, poll provide convincing evidence two thirds population think licensed drivers required retake road test turn 65?\nElderly drivers.\nMarist Poll published report stating 66% adults nationally think licensed drivers required retake road test reach 65 years age. also reported interviews conducted random sample 1,018 American adults, margin error 3% using 95% confidence level.109Verify margin error reported Marist Poll using mathematical model.Verify margin error reported Marist Poll using mathematical model.Based 95% confidence interval, poll provide convincing evidence two thirds population think licensed drivers required retake road test turn 65?Based 95% confidence interval, poll provide convincing evidence two thirds population think licensed drivers required retake road test turn 65?Fireworks July 4\\(^{\\text{th}}\\).\nlocal news outlet reported 56% 600 randomly sampled Kansas residents planned set fireworks July \\(4^{th}\\).\nDetermine margin error 56% point estimate using 95% confidence level using mathematical model.110\nFireworks July 4\\(^{\\text{th}}\\).\nlocal news outlet reported 56% 600 randomly sampled Kansas residents planned set fireworks July \\(4^{th}\\).\nDetermine margin error 56% point estimate using 95% confidence level using mathematical model.110Proof COVID-19 vaccination.\nGallup poll surveyed 3,731 randomly sampled US April 2021, asking felt requiring proof COVID-19 vaccination travel airplane.\npoll found 57% said favor .111\nDescribe population parameter interest. value point estimate parameter?\nCheck conditions required constructing confidence interval using mathematical model based data met.\nConstruct 95% confidence interval proportion US adults favor requiring proof COVID-19 vaccination travel airplane.\nWithout calculations, describe happen confidence interval decided use higher confidence level.\nWithout calculations, describe happen confidence interval used larger sample.\nProof COVID-19 vaccination.\nGallup poll surveyed 3,731 randomly sampled US April 2021, asking felt requiring proof COVID-19 vaccination travel airplane.\npoll found 57% said favor .111Describe population parameter interest. value point estimate parameter?Describe population parameter interest. value point estimate parameter?Check conditions required constructing confidence interval using mathematical model based data met.Check conditions required constructing confidence interval using mathematical model based data met.Construct 95% confidence interval proportion US adults favor requiring proof COVID-19 vaccination travel airplane.Construct 95% confidence interval proportion US adults favor requiring proof COVID-19 vaccination travel airplane.Without calculations, describe happen confidence interval decided use higher confidence level.Without calculations, describe happen confidence interval decided use higher confidence level.Without calculations, describe happen confidence interval used larger sample.Without calculations, describe happen confidence interval used larger sample.Study abroad.\nsurvey 1,509 high school seniors took SAT completed optional web survey shows 55% high school seniors fairly certain participate study abroad program college.112\nsample representative sample population high school seniors US? Explain reasoning.\nLet’s suppose conditions inference met. Even answer part () indicated approach reliable, analysis may still interesting carry (though report). Using mathematical model, construct 90% confidence interval proportion high school seniors (took SAT) fairly certain participate study abroad program college, interpret interval context.\n“90% confidence” mean?\nBased interval, appropriate claim majority high school seniors fairly certain participate study abroad program college?\nStudy abroad.\nsurvey 1,509 high school seniors took SAT completed optional web survey shows 55% high school seniors fairly certain participate study abroad program college.112Is sample representative sample population high school seniors US? Explain reasoning.sample representative sample population high school seniors US? Explain reasoning.Let’s suppose conditions inference met. Even answer part () indicated approach reliable, analysis may still interesting carry (though report). Using mathematical model, construct 90% confidence interval proportion high school seniors (took SAT) fairly certain participate study abroad program college, interpret interval context.Let’s suppose conditions inference met. Even answer part () indicated approach reliable, analysis may still interesting carry (though report). Using mathematical model, construct 90% confidence interval proportion high school seniors (took SAT) fairly certain participate study abroad program college, interpret interval context.“90% confidence” mean?“90% confidence” mean?Based interval, appropriate claim majority high school seniors fairly certain participate study abroad program college?Based interval, appropriate claim majority high school seniors fairly certain participate study abroad program college?Legalization marijuana, mathematical interval.\nGeneral Social Survey asked random sample 1,563 US adults: “think use marijuana made legal, ?” 60% respondents said made legal.113\n60% sample statistic population parameter? Explain.\nUsing mathematical model, construct 95% confidence interval proportion US adults think marijuana made legal, interpret .\ncritic points 95% confidence interval accurate statistic follows normal distribution, normal model good approximation. true data? Explain.\nnews piece survey’s findings states, “Majority US adults think marijuana legalized.” Based confidence interval, statement justified?\nLegalization marijuana, mathematical interval.\nGeneral Social Survey asked random sample 1,563 US adults: “think use marijuana made legal, ?” 60% respondents said made legal.113Is 60% sample statistic population parameter? Explain.60% sample statistic population parameter? Explain.Using mathematical model, construct 95% confidence interval proportion US adults think marijuana made legal, interpret .Using mathematical model, construct 95% confidence interval proportion US adults think marijuana made legal, interpret .critic points 95% confidence interval accurate statistic follows normal distribution, normal model good approximation. true data? Explain.critic points 95% confidence interval accurate statistic follows normal distribution, normal model good approximation. true data? Explain.news piece survey’s findings states, “Majority US adults think marijuana legalized.” Based confidence interval, statement justified?news piece survey’s findings states, “Majority US adults think marijuana legalized.” Based confidence interval, statement justified?National Health Plan, mathematical inference.\nKaiser Family Foundation poll random sample US adults 2019 found 79% Democrats, 55% Independents, 24% Republicans supported generic “National Health Plan”.\n347 Democrats, 298 Republicans, 617 Independents surveyed.114\npolitical pundit TV claims majority Independents support National Health Plan. data provide strong evidence support type statement? response use mathematical model.\nexpect confidence interval proportion Independents oppose public option plan include 0.5? Explain.\nNational Health Plan, mathematical inference.\nKaiser Family Foundation poll random sample US adults 2019 found 79% Democrats, 55% Independents, 24% Republicans supported generic “National Health Plan”.\n347 Democrats, 298 Republicans, 617 Independents surveyed.114A political pundit TV claims majority Independents support National Health Plan. data provide strong evidence support type statement? response use mathematical model.political pundit TV claims majority Independents support National Health Plan. data provide strong evidence support type statement? response use mathematical model.expect confidence interval proportion Independents oppose public option plan include 0.5? Explain.expect confidence interval proportion Independents oppose public option plan include 0.5? Explain.college worth ?\nAmong simple random sample 331 American adults four-year college degree currently enrolled school, 48% said decided go college afford school.115\nnewspaper article states minority Americans decide go college afford uses point estimate survey evidence. Conduct hypothesis test determine data provide strong evidence supporting statement.\nexpect confidence interval proportion American adults decide go college afford include 0.5? Explain.\ncollege worth ?\nAmong simple random sample 331 American adults four-year college degree currently enrolled school, 48% said decided go college afford school.115A newspaper article states minority Americans decide go college afford uses point estimate survey evidence. Conduct hypothesis test determine data provide strong evidence supporting statement.newspaper article states minority Americans decide go college afford uses point estimate survey evidence. Conduct hypothesis test determine data provide strong evidence supporting statement.expect confidence interval proportion American adults decide go college afford include 0.5? Explain.expect confidence interval proportion American adults decide go college afford include 0.5? Explain.Taste test.\npeople claim can tell difference diet soda regular soda first sip.\nresearcher wanting test claim randomly sampled 80 people.\nfilled 80 plain white cups soda, half diet half regular random assignment, asked person take one sip cup identify soda diet regular.\n53 participants correctly identified soda.\ndata provide strong evidence people able detect difference diet regular soda, words, results significantly better just random guessing? response use mathematical model.\nInterpret p-value context.\nTaste test.\npeople claim can tell difference diet soda regular soda first sip.\nresearcher wanting test claim randomly sampled 80 people.\nfilled 80 plain white cups soda, half diet half regular random assignment, asked person take one sip cup identify soda diet regular.\n53 participants correctly identified soda.data provide strong evidence people able detect difference diet regular soda, words, results significantly better just random guessing? response use mathematical model.data provide strong evidence people able detect difference diet regular soda, words, results significantly better just random guessing? response use mathematical model.Interpret p-value context.Interpret p-value context.coronavirus bring world closer together?\nApril 2021 YouGov poll asked 4,265 UK adults whether think coronavirus bring world closer together leave us apart.\n12% respondents said bring world closer together. 37% said leave us apart, 39% said won’t make difference remainder didn’t opinion matter.116\nCalculate, using mathematical model, 90% confidence interval proportion UK adults think coronavirus bring world closer together, interpret interval context.\nSuppose wanted margin error 90% confidence level 0.5%. large sample size recommend poll?\ncoronavirus bring world closer together?\nApril 2021 YouGov poll asked 4,265 UK adults whether think coronavirus bring world closer together leave us apart.\n12% respondents said bring world closer together. 37% said leave us apart, 39% said won’t make difference remainder didn’t opinion matter.116Calculate, using mathematical model, 90% confidence interval proportion UK adults think coronavirus bring world closer together, interpret interval context.Calculate, using mathematical model, 90% confidence interval proportion UK adults think coronavirus bring world closer together, interpret interval context.Suppose wanted margin error 90% confidence level 0.5%. large sample size recommend poll?Suppose wanted margin error 90% confidence level 0.5%. large sample size recommend poll?Quality control.\npart quality control process computer chips, engineer factory randomly samples 212 chips week production test current rate chips severe defects.\nfinds 27 chips defective.\npopulation consideration data set?\nparameter estimated?\npoint estimate parameter?\nname statistic can used measure uncertainty point estimate?\nCompute value statistic part (d) using mathematical model.\nhistorical rate defects 10%. engineer surprised observed rate defects current week?\nSuppose true population value found 10%. use proportion recompute value part (d) using \\(p = 0.1\\) instead \\(\\hat{p}\\), much resulting value statistic change?\nQuality control.\npart quality control process computer chips, engineer factory randomly samples 212 chips week production test current rate chips severe defects.\nfinds 27 chips defective.population consideration data set?population consideration data set?parameter estimated?parameter estimated?point estimate parameter?point estimate parameter?name statistic can used measure uncertainty point estimate?name statistic can used measure uncertainty point estimate?Compute value statistic part (d) using mathematical model.Compute value statistic part (d) using mathematical model.historical rate defects 10%. engineer surprised observed rate defects current week?historical rate defects 10%. engineer surprised observed rate defects current week?Suppose true population value found 10%. use proportion recompute value part (d) using \\(p = 0.1\\) instead \\(\\hat{p}\\), much resulting value statistic change?Suppose true population value found 10%. use proportion recompute value part (d) using \\(p = 0.1\\) instead \\(\\hat{p}\\), much resulting value statistic change?Nearsighted children.\nNearsightedness (myopia) common vision condition can see objects near clearly, objects farther away blurry.\nbelieved nearsightedness affects 8% children.\nrandom sample 194 children, 21 nearsighted.\nUsing mathematical model, conduct hypothesis test following question: data provide evidence 8% value inaccurate?Nearsighted children.\nNearsightedness (myopia) common vision condition can see objects near clearly, objects farther away blurry.\nbelieved nearsightedness affects 8% children.\nrandom sample 194 children, 21 nearsighted.\nUsing mathematical model, conduct hypothesis test following question: data provide evidence 8% value inaccurate?Website registration.\nwebsite trying increase registration first-time visitors, exposing 1% visitors new site design.\n752 randomly sampled visitors month saw new design, 64 registered.\nCheck conditions constructing confidence interval using mathematical model.\nCompute standard error describe variability associated repeated samples size 752.\nConstruct interpret 90% confidence interval fraction first-time visitors site register new design (assuming stable behaviors new visitors time).\nWebsite registration.\nwebsite trying increase registration first-time visitors, exposing 1% visitors new site design.\n752 randomly sampled visitors month saw new design, 64 registered.Check conditions constructing confidence interval using mathematical model.Check conditions constructing confidence interval using mathematical model.Compute standard error describe variability associated repeated samples size 752.Compute standard error describe variability associated repeated samples size 752.Construct interpret 90% confidence interval fraction first-time visitors site register new design (assuming stable behaviors new visitors time).Construct interpret 90% confidence interval fraction first-time visitors site register new design (assuming stable behaviors new visitors time).Coupons driving visits.\nstore randomly samples 603 shoppers course year finds 142 made visit coupon ’d received mail.\nUsing mathematical model, construct 95% confidence interval fraction shoppers year whose visit coupon ’d received mail.Coupons driving visits.\nstore randomly samples 603 shoppers course year finds 142 made visit coupon ’d received mail.\nUsing mathematical model, construct 95% confidence interval fraction shoppers year whose visit coupon ’d received mail.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\n\ndefund_police <- tibble(opinion = c(rep(\"support\", 159), rep(\"do not support\", 650-159)))\n\nset.seed(47)\nnull_dist <- defund_police %>%\n  specify(response = opinion, success = \"support\") %>%\n  hypothesize(null = \"point\", p = 0.20) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\")\n\nnull_dist %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(bins = 20, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.20\",\n    x = \"Bootstrapped proportion of\\nthose who support proposals to defund police departments\",\n    y = \"Count\"\n    ) +\n  scale_x_continuous(breaks = seq(0.15, 0.25, 0.02))\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\n\nart <- tibble(fertility = c(rep(\"success\", 12), rep(\"failure\", 18)))\n\nset.seed(47)\nnull_dist <- art %>%\n  specify(response = fertility, success = \"success\") %>%\n  hypothesize(null = \"point\", p = 0.488) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") \n\nnull_dist %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.488\",\n    x = \"Proportion of successful ART cases\",\n    y = \"Count\"\n    ) +\n  geom_vline(xintercept = 0.6, color = IMSCOL[\"red\", 1], size = 1)\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\ncats <- tibble(choice = c(rep(\"square\", 5), rep(\"Kanizsa\", 2)))\n\nset.seed(47)\ncats %>%\n  specify(response = choice, success = \"square\") %>%\n  hypothesize(null = \"point\", p = 0.5) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.5\",\n    x = \"Proportion of cats who prefer squares\",\n    y = \"Count\"\n    )\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\nmj <- tibble(opinion = c(rep(\"legal\", 938),rep(\"not legal\", 625)))\n\nset.seed(47)\nmj %>%\n  specify(response = opinion, success = \"legal\") %>%\n  hypothesize(null = \"point\", p = 0.55) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.005, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.55\",\n    x = \"Proportion of US adults who support legalizing marijuana\",\n    y = \"Count\"\n    )\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\ncats <- tibble(choice = c(rep(\"square\", 5), rep(\"Kanizsa\", 2)))\n\nset.seed(47)\ncats %>%\n  specify(response = choice, success = \"square\") %>%\n  hypothesize(null = \"point\", p = 0.5) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.5\",\n    x = \"Bootstrapped proportion of\\ncats who prefer squares\",\n    y = \"Count\"\n    )\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\n\nmj <- tibble(opinion = c(rep(\"legal\", 938),rep(\"not legal\", 625)))\n\nset.seed(47)\nmj %>%\n  specify(response = opinion, success = \"legal\") %>%\n  hypothesize(null = \"point\", p = 0.55) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) +\n  geom_histogram(binwidth = 0.005, fill = IMSCOL[\"green\", \"full\"]) + \n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.55\",\n    x = \"Proportion of US adults who support legalizing marijuana\",\n    y = \"Count\"\n    )\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\nlibrary(patchwork)\n\nstudents <- tibble(outside = c(rep(\"work\", 15), rep(\"don't work\", 10)))\n\nset.seed(47)\np_para <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  hypothesize(null = \"point\", p = 0.7) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Parametric bootstrap\",\n    subtitle = \"p = 0.7\",\n    x = \"Bootstrapped proportion of\\nthose who work\",\n    y = \"Count\"\n  )\n\nset.seed(47)\np_data <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Data bootstrap\",\n    x = \"Bootstrapped proportion of\\nthose who work\",\n    y = \"Count\"\n  )\n\np_para + p_data\nlibrary(tidyverse)\nlibrary(infer)\n\nnhp <- tibble(opinion = c(rep(\"yes plan\", 339),rep(\"no plan\", 278)))\n\nset.seed(47)\nnull_dist <- nhp %>%\n  specify(response = opinion, success = \"yes plan\") %>%\n  hypothesize(null = \"point\", p = 0.50) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\")\n\nnull_dist %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.005, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"1,000 parametric bootstrapped proportions\",\n    subtitle = \"p = 0.50\",\n    x = \"Bootstrapped proportion of\\nthose who support a national health plan\",\n    y = \"Count\"\n    )\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(openintro)\nlibrary(patchwork)\n\nstudents <- tibble(outside = c(rep(\"work\", 15), rep(\"don't work\", 10)))\n\nset.seed(47)\np_para <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  hypothesize(null = \"point\", p = 0.7) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Parametric bootstrap\",\n    subtitle = \"p = 0.7\",\n    x = \"Bootstrapped proportion of\\nthose who work\",\n    y = \"Count\"\n  )\n\nset.seed(47)\np_data <- students %>%\n  specify(response = outside, success = \"work\") %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  calculate(stat = \"prop\") %>%\n  ggplot(aes(x = stat)) + \n  geom_histogram(binwidth = 0.05, fill = IMSCOL[\"green\", \"full\"]) +\n  labs(\n    title = \"Data bootstrap\",\n    x = \"Bootstrapped proportion of\\nthose who work\",\n    y = \"Count\"\n  )\n\np_para + p_data"},{"path":"inference-one-prop.html","id":"chp16-exercises-sol","chapter":"15 Inference for a single proportion","heading":"15.5 Solutions to odd numbered exercises","text":"First, hypotheses population proportion (\\(p\\)), sample proportion. Second, null value testing (0.25), observed value (0.29). correct way set hypotheses : \\(H_0: p = 0.25\\) \\(H_A: p > 0.25.\\)() \\(H_0 : p = 0.20,\\) \\(H_A : p > 0.20.\\) (b) \\(\\hat{p} = 159/650 = 0.245.\\) (c) Answers vary. student can represented card. Take 100 cards, 20 black cards representing support proposals defund police departments 80 red cards representing . Shuffle cards draw replacement (shuffling time draws) 650 cards representing 650 respondents poll. Calculate proportion black cards sample, \\(\\hat{p}_{sim},\\) .e., proportion upport proposals defund police departments. p-value proportion simulations \\(\\hat{p}_{sim} \\geq 0.245.\\) (Note: generally use computer perform simulations.) (d) 1 one simulated proportion least 0.245, therefore approximate p-value 0.001. p-value may vary slightly since based visual estimate. Since p-value smaller 0.05, reject \\(H_0.\\) data provide convincing evidence proportion Seattle adults support proposals defund police departments greater 0.20, .e. one five.() \\(H_0: p = 0.5\\), \\(H_A: p \\ne 0.5\\). (b) p-value roughly 0.4, evidence data (possibly 7 cats measured!) conclude cats preference one way two shapes.() \\(SE(\\hat{p}) = 0.189\\). (c) Roughly 0.188. (c) Yes. (d) . (e) parametric bootstrap discrete (distinct options) mathematical model continuous (infinite options continuum).() parametric bootstrap simulation done \\(p=0.7\\), data bootstrap simulation done \\(p = 0.6.\\) (b) parametric bootstrap centered 0.7; data bootstrap centered 0.6. (c) standard error sample proportion given roughly 0.1 histograms. (d) histograms reasonably symmetric. Note histograms describe variability proportions become skewed center distribution gets closer 1 (zero) boundary 1.0 restricts symmetry tail distribution. reason, parametric bootstrap histogram slightly skewed (left).() parametric bootstrap testing. data bootstrap distribution confidence intervals. (b) \\(H_0: p = 0.7;\\) \\(H_A: p \\ne 0.7.\\) p-value \\(> 0.05.\\) evidence proportion full-time statistics majors work different 70%. (c) 98% confident true proportion full-time student statistics majors work least 5 hours per week 35% 80%. (d) Using \\(z^\\star = 2.33\\), 98% confidence interval 0.367 0.833.()  False. Doesn’t satisfy success-failure condition. (b) True. success-failure condition satisfied. samples expect \\(\\hat{p}\\) close 0.08, true population proportion. \\(\\hat{p}\\) can much 0.08, bound 0, suggesting take right skewed shape. Plotting sampling distribution confirm suspicion. (c) False. \\(SE_{\\hat{p}} = 0.0243\\), \\(\\hat{p} = 0.12\\) \\(\\frac{0.12 - 0.08}{0.0243} = 1.65\\) SEs away mean, considered unusual. (d) True. \\(\\hat{p}=0.12\\) 2.32 standard errors away mean, often considered unusual. (e) False. Decreases SE factor \\(1/\\sqrt{2}\\).()  True. See reasoning 6.1(b). (b) True. take square root sample size SE formula. (c) True. independence success-failure conditions satisfied. (d) True. independence success-failure conditions satisfied.()  False. confidence interval constructed estimate population proportion, sample proportion. (b) True. 95% CI: \\(82\\%\\ \\pm\\ 2\\%\\). (c) True. definition confidence level. (d) True. Quadrupling sample size decreases SE factor \\(1/\\sqrt{4}\\). (e) True. 95% CI entirely 50%.random sample, independence satisfied. success-failure condition also satisfied. \\(= z^{\\star} \\sqrt{ \\frac{\\hat{p} (1-\\hat{p})} {n} } = 1.96 \\sqrt{ \\frac{0.56 \\times 0.44}{600} }= 0.0397 \\approx 4\\%.\\)()  . sample represents students took SAT, also online survey. (b) (0.5289, 0.5711). 90% confident 53% 57% high school seniors took SAT fairly certain participate study abroad program college. (c) 90% random samples produce 90% confidence interval includes true proportion. (d) Yes. interval lies entirely 50%.()  want check majority (minority), use following hypotheses: \\(H_0: p = 0.5\\) \\(H_A: p \\neq 0.5\\). sample proportion \\(\\hat{p} = 0.55\\) sample size \\(n = 617\\) independents. Since random sample, independence satisfied. success-failure condition also satisfied: \\(617 \\times 0.5\\) \\(617 \\times (1 - 0.5)\\) least 10 (use null proportion \\(p_0 = 0.5\\) check one-proportion hypothesis test). Therefore, can model \\(\\hat{p}\\) using normal distribution standard error \\(SE = \\sqrt{\\frac{p(1 - p)}{n}} = 0.02\\). (use null proportion \\(p_0 = 0.5\\) compute standard error one-proportion hypothesis test.) Next, compute test statistic: \\(Z = \\frac{0.55 - 0.5}{0.02} = 2.5.\\) yields one-tail area 0.0062, p-value \\(2 \\times 0.0062 = 0.0124.\\) p-value smaller 0.05, reject null hypothesis. strong evidence support different 0.5, since data provide point estimate 0.5, strong evidence support claim TV pundit. (b) . Generally expect hypothesis test confidence interval align, expect confidence interval show range plausible values entirely 0.5. However, confidence level misaligned (e.g., 99% confidence level \\(\\alpha = 0.05\\) significance level), longer generally true.()  \\(H_0: p = 0.5\\). \\(H_A: p > 0.5\\). Independence (random sample, \\(<10\\%\\) population) satisfied, success-failure conditions (using \\(p_0 = 0.5\\), expect 40 successes 40 failures). \\(Z = 2.91\\) \\(\\\\) p- value \\(= 0.0018\\). Since p-value \\(< 0.05\\), reject null hypothesis. data provide strong evidence rate correctly identifying soda people significantly better just random guessing. (b) fact people tell difference diet regular soda randomly guess, probability getting random sample 80 people 53 identify soda correctly 0.0018.() sample computer chips manufactured factory week production. might tempted generalize population represent weeks, exercise caution since rate defects may change time. (b) fraction computer chips manufactured factory week production defects. (c) Estimate parameter using data: \\(\\hat{p} = \\frac{27}{212} = 0.127\\). (d) Standard error (\\(SE\\)). (e) Compute \\(SE\\) using \\(\\hat{p} = 0.127\\) place \\(p\\): \\(SE \\approx \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} = \\sqrt{\\frac{0.127(1 - 0.127)}{212}} = 0.023\\). (f) standard error standard deviation \\(\\hat{p}\\). value 0.10 one standard error away observed value, represent uncommon deviation. (Usually beyond 2 standard errors good rule thumb.) engineer surprised. (g) Recomputed standard error using \\(p = 0.1\\): \\(SE = \\sqrt{\\frac{0.1(1 - 0.1)}{212}} = 0.021\\). value isn’t different, typical standard error computed using relatively similar proportions (even sometimes proportions quite different!).() visitors simple random sample, independence satisfied. success-failure condition also satisfied, 64 \\(752 - 64 = 688\\) 10. Therefore, can use normal distribution model \\(\\hat{p}\\) construct confidence interval. (b) sample proportion \\(\\hat{p} = \\frac{64}{752} = 0.085\\). standard error \\(SE = \\sqrt{\\frac{0.085 (1 - 0.085)}{752}} = 0.010.\\) (c) 90% confidence interval, use \\(z^{\\star} = 1.65\\). confidence interval \\(0.085 \\pm 1.65 \\times 0.010 \\(0.0685, 0.1015)\\). 90% confident 6.85% 10.15% first-time site visitors register using new design.","code":""},{"path":"install.html","id":"install","chapter":"16 Installing R and RStudio","heading":"16 Installing R and RStudio","text":"section adapted books Data Science Education Using R117 Epidemiologist R Handbook118","code":""},{"path":"install.html","id":"why-use-r","chapter":"16 Installing R and RStudio","heading":"16.1 Why use R?","text":"stated R project website, R programming language environment statistical computing graphics. highly versatile, extendable, community-driven.CostR free use! strong ethic community free open-source material.ReproducibilityConducting data management analysis programming language (compared Excel another primarily point-click/manual tool) enhances reproducibility, makes error-detection easier, eases workload.CommunityThe R community users enormous collaborative. New packages tools address real-life problems developed daily, vetted community users. one example, R-Ladies worldwide organization whose mission promote gender diversity R community, one largest organizations R users. likely chapter near !","code":""},{"path":"install.html","id":"downloading-r-rstudio-and-latex","chapter":"16 Installing R and RStudio","heading":"16.2 Downloading R, RStudio and LaTeX","text":"First, need download latest versions R119 RStudio.120\nR free environment statistical computing graphics using programming language R.\nRStudio set integrated tools allows user-friendly experience using R.Although likely use RStudio main console editor, must first install R RStudio uses R behind scenes. R RStudio freely available, cross-platform, open-source.Permissions\nNote install R RStudio drive read write permissions. Otherwise, ability install R packages (frequent occurrence) impacted. encounter problems, try opening RStudio right-clicking icon selecting “Run administrator”. tips can found page [R network drives].","code":""},{"path":"install.html","id":"to-download-r","chapter":"16 Installing R and RStudio","heading":"16.2.1 To Download R:","text":"Visit CRAN (https://cran.r-project.org/) download RFind operating system (Mac, Windows, Linux)Select “latest release” page operating systemDownload install applicationDon’t worry; mess anything download (even install!) wrong file. ’ve installed R, can get started.","code":""},{"path":"install.html","id":"to-download-rstudio","chapter":"16 Installing R and RStudio","heading":"16.2.2 To Download RStudio:","text":"Visit RStudio’s website (https://www.rstudio.com/products/rstudio/download/) download RStudioUnder column called “RStudio Desktop FREE”, click “Download”Find operating system (Mac, Windows, Linux)Select “latest release” page operating systemDownload install applicationIf issues, excellent place get help RStudio Community forums (https://community.rstudio.com/).","code":""},{"path":"install.html","id":"to-download-latex","chapter":"16 Installing R and RStudio","heading":"16.2.3 To Download LaTeX:","text":"TinyTex custom LaTeX distribution, useful trying produce PDFs R.\nSee https://yihui.org/tinytex/ informaton.install TinyTex R:","code":"\ninstall.packages('tinytex')\ntinytex::install_tinytex()\n# to uninstall TinyTeX, run tinytex::uninstall_tinytex()"},{"path":"basics.html","id":"basics","chapter":"17 Basics of R and Rstudio","heading":"17 Basics of R and Rstudio","text":"section adapted book Data Science Education Using R121","code":""},{"path":"basics.html","id":"rstudio-layout-and-customization-getting-to-know-r-through-rstudio","chapter":"17 Basics of R and Rstudio","heading":"17.1 RStudio Layout and Customization: Getting to Know R through RStudio","text":"Now ’ve installed R RStudio, accessing R RStudio.\nOne reliable ways tell ’re opening R RStudio look icons:\nFigure 17.1: Icons\nWhenever want work R, ’ll open RStudio. RStudio interfaces directly R, Integrated Development Environment (IDE). means RStudio comes built-features make using R little easier.use RStudio access R, many people don’t!IDEs work R include:Jupyter notebook (https://jupyter.org/)VisualStudio (https://visualstudio.microsoft.com/services/visual-studio-online/)VIM (https://github.com/jalvesaq/Nvim-R)IntelliJ IDEA (https://plugins.jetbrains.com/plugin/6632-r-language--intellij)EMACS Speaks Statistics (ESS) (https://ess.r-project.org/)non-exhaustive list, options require good deal familiarity given IDE.\nHowever bring alternative IDEs—particularly ESS—RStudio, writing, fully accessible learners utilize screen readers. chosen use RStudio text order standardize experience, encourage choose IDE best suits needs!","code":""},{"path":"basics.html","id":"rstudio-layout","chapter":"17 Basics of R and Rstudio","heading":"17.1.1 RStudio Layout","text":"open RStudio first time, default RStudio displays four rectangle panes:RStudio displays one left pane scripts open yet.Source Pane\npane, default upper-left, space edit, run, save scripts. Scripts contain commands want run. pane can also display datasets (data frames) viewing.Stata users, pane similar -file Data Editor windows.R Console PaneThe R Console, default left lower-left pane R Studio, home R “engine”. commands actually run non-graphic outputs error/warning messages appear. can directly enter run commands R Console, realize commands saved running commands script.familiar Stata, R Console like Command Window also Results Window.Environment Pane\npane, default upper-right, often used see brief summaries objects R Environment current session. objects include imported, modified, created datasets, parameters defined, vectors lists defined analysis (e.g. names regions). can click arrow next data frame name see variables.Stata, similar Variables Manager window.pane also contains History can see commands can previously. also “Tutorial” tab can complete interactive R tutorials learnr package installed. also “Connections” pane external connections, can “Git” pane choose interface Github.Plots, Viewer, Packages, Help Pane\nlower-right pane includes several important tabs. Typical plot graphics including maps display Plot pane. Interactive HTML outputs display Viewer pane. Help pane can display documentation help files. Files pane browser can used open delete files. Packages pane allows see, install, update, delete, load/unload R packages, see version package . learn packages see Section 20.pane contains Stata equivalents Plots Manager Project Manager windows.work R , ’ll find using tabs within panes.can open .R script source pane going “File”, selecting “New File”, selecting “R Script”:\nFigure 17.2: Creating New R Script RStudio\nneed anything specific file, encourage experiment like!","code":""},{"path":"basics.html","id":"customizing-rstudio","chapter":"17 Basics of R and Rstudio","heading":"17.1.2 Customizing RStudio","text":"One balances ’ve tried strike text balance best practices workflow (’ll use R projects) R code. best practice workflow ensure ’re starting blank slate every time open R (RStudio).\naccomplish , go “Tools”, select “Global Options” dropdown menu.\nFigure 17.3: Selecting Global Options Tool Dropdown Menu\n“General” tab open, several checkboxes selected unselected. important thing can select “Never” next “Save workspace .RData exit:” prompt. selecting “Never”, go check uncheck boxes General tab looks like :\nFigure 17.4: General Tab Global Options\nbecome clear good idea Section 18.Last, certainly least, click “Appearance” tab within Global Options. can select RStudio Font, Font Size, Theme. Go options select appearance works best , know can always come back change !","code":""},{"path":"basics.html","id":"minimized-and-missing-panes","chapter":"17 Basics of R and Rstudio","heading":"17.1.3 Minimized and Missing Panes","text":", point, find one panes seems “disappeared”, one two things likely happened:pane minimizedA pane closedLet’s look Environment pane example.\nEnvironment pane minimized, ’ll see something like :\nFigure 17.5: RStudio Layout Environment Pane Minimized\nknow Environment pane minimized although can see pane headers top right, can’t see information within Environment pane. fix , can click icon two squares top right Environment pane. click icon large square top right Environment pane, ’ll maximize Environment pane minimize Files pane. want , since prefer see panes .Environment pane somehow closed, can recover going “View” menu, selecting “Panes”, selecting “Pane Layout”, like :\nFigure 17.6: Accessing Pane Layout View Dropdown Menu\nselect Pane Layout, ’ll see :\nFigure 17.7: Pane Layout Options within RStudio\n, can select tabs ’d like appear within pane, can even change pane appears within RStudio. Environment Pane closed, select Pane Layout order re-open within RStudio.Restart\nR freezes, can re-start R going Session menu clicking “Restart R”. avoids hassle closing opening RStudio. Everything R environment removed .","code":""},{"path":"basics.html","id":"writing-and-running-code-in-rstudio","chapter":"17 Basics of R and Rstudio","heading":"17.2 Writing and Running Code in RStudio","text":"point, ’ve exploring RStudio interface setting preferences.\nNow, ’ll shift basic coding practices.\norder run code R, need type code either Console within .R script.generally recommend creating .R script ’re learning, allows type code, add comments, save .R script reference. instead work entirely Console, anything type Console disappear soon restart close R, able reference future.","code":""},{"path":"basics.html","id":"running-code-in-the-console","chapter":"17 Basics of R and Rstudio","heading":"17.2.1 Running Code in the Console","text":"run code Console, type code next > hit Enter.\n’ll spend little time practicing running code Console exploring basic properties coding R.Console, type 3 + 4 hit Enter.\nsee following:\nFigure 17.8: Using Console Calculator\n’ve just used R add numbers 3 4.\nR returned sum 3 + 4 new line, next [1].\n[1] tells us one row data.can also use R print text.\nType following Console hit Enter:see Console:\nFigure 17.9: Printing Text Console\n’s one error ’re likely going come across, running code Console well R script.\nLet’s explore error now running following code Console hitting Enter:Make sure left closing parenthesis!\n’ll see Console :\nFigure 17.10: Incomplete Parentheses Change R Expects Next\n’re missing closing parenthesis, R expecting us provide code.\nknow instead seeing carat > Console, see +, R returned print statement expecting!\ntwo ways fix problem:Type closing ) Console hit EnterHit Esc keyGo ahead run intentional error, try options .\nCompare output , think ’re different.\nCan think might want use one option instead ?","code":"\nprint(\"I am learning R\")print(\"This is going to cause a problem\""},{"path":"basics.html","id":"running-code-in-an-r-script","chapter":"17 Basics of R and Rstudio","heading":"17.2.2 Running Code in an R Script","text":"key using script editor effectively memorize one important keyboard shortcuts: Cmd + Enter Mac Ctrl + Enter otherwise. executes current R expression console.\nexample, take code .\ncursor █, pressing Cmd/Ctrl + Enter run complete command generates fit.\nalso move cursor next statement (summary(fit)).\nmakes easy run complete script repeatedly pressing Cmd/Ctrl + Enter.Instead running expression--expression, can also execute complete script one step: Cmd/Ctrl + Shift + S.\nregularly great way check ’ve captured important parts code script.recommend always start script packages need.\nway, share code others, can easily see packages need install.\nNote, however, never include install.packages() setwd() script share.\n’s antisocial change settings someone else’s computer!working assignments research projects, highly recommend starting editor practicing keyboard shortcuts.\ntime, sending code console way become natural won’t even think .","code":"data(\"mtcars\")\n\nfit <- lm(mpg ~ disp + hp + drat, \n          data = mtcars, █\n          weights = runif(nrow(mtcars)))\n\nsummary(fit)"},{"path":"basics.html","id":"rstudio-diagnostics","chapter":"17 Basics of R and Rstudio","heading":"17.2.3 RStudio diagnostics","text":"script editor also highlight syntax errors red squiggly line cross sidebar:Hover cross see problem :RStudio also let know potential problems:","code":""},{"path":"basics.html","id":"commenting-your-code-in-r","chapter":"17 Basics of R and Rstudio","heading":"17.2.4 Commenting Your Code in R","text":"considered good practice comment code working .R script.\nEven person ever work code, can helpful write notes trying specific piece code.\nMoreover, writing comments code work examples book great way help reinforce ’re learning.\nComments ignored R running script, affect code analysis.comment line code, can place pound sign (also called octothorpe) # front line code want exclude ’re running script.\ncareful excluding certain lines code, especially longer files, can easy forget ’ve commented code.\noften better simply start new section code tinker get working expected, rather commenting individual lines code.can also write comments -line code, like :think ’ll writing one line comments, can pound sign followed single quotation mark (#').\ncontinue comment lines text code time hit Enter.\ncan delete #' new line want write code R run.\nmethod useful ’re writing long description ’re R.refer “commenting” ’re referring adding actual text comments, whereas “commenting ” refers using pound sign (octothorpe) front line code R ignores . also use phrase “uncomment code”, means delete (omit typing ) # #' example.","code":"\n#' this will be a short code example.\n#' you are not expected to know what this does,\n#' nor do you need to try running it on your computer.\nlibrary(readr)  # load the readr package\nlibrary(here)  # load the here package\ndata <- read_csv(here(\"file_path\", \"file_name.csv\"))  # save file_name.csv as data"},{"path":"basics.html","id":"autocomplete-to-avoid-coding-mistakes","chapter":"17 Basics of R and Rstudio","heading":"17.3 Autocomplete to avoid coding mistakes","text":"errors undoubtedly happen several students every time teach course. ’s always mistake filename, file doesn’t exist current working directory, function argument misspelled. might makes sense point, please just get habit using Tab button. demo shown Figure 17.13.\nFigure 17.11: Tab button Mac keyboard\n\nFigure 17.12: Tab button keyboards\nUse Tab key typing engage RStudio’s auto-complete functionality. can prevent spelling errors, remind function arguments, easily find file paths. Press Tab typing produce drop-menu likely functions objects, based typed far. Figure 17.13, show three commands use Tab functionality. common use cases.\nFigure 17.13: Autocomplete RStudio using Tab button. First line: first type lib hit Tab. scroll (using arrow key) search library function, hit Enter. type sp hit Tab . scroll (using arrow key) search splines library. hit Enter. two lines follow strategy. encourage try .\n","code":"\nread.csv(\"myfile.csv\")\n#> Error in file(file, \"rt\"): cannot open the connection\nmean(y = 1:5)\n#> Error in mean.default(y = 1:5): argument \"x\" is missing, with no default"},{"path":"basics.html","id":"keyboard-shortcuts","chapter":"17 Basics of R and Rstudio","heading":"17.4 Keyboard shortcuts","text":"useful keyboard shortcuts . See keyboard shortcuts Windows, Max, Linux second page RStudio user interface cheatsheet.","code":""},{"path":"basics.html","id":"resources-for-learning-r-and-rstudio","chapter":"17 Basics of R and Rstudio","heading":"17.5 Resources for learning R and Rstudio","text":"many opportunities formally learn R EPIB613. course meant provide necessary coding background EPIB607. section provides links several additional learning resources, finishes thoughts difficulty learning R ","code":""},{"path":"basics.html","id":"help-documentation","chapter":"17 Basics of R and Rstudio","heading":"17.5.1 Help documentation","text":"Functions, datasets, built-objects R documented help system. Search RStudio “Help” tab documentation R packages specific functions. within pane also contains Files, Plots, Packages (typically lower-right pane). shortcut, can also type name package function R console question-mark open relevant Help page. include parentheses.example: ?mean ?plot help(plot).quality R’s help pages varies somewhat. tend terse side. However, essentially structure useful know read . Figure 17.14 provides overview look . Remember, functions take inputs, perform actions, return outputs. Something goes , gets worked , something comes . means want know function requires, , returns. requires shown Usage Arguments sections help page. names required optional arguments given name order function expects . arguments default values. case mean() function argument na.rm set FALSE default. shown Usage section. named argument default, give value. Depending argument , might logical value, number, dataset, object.\nFigure 17.14: read help page.122\n","code":""},{"path":"basics.html","id":"online-forums","chapter":"17 Basics of R and Rstudio","heading":"17.5.2 Online Forums","text":"community driven forums hundreds thousands users. looking help page, next thing find help.StackOverflow (Q&site hundreds thousands answers sorts programming questions)RStudio Community (forum specifically designed people using RStudio tidyverse.Searching help R Google can sometimes tricky program name single letter. Google generally smart enough figure mean search “r scatterplot”, struggle, try searching “rstats” instead (e.g. “rstats scatterplot”).","code":""},{"path":"basics.html","id":"cheatsheets","chapter":"17 Basics of R and Rstudio","heading":"17.5.3 Cheatsheets","text":"many PDF “cheatsheets” available RStudio website. particular interest course :Base RRStudio IDEDynamic documents rmarkdownData visualization ggplot2 cheatsheetData transformation dplyr cheatsheetData import readr, readxl, googlesheets4 cheatsheetData tidying tidyr cheatsheet","code":""},{"path":"basics.html","id":"twitter","chapter":"17 Basics of R and Rstudio","heading":"17.5.4 Twitter","text":"R vibrant twitter community can learn tips, shortcuts, news - follow accounts:R Function Day @rfuntionaday incredible resourceR Data Science @rstats4dsRStudio @RStudioRStudio Tips @rstudiotipsR-Bloggers @RbloggersR-ladies @RLadiesGlobalYou can post R-related questions content #rstats. community exceptionally generous helpful.","code":""},{"path":"basics.html","id":"exploring-r-with-the-swirl-package","chapter":"17 Basics of R and Rstudio","heading":"17.5.5 Exploring R with the {swirl} Package","text":"addition Datacamp courses needed complete A1, can supplement learning {swirl} (https://swirlstats.com/students.html). Students previous years found helpful. can install {swirl} running following code:{swirl} set packages can download, providing interactive method learning R using R RStudio Console.\nSince ’ve already installed R, RStudio, {swirl} package, can follow instructions {swirl} webpage run following code Console pane get started beginner-level course {swirl}:multiple courses available {swirl}, can access installing running swirl() command console.","code":"\ninstall.packages(\"swirl\")\nlibrary(swirl)\ninstall_course(\"R_Programming_E\")\nswirl()"},{"path":"basics.html","id":"free-online-resources","chapter":"17 Basics of R and Rstudio","heading":"17.5.6 Free online resources","text":"R Data ScienceRMarkdown CookbookHands-Programming RData Visualization RFundamentals Data VisualizationModern Data Science RBig Book RMateriales de RStudio en EspañolIntroduction à R et au tidyverse (Francais)","code":""},{"path":"basics.html","id":"learning-r-can-be-difficult","chapter":"17 Basics of R and Rstudio","heading":"17.5.7 Learning R can be difficult","text":"Learning R can difficult first—’s like learning new language, just like Spanish, French, Chinese. Hadley Wickham—chief data scientist RStudio author amazing R packages ’ll using like ggplot2—made wise observation:’s easy start programming get really frustrated think, “Oh ’s , ’m really stupid,” , “’m made program.” , absolutely case. Everyone gets frustrated. still get frustrated occasionally writing R code. ’s just natural part programming. , happens everyone gets less less time. Don’t blame . Just take break, something fun, come back try later.","code":""},{"path":"basics.html","id":"whats-next","chapter":"17 Basics of R and Rstudio","heading":"17.6 What’s next","text":"next four Chapters designed give skills knowledge necessary get started programming task. yet installed R /RStudio, please go steps outlined Chapter 16 beginning next three.Please note Chapters intended full complete introduction programming R using R data science. Datacamp courses need complete A1 well EPIB613 provide tools program data science related tasks. point Chapters establish good programming habits even start coding. gathered resources succinct reference find useful successful course. four core concepts :ProjectsFunctionsPackagesImporting data","code":""},{"path":"projects.html","id":"projects","chapter":"18 RStudio Projects","heading":"18 RStudio Projects","text":"Chapter adapted book R Data Science.123","code":""},{"path":"projects.html","id":"overview","chapter":"18 RStudio Projects","heading":"18.1 Overview","text":"One first steps every workflow set “Project” within RStudio.\nProject home files, images, reports, code used given project.Note capitalize word “Project”, ’re referring specific setup within RStudio, refer general projects might work lowercase “project”.use Projects create self-contained folder given analysis R.\nmeans want share Project colleague, reset file paths (even know anything file paths!) order re-run analysis.\nFurthermore, even person ever collaborate future version , using Project analyses mean can move Project folder around computer, even move new computer, remain confident analysis run future (least terms file path structures).One day need quit R, go something else return analysis next day.\nOne day working multiple analyses simultaneously use R want keep separate.\nOne day need bring data outside world R send numerical results figures R back world.handle real life situations, need make two decisions:analysis “real”, .e. save lasting record happened?analysis “real”, .e. save lasting record happened?analysis “live”?analysis “live”?","code":""},{"path":"projects.html","id":"what-is-real","chapter":"18 RStudio Projects","heading":"18.2 What is real?","text":"beginning R user, ’s OK consider environment (.e. objects listed environment pane) “real”.\nHowever, long run, ’ll much better consider R scripts “real”.R scripts (data files), can recreate environment.\n’s much harder recreate R scripts environment!\n’ll either retype lot code memory (making mistakes way) ’ll carefully mine R history.foster behaviour, highly recommend instruct RStudio preserve workspace sessions:cause short-term pain, now restart RStudio remember results code ran last time.\nshort-term pain save long-term agony forces capture important interactions code.\n’s nothing worse discovering three months fact ’ve stored results important calculation workspace, calculation code.great pair keyboard shortcuts work together make sure ’ve captured important parts code editor:Press Cmd/Ctrl + Shift + F10 restart RStudio.Press Cmd/Ctrl + Shift + S rerun current script.use pattern hundreds times week.","code":""},{"path":"projects.html","id":"where-does-your-analysis-live","chapter":"18 RStudio Projects","heading":"18.3 Where does your analysis live?","text":"R powerful notion working directory.\nR looks files ask load, put files ask save.\nRStudio shows current working directory top console:can print R code running getwd():Evan beginning R user organize analytical projects directories , working project, setting R’s working directory associated directory.recommend , can also set working directory within R:never ’s better way; way also puts path managing R work like expert.","code":"\ngetwd()\n#> [1] \"/Users/runner/work/EPIB607/EPIB607\"\nsetwd(\"/path/to/my/CoolProject\")"},{"path":"projects.html","id":"paths-and-directories","chapter":"18 RStudio Projects","heading":"18.4 Paths and directories","text":"Paths directories little complicated two basic styles paths: Mac/Linux Windows.\nthree chief ways differ:important difference separate components path.\nMac Linux uses slashes (e.g. plots/diamonds.pdf) Windows uses backslashes (e.g. plots\\diamonds.pdf).\nR can work either type (matter platform ’re currently using), unfortunately, backslashes mean something special R, get single backslash path, need type two backslashes!\nmakes life frustrating, recommend always using Linux/Mac style forward slashes.important difference separate components path.\nMac Linux uses slashes (e.g. plots/diamonds.pdf) Windows uses backslashes (e.g. plots\\diamonds.pdf).\nR can work either type (matter platform ’re currently using), unfortunately, backslashes mean something special R, get single backslash path, need type two backslashes!\nmakes life frustrating, recommend always using Linux/Mac style forward slashes.Absolute paths (.e. paths point place regardless working directory) look different.\nWindows start drive letter (e.g. C:) two backslashes (e.g. \\\\servername) Mac/Linux start slash “/” (e.g. /home/sahir/).\nnever use absolute paths scripts, hinder sharing: one else exactly directory configuration .Absolute paths (.e. paths point place regardless working directory) look different.\nWindows start drive letter (e.g. C:) two backslashes (e.g. \\\\servername) Mac/Linux start slash “/” (e.g. /home/sahir/).\nnever use absolute paths scripts, hinder sharing: one else exactly directory configuration .last minor difference place ~ points .\n~ convenient shortcut home directory. example ~/git_repositories/ equivalent /home/sahir/git_repositories.\nWindows doesn’t really notion home directory, instead points documents directory.last minor difference place ~ points .\n~ convenient shortcut home directory. example ~/git_repositories/ equivalent /home/sahir/git_repositories.\nWindows doesn’t really notion home directory, instead points documents directory.","code":""},{"path":"projects.html","id":"how-to-create-an-rstudio-project","chapter":"18 RStudio Projects","heading":"18.5 How to create an RStudio Project","text":"R experts keep files associated project together — input data, R scripts, analytical results, figures.\nwise common practice RStudio built-support via Projects.Let’s make Project:\nClick File > New Project, :Call Project r4ds think carefully subdirectory put project .\ndon’t store somewhere sensible, hard find future!EPIB607, highly recommend creating new Project assignmentOnce process complete, ’ll get new RStudio Project.\nCheck “home” directory project current working directory:Whenever refer file relative path look .Now enter following commands script editor, save file, calling “diamonds.R”.\nNext, run complete script save PDF CSV file project directory.\nDon’t worry script actually , ’ll learn throughout course EPIB613.Quit RStudio.\nInspect folder associated Project — notice .Rproj file.\nDouble-click file re-open Project.\nNotice get back left : ’s working directory command history, files working still open.\nfollowed instructions , , however, completely fresh environment, guaranteeing ’re starting clean slate.favorite OS-specific way, search computer diamonds.pdf find PDF (surprise) also script created (diamonds.R).\nhuge win!\nOne day want remake figure just understand came .\nrigorously save figures files R code never mouse clipboard, able reproduce old work ease!","code":"\ngetwd()\n#> [1] /home/sahir/git_repositories/r4ds\nlibrary(tidyverse)\n\nggplot(diamonds, aes(carat, price)) + \n  geom_hex()\nggsave(\"diamonds.pdf\")\n\nwrite_csv(diamonds, \"diamonds.csv\")"},{"path":"projects.html","id":"summary-8","chapter":"18 RStudio Projects","heading":"18.6 Summary","text":"summary, RStudio Projects give solid workflow serve well future:Create RStudio Project data analysis project.Create RStudio Project data analysis project.Keep data files ; ’ll talk loading R [data import].Keep data files ; ’ll talk loading R [data import].Keep scripts ; edit , run bits whole.Keep scripts ; edit , run bits whole.Save outputs (plots cleaned data) .Save outputs (plots cleaned data) .ever use relative paths, absolute paths.ever use relative paths, absolute paths.Everything need one place, cleanly separated projects working .","code":""},{"path":"functionsHOP.html","id":"functionsHOP","chapter":"19 Functions","heading":"19 Functions","text":"section reproduced R Data Science.124","code":""},{"path":"functionsHOP.html","id":"introduction-2","chapter":"19 Functions","heading":"19.1 Introduction","text":"function reusable piece code allows us consistently repeat programming task. Functions R can identified word followed set parentheses, like : word().\noften , word verb, dplyr::filter(), suggesting ’re perform action. Indeed, functions act like verbs: tell R data.word (set words) represents name function, parentheses can provide arguments function, arguments needed function run.\nMany functions R packages require arguments, use set default arguments unless provide something different default.\nhard fast rules function needs argument (series arguments). However, trouble running code, first check typos, check Help documentation see can provide arguments clearly direct R .One best ways improve reach data scientist write functions.\nFunctions allow automate common tasks powerful general way copy--pasting.\nWriting function three big advantages using copy--paste:can give function evocative name makes code easier understand.can give function evocative name makes code easier understand.requirements change, need update code one place, instead many.requirements change, need update code one place, instead many.eliminate chance making incidental mistakes copy paste (.e. updating variable name one place, another).eliminate chance making incidental mistakes copy paste (.e. updating variable name one place, another).Writing good functions lifetime journey.\nEven using R many years still learn new techniques better ways approaching old problems.\ngoal Chapter teach every esoteric detail functions get started pragmatic advice can apply immediately.well practical advice writing functions, chapter also gives suggestions style code.\nGood code style like correct punctuation.\nYoucanmanagewithoutit, sure makes things easier read!\nstyles punctuation, many possible variations.\npresent style use code, important thing consistent","code":""},{"path":"functionsHOP.html","id":"when-should-you-write-a-function","chapter":"19 Functions","heading":"19.2 When should you write a function?","text":"consider writing function whenever ’ve copied pasted block code twice (.e. now three copies code).\nexample, take look code.\n?might able puzzle rescales column range 0 1.\nspot mistake?\nmade error copying--pasting code df$b: forgot change b.\nExtracting repeated code function good idea prevents making type mistake.write function need first analyse code.\nmany inputs ?code one input: df$.\n(’re surprised TRUE input, can explore exercise .) make inputs clear, ’s good idea rewrite code using temporary variables general names.\ncode requires single numeric vector, ’ll call x:duplication code.\n’re computing range data three times, makes sense one step:Pulling intermediate calculations named variables good practice makes clear code .\nNow ’ve simplified code, checked still works, can turn function:three key steps creating new function:need pick name function.\n’ve used rescale01 function rescales vector lie 0 1.need pick name function.\n’ve used rescale01 function rescales vector lie 0 1.list inputs, arguments, function inside function.\njust one argument.\ncall look like function(x, y, z).list inputs, arguments, function inside function.\njust one argument.\ncall look like function(x, y, z).place code developed body function, { block immediately follows function(...).place code developed body function, { block immediately follows function(...).Note overall process: made function ’d figured make work simple input.\n’s easier start working code turn function; ’s harder create function try make work.point ’s good idea check function different inputs:write functions ’ll eventually want convert informal, interactive tests formal, automated tests.\nprocess called unit testing.\nUnfortunately, ’s beyond scope course, can learn http://r-pkgs..co.nz/tests.html.can simplify original example now function:Compared original, code easier understand ’ve eliminated one class copy--paste errors.\nstill quite bit duplication since ’re thing multiple columns.\nloops good starting point reduce duplication even .Another advantage functions requirements change, need make change one place.\nexample, might discover variables include infinite values, rescale01() fails:’ve extracted code function, need make fix one place:important part “repeat ” (DRY) principle.\nrepetition code, places need remember update things change (always !), likely create bugs time.","code":"\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n(df$a - min(df$a, na.rm = TRUE)) /\n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\nx <- df$a\n(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n#>  [1] 0.31039222 0.09481225 0.53792476 0.11810987 0.38864219\n#>  [6] 0.33749979 0.62280687 0.00000000 0.48461314 1.00000000\nrng <- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n#>  [1] 0.31039222 0.09481225 0.53792476 0.11810987 0.38864219\n#>  [6] 0.33749979 0.62280687 0.00000000 0.48461314 1.00000000\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(c(0, 5, 10))\n#> [1] 0.0 0.5 1.0\nrescale01(c(-10, 0, 10))\n#> [1] 0.0 0.5 1.0\nrescale01(c(1, 2, 3, NA, 5))\n#> [1] 0.00 0.25 0.50   NA 1.00\ndf$a <- rescale01(df$a)\ndf$b <- rescale01(df$b)\ndf$c <- rescale01(df$c)\ndf$d <- rescale01(df$d)\nx <- c(1:10, Inf)\nrescale01(x)\n#>  [1]   0   0   0   0   0   0   0   0   0   0 NaN\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(x)\n#>  [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444\n#>  [6] 0.5555556 0.6666667 0.7777778 0.8888889 1.0000000\n#> [11]       Inf"},{"path":"functionsHOP.html","id":"exercises-4","chapter":"19 Functions","heading":"19.2.1 Exercises","text":"TRUE parameter rescale01()?\nhappen x contained single missing value, na.rm FALSE?TRUE parameter rescale01()?\nhappen x contained single missing value, na.rm FALSE?second variant rescale01(), infinite values left unchanged.\nRewrite rescale01() -Inf mapped 0, Inf mapped 1.second variant rescale01(), infinite values left unchanged.\nRewrite rescale01() -Inf mapped 0, Inf mapped 1.Practice turning following code snippets functions.\nThink function .\ncall ?\nmany arguments need?\nCan rewrite expressive less duplicative?\n\nmean(.na(x))\n\nx / sum(x, na.rm = TRUE)\n\nsd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)Practice turning following code snippets functions.\nThink function .\ncall ?\nmany arguments need?\nCan rewrite expressive less duplicative?Write functions compute variance skewness numeric vector.\nVariance defined \\[\n\\mathrm{Var}(x) = \\frac{1}{n - 1} \\sum_{=1}^n (x_i - \\bar{x}) ^2 \\text{,}\n\\] \\(\\bar{x} = (\\sum_i^n x_i) / n\\) sample mean.\nSkewness defined \\[\n\\mathrm{Skew}(x) = \\frac{\\frac{1}{n-2}\\left(\\sum_{=1}^n(x_i - \\bar x)^3\\right)}{\\mathrm{Var}(x)^{3/2}} \\text{.}\n\\]Write functions compute variance skewness numeric vector.\nVariance defined \\[\n\\mathrm{Var}(x) = \\frac{1}{n - 1} \\sum_{=1}^n (x_i - \\bar{x}) ^2 \\text{,}\n\\] \\(\\bar{x} = (\\sum_i^n x_i) / n\\) sample mean.\nSkewness defined \\[\n\\mathrm{Skew}(x) = \\frac{\\frac{1}{n-2}\\left(\\sum_{=1}^n(x_i - \\bar x)^3\\right)}{\\mathrm{Var}(x)^{3/2}} \\text{.}\n\\]Write both_na(), function takes two vectors length returns number positions NA vectors.Write both_na(), function takes two vectors length returns number positions NA vectors.following functions ?\nuseful even though short?\n\nis_directory <- function(x) file.info(x)$isdir\nis_readable <- function(x) file.access(x, 4) == 0What following functions ?\nuseful even though short?Read complete lyrics “Little Bunny Foo Foo”.\n’s lot duplication song.\nExtend initial piping example recreate complete song, use functions reduce duplication.Read complete lyrics “Little Bunny Foo Foo”.\n’s lot duplication song.\nExtend initial piping example recreate complete song, use functions reduce duplication.","code":"\nmean(is.na(x))\n\nx / sum(x, na.rm = TRUE)\n\nsd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)\nis_directory <- function(x) file.info(x)$isdir\nis_readable <- function(x) file.access(x, 4) == 0"},{"path":"functionsHOP.html","id":"functions-are-for-humans-and-computers","chapter":"19 Functions","heading":"19.3 Functions are for humans and computers","text":"’s important remember functions just computer, also humans.\nR doesn’t care function called, comments contains, important human readers.\nsection discusses things bear mind writing functions humans can understand.name function important.\nIdeally, name function short, clearly evoke function .\n’s hard!\n’s better clear short, RStudio’s autocomplete makes easy type long names.Generally, function names verbs, arguments nouns.\nexceptions: nouns ok function computes well known noun (.e. mean() better compute_mean()), accessing property object (.e. coef() better get_coefficients()).\ngood sign noun might better choice ’re using broad verb like “get”, “compute”, “calculate”, “determine”.\nUse best judgement don’t afraid rename function figure better name later.function name composed multiple words, recommend using “snake_case”, lowercase word separated underscore.\ncamelCase popular alternative.\ndoesn’t really matter one pick, important thing consistent: pick one stick .\nR consistent, ’s nothing can .\nMake sure don’t fall trap making code consistent possible.family functions similar things, make sure consistent names arguments.\nUse common prefix indicate connected.\n’s better common suffix autocomplete allows type prefix see members family.good example design stringr package: don’t remember exactly function need, can type str_ jog memory.possible, avoid overriding existing functions variables.\n’s impossible general many good names already taken packages, avoiding common names base R avoid confusion.Use comments, lines starting #, explain “” code.\ngenerally avoid comments explain “” “”.\ncan’t understand code reading , think rewrite clear.\nneed add intermediate variables useful names?\nneed break subcomponent large function can name ?\nHowever, code can never capture reasoning behind decisions: choose approach instead alternative?\nelse try didn’t work?\n’s great idea capture sort thinking comment.Another important use comments break file easily readable chunks.\nUse long lines - = make easy spot breaks.RStudio provides keyboard shortcut create headers (Cmd/Ctrl + Shift + R), display code navigation drop-bottom-left editor:","code":"\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\ncollapse_years()\n# Never do this!\ncol_mins <- function(x, y) {}\nrowMaxes <- function(y, x) {}\n# Good\ninput_select()\ninput_checkbox()\ninput_text()\n\n# Not so good\nselect_input()\ncheckbox_input()\ntext_input()\n# Don't do this!\nT <- FALSE\nc <- 10\nmean <- function(x) sum(x)\n# Load data --------------------------------------\n\n# Plot data --------------------------------------"},{"path":"functionsHOP.html","id":"exercises-5","chapter":"19 Functions","heading":"19.3.1 Exercises","text":"Read source code following three functions, puzzle , brainstorm better names.\n\nf1 <- function(string, prefix) {\n  substr(string, 1, nchar(prefix)) == prefix\n}\nf2 <- function(x) {\n  (length(x) <= 1) return(NULL)\n  x[-length(x)]\n}\nf3 <- function(x, y) {\n  rep(y, length.= length(x))\n}Read source code following three functions, puzzle , brainstorm better names.Take function ’ve written recently spend 5 minutes brainstorming better name arguments.Take function ’ve written recently spend 5 minutes brainstorming better name arguments.Compare contrast rnorm() MASS::mvrnorm().\nmake consistent?Compare contrast rnorm() MASS::mvrnorm().\nmake consistent?Make case norm_r(), norm_d() etc better rnorm(), dnorm().\nMake case opposite.Make case norm_r(), norm_d() etc better rnorm(), dnorm().\nMake case opposite.","code":"\nf1 <- function(string, prefix) {\n  substr(string, 1, nchar(prefix)) == prefix\n}\nf2 <- function(x) {\n  if (length(x) <= 1) return(NULL)\n  x[-length(x)]\n}\nf3 <- function(x, y) {\n  rep(y, length.out = length(x))\n}"},{"path":"functionsHOP.html","id":"conditional-execution","chapter":"19 Functions","heading":"19.4 Conditional execution","text":"statement allows conditionally execute code.\nlooks like :get help need surround backticks: ?``.\nhelp isn’t particularly helpful ’re already experienced programmer, least know get !’s simple function uses statement.\ngoal function return logical vector describing whether element vector named.function takes advantage standard return rule: function returns last value computed.\neither one two branches statement.","code":"\nif (condition) {\n  # code executed when condition is TRUE\n} else {\n  # code executed when condition is FALSE\n}\nhas_name <- function(x) {\n  nms <- names(x)\n  if (is.null(nms)) {\n    rep(FALSE, length(x))\n  } else {\n    !is.na(nms) & nms != \"\"\n  }\n}"},{"path":"functionsHOP.html","id":"conditions-1","chapter":"19 Functions","heading":"19.4.1 Conditions","text":"condition must evaluate either TRUE FALSE.\n’s vector, ’ll get warning message; ’s NA, ’ll get error.\nWatch messages code:can use || () && () combine multiple logical expressions.\noperators “short-circuiting”: soon || sees first TRUE returns TRUE without computing anything else.\nsoon && sees first FALSE returns FALSE.\nnever use | & statement: vectorised operations apply multiple values (’s use filter()).\nlogical vector, can use () () collapse single value.careful testing equality.\n== vectorised, means ’s easy get one output.\nEither check length already 1, collapse () (), use non-vectorised identical().\nidentical() strict: always returns either single TRUE single FALSE, doesn’t coerce types.\nmeans need careful comparing integers doubles:also need wary floating point numbers:Instead use dplyr::near() comparisons, described [comparisons].remember, x == NA doesn’t anything useful!","code":"\nif (c(TRUE, FALSE)) {}\n#> NULL\n\nif (NA) {}\n#> Error in if (NA) {: missing value where TRUE/FALSE needed\nidentical(0L, 0)\n#> [1] FALSE\nx <- sqrt(2) ^ 2\nx\n#> [1] 2\nx == 2\n#> [1] FALSE\nx - 2\n#> [1] 4.440892e-16"},{"path":"functionsHOP.html","id":"multiple-conditions","chapter":"19 Functions","heading":"19.4.2 Multiple conditions","text":"can chain multiple statements together:end long series chained statements, consider rewriting.\nOne useful technique switch() function.\nallows evaluate selected code based position name.Another useful function can often eliminate long chains statements cut().\n’s used discretise continuous variables.","code":"\nif (this) {\n  # do that\n} else if (that) {\n  # do something else\n} else {\n  # \n}#> function(x, y, op) {\n#>   switch(op,\n#>     plus = x + y,\n#>     minus = x - y,\n#>     times = x * y,\n#>     divide = x / y,\n#>     stop(\"Unknown op!\")\n#>   )\n#> }"},{"path":"functionsHOP.html","id":"code-style","chapter":"19 Functions","heading":"19.4.3 Code style","text":"function (almost) always followed squiggly brackets ({}), contents indented two spaces.\nmakes easier see hierarchy code skimming left-hand margin.opening curly brace never go line always followed new line.\nclosing curly brace always go line, unless ’s followed else.\nAlways indent code inside curly braces.’s ok drop curly braces short statement can fit one line:recommend brief statements.\nOtherwise, full form easier read:","code":"# Good\nif (y < 0 && debug) {\n  message(\"Y is negative\")\n}\n\nif (y == 0) {\n  log(x)\n} else {\n  y ^ x\n}\n\n# Bad\nif (y < 0 && debug)\nmessage(\"Y is negative\")\n\nif (y == 0) {\n  log(x)\n} \nelse {\n  y ^ x\n}\ny <- 10\nx <- if (y < 20) \"Too low\" else \"Too high\"\nif (y < 20) {\n  x <- \"Too low\" \n} else {\n  x <- \"Too high\"\n}"},{"path":"functionsHOP.html","id":"exercises-6","chapter":"19 Functions","heading":"19.4.4 Exercises","text":"’s difference ifelse()?\nCarefully read help construct three examples illustrate key differences.’s difference ifelse()?\nCarefully read help construct three examples illustrate key differences.Write greeting function says “good morning”, “good afternoon”, “good evening”, depending time day.\n(Hint: use time argument defaults lubridate::now().\nmake easier test function.)Write greeting function says “good morning”, “good afternoon”, “good evening”, depending time day.\n(Hint: use time argument defaults lubridate::now().\nmake easier test function.)Implement fizzbuzz function.\ntakes single number input.\nnumber divisible three, returns “fizz”.\n’s divisible five returns “buzz”.\n’s divisible three five, returns “fizzbuzz”.\nOtherwise, returns number.\nMake sure first write working code create function.Implement fizzbuzz function.\ntakes single number input.\nnumber divisible three, returns “fizz”.\n’s divisible five returns “buzz”.\n’s divisible three five, returns “fizzbuzz”.\nOtherwise, returns number.\nMake sure first write working code create function.use cut() simplify set nested -else statements?\n\n(temp <= 0) {\n  \"freezing\"\n} else (temp <= 10) {\n  \"cold\"\n} else (temp <= 20) {\n  \"cool\"\n} else (temp <= 30) {\n  \"warm\"\n} else {\n  \"hot\"\n}\nchange call cut() ’d used < instead <=?\nchief advantage cut() problem?\n(Hint: happens many values temp?)use cut() simplify set nested -else statements?change call cut() ’d used < instead <=?\nchief advantage cut() problem?\n(Hint: happens many values temp?)happens use switch() numeric values?happens use switch() numeric values?switch() call ?\nhappens x “e”?\n\nswitch(x, \n  = ,\n  b = \"ab\",\n  c = ,\n  d = \"cd\"\n)\nExperiment, carefully read documentation.switch() call ?\nhappens x “e”?Experiment, carefully read documentation.","code":"\nif (temp <= 0) {\n  \"freezing\"\n} else if (temp <= 10) {\n  \"cold\"\n} else if (temp <= 20) {\n  \"cool\"\n} else if (temp <= 30) {\n  \"warm\"\n} else {\n  \"hot\"\n}\nswitch(x, \n  a = ,\n  b = \"ab\",\n  c = ,\n  d = \"cd\"\n)"},{"path":"functionsHOP.html","id":"function-arguments","chapter":"19 Functions","heading":"19.5 Function arguments","text":"arguments function typically fall two broad sets: one set supplies data compute , supplies arguments control details computation.\nexample:log(), data x, detail base logarithm.log(), data x, detail base logarithm.mean(), data x, details much data trim ends (trim) handle missing values (na.rm).mean(), data x, details much data trim ends (trim) handle missing values (na.rm).t.test(), data x y, details test alternative, mu, paired, var.equal, conf.level.t.test(), data x y, details test alternative, mu, paired, var.equal, conf.level.str_c() can supply number strings ..., details concatenation controlled sep collapse.str_c() can supply number strings ..., details concatenation controlled sep collapse.Generally, data arguments come first.\nDetail arguments go end, usually default values.\nspecify default value way call function named argument:default value almost always common value.\nexceptions rule safety.\nexample, makes sense na.rm default FALSE missing values important.\nEven though na.rm = TRUE usually put code, ’s bad idea silently ignore missing values default.call function, typically omit names data arguments, used commonly.\noverride default value detail argument, use full name:can refer argument unique prefix (e.g. mean(x, n = TRUE)), generally best avoided given possibilities confusion.Notice call function, place space around = function calls, always put space comma, (just like regular English).\nUsing whitespace makes easier skim function important components.","code":"\n# Compute confidence interval around mean using normal approximation\nmean_ci <- function(x, conf = 0.95) {\n  se <- sd(x) / sqrt(length(x))\n  alpha <- 1 - conf\n  mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2))\n}\n\nx <- runif(100)\nmean_ci(x)\n#> [1] 0.4607160 0.5816921\nmean_ci(x, conf = 0.99)\n#> [1] 0.4417093 0.6006988\n# Good\nmean(1:10, na.rm = TRUE)\n\n# Bad\nmean(x = 1:10, , FALSE)\nmean(, TRUE, x = c(1:10, NA))\n# Good\naverage <- mean(feet / 12 + inches, na.rm = TRUE)\n\n# Bad\naverage<-mean(feet/12+inches,na.rm=TRUE)"},{"path":"functionsHOP.html","id":"choosing-names","chapter":"19 Functions","heading":"19.5.1 Choosing names","text":"names arguments also important.\nR doesn’t care, readers code (including future-!) .\nGenerally prefer longer, descriptive names, handful common, short names.\n’s worth memorising :x, y, z: vectors.w: vector weights.df: data frame., j: numeric indices (typically rows columns).n: length, number rows.p: number columns.Otherwise, consider matching names arguments existing R functions.\nexample, use na.rm determine missing values removed.","code":""},{"path":"functionsHOP.html","id":"checking-values","chapter":"19 Functions","heading":"19.5.2 Checking values","text":"start write functions, ’ll eventually get point don’t remember exactly function works.\npoint ’s easy call function invalid inputs.\navoid problem, ’s often useful make constraints explicit.\nexample, imagine ’ve written functions computing weighted summary statistics:happens x w length?case, R’s vector recycling rules, don’t get error.’s good practice check important preconditions, throw error (stop()), true:careful take far.\n’s tradeoff much time spend making function robust, versus long spend writing .\nexample, also added na.rm argument, probably wouldn’t check carefully:lot extra work little additional gain.\nuseful compromise built-stopifnot(): checks argument TRUE, produces generic error message .Note using stopifnot() assert true rather checking might wrong.","code":"\nwt_mean <- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_var <- function(x, w) {\n  mu <- wt_mean(x, w)\n  sum(w * (x - mu) ^ 2) / sum(w)\n}\nwt_sd <- function(x, w) {\n  sqrt(wt_var(x, w))\n}\nwt_mean(1:6, 1:3)\n#> [1] 7.666667\nwt_mean <- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  }\n  sum(w * x) / sum(w)\n}\nwt_mean <- function(x, w, na.rm = FALSE) {\n  if (!is.logical(na.rm)) {\n    stop(\"`na.rm` must be logical\")\n  }\n  if (length(na.rm) != 1) {\n    stop(\"`na.rm` must be length 1\")\n  }\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  }\n  \n  if (na.rm) {\n    miss <- is.na(x) | is.na(w)\n    x <- x[!miss]\n    w <- w[!miss]\n  }\n  sum(w * x) / sum(w)\n}\nwt_mean <- function(x, w, na.rm = FALSE) {\n  stopifnot(is.logical(na.rm), length(na.rm) == 1)\n  stopifnot(length(x) == length(w))\n  \n  if (na.rm) {\n    miss <- is.na(x) | is.na(w)\n    x <- x[!miss]\n    w <- w[!miss]\n  }\n  sum(w * x) / sum(w)\n}\nwt_mean(1:6, 6:1, na.rm = \"foo\")\n#> Error in wt_mean(1:6, 6:1, na.rm = \"foo\"): is.logical(na.rm) is not TRUE"},{"path":"functionsHOP.html","id":"dot-dot-dot","chapter":"19 Functions","heading":"19.5.3 Dot-dot-dot (…)","text":"Many functions R take arbitrary number inputs:functions work?\nrely special argument: ... (pronounced dot-dot-dot).\nspecial argument captures number arguments aren’t otherwise matched.’s useful can send ... another function.\nuseful catch-function primarily wraps another function.\nexample, commonly create helper functions wrap around str_c():... lets forward arguments don’t want deal str_c().\n’s convenient technique.\ncome price: misspelled arguments raise error.\nmakes easy typos go unnoticed:just want capture values ..., use list(...).","code":"\nsum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n#> [1] 55\nstringr::str_c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\n#> [1] \"abcdef\"\ncommas <- function(...) stringr::str_c(..., collapse = \", \")\ncommas(letters[1:10])\n#> [1] \"a, b, c, d, e, f, g, h, i, j\"\n\nrule <- function(..., pad = \"-\") {\n  title <- paste0(...)\n  width <- getOption(\"width\") - nchar(title) - 5\n  cat(title, \" \", stringr::str_dup(pad, width), \"\\n\", sep = \"\")\n}\nrule(\"Important output\")\n#> Important output ---------------------------------------\nx <- c(1, 2)\nsum(x, na.mr = TRUE)\n#> [1] 4"},{"path":"functionsHOP.html","id":"lazy-evaluation","chapter":"19 Functions","heading":"19.5.4 Lazy evaluation","text":"Arguments R lazily evaluated: ’re computed ’re needed.\nmeans ’re never used, ’re never called.\nimportant property R programming language, generally important ’re writing functions data analysis.\ncan read lazy evaluation http://adv-r..co.nz/Functions.html#lazy-evaluation.","code":""},{"path":"functionsHOP.html","id":"exercises-7","chapter":"19 Functions","heading":"19.5.5 Exercises","text":"commas(letters, collapse = \"-\") ?\n?commas(letters, collapse = \"-\") ?\n?’d nice supply multiple characters pad argument, e.g. rule(\"Title\", pad = \"-+\").\ndoesn’t currently work?\nfix ?’d nice supply multiple characters pad argument, e.g. rule(\"Title\", pad = \"-+\").\ndoesn’t currently work?\nfix ?trim argument mean() ?\nmight use ?trim argument mean() ?\nmight use ?default value method argument cor() c(\"pearson\", \"kendall\", \"spearman\").\nmean?\nvalue used default?default value method argument cor() c(\"pearson\", \"kendall\", \"spearman\").\nmean?\nvalue used default?","code":""},{"path":"functionsHOP.html","id":"return-values","chapter":"19 Functions","heading":"19.6 Return values","text":"Figuring function return usually straightforward: ’s created function first place!\ntwo things consider returning value:returning early make function easier read?returning early make function easier read?Can make function pipeable?Can make function pipeable?","code":""},{"path":"functionsHOP.html","id":"explicit-return-statements","chapter":"19 Functions","heading":"19.6.1 Explicit return statements","text":"value returned function usually last statement evaluates, can choose return early using return().\nthink ’s best save use return() signal can return early simpler solution.\ncommon reason inputs empty:Another reason statement one complex block one simple block.\nexample, might write statement like :first block long, time get else, ’ve forgotten condition.\nOne way rewrite use early return simple case:tends make code easier understand, don’t need quite much context understand .","code":"\ncomplicated_function <- function(x, y, z) {\n  if (length(x) == 0 || length(y) == 0) {\n    return(0)\n  }\n    \n  # Complicated code here\n}\nf <- function() {\n  if (x) {\n    # Do \n    # something\n    # that\n    # takes\n    # many\n    # lines\n    # to\n    # express\n  } else {\n    # return something short\n  }\n}\n\nf <- function() {\n  if (!x) {\n    return(something_short)\n  }\n\n  # Do \n  # something\n  # that\n  # takes\n  # many\n  # lines\n  # to\n  # express\n}"},{"path":"functionsHOP.html","id":"writing-pipeable-functions","chapter":"19 Functions","heading":"19.6.2 Writing pipeable functions","text":"want write pipeable functions, ’s important think return value.\nKnowing return value’s object type mean pipeline “just work”.\nexample, dplyr tidyr object type data frame.two basic types pipeable functions: transformations side-effects.\ntransformations, object passed function’s first argument modified object returned.\nside-effects, passed object transformed.\nInstead, function performs action object, like drawing plot saving file.\nSide-effects functions “invisibly” return first argument, ’re printed can still used pipeline.\nexample, simple function prints number missing values data frame:call interactively, invisible() means input df doesn’t get printed :’s still , ’s just printed default:can still use pipe:","code":"\nshow_missings <- function(df) {\n  n <- sum(is.na(df))\n  cat(\"Missing values: \", n, \"\\n\", sep = \"\")\n  \n  invisible(df)\n}\nshow_missings(mtcars)\n#> Missing values: 0\nx <- show_missings(mtcars) \n#> Missing values: 0\nclass(x)\n#> [1] \"data.frame\"\ndim(x)\n#> [1] 32 11\nmtcars %>% \n  show_missings() %>% \n  mutate(mpg = ifelse(mpg < 20, NA, mpg)) %>% \n  show_missings() \n#> Missing values: 0\n#> Missing values: 18"},{"path":"functionsHOP.html","id":"environment","chapter":"19 Functions","heading":"19.7 Environment","text":"last component function environment.\nsomething need understand deeply first start writing functions.\nHowever, ’s important know little bit environments crucial functions work.\nenvironment function controls R finds value associated name.\nexample, take function:many programming languages, error, y defined inside function.\nR, valid code R uses rules called lexical scoping find value associated name.\nSince y defined inside function, R look environment function defined:behaviour seems like recipe bugs, indeed avoid creating functions like deliberately, large doesn’t cause many problems (especially regularly restart R get clean slate).advantage behaviour language standpoint allows R consistent.\nEvery name looked using set rules.\nf() includes behaviour two things might expect: { +.\nallows devious things like:common phenomenon R.\nR places limits power.\ncan many things can’t programming languages.\ncan many things 99% time extremely ill-advised (like overriding addition works!).\npower flexibility makes tools like ggplot2 dplyr possible.\nLearning make best use flexibility beyond scope course, can read Advanced R.","code":"\nf <- function(x) {\n  x + y\n} \ny <- 100\nf(10)\n#> [1] 110\n\ny <- 1000\nf(10)\n#> [1] 1010\n`+` <- function(x, y) {\n  if (runif(1) < 0.1) {\n    sum(x, y)\n  } else {\n    sum(x, y) * 1.1\n  }\n}\ntable(replicate(1000, 1 + 2))\n#> \n#>   3 3.3 \n#> 106 894\nrm(`+`)"},{"path":"packages.html","id":"packages","chapter":"20 Packages","heading":"20 Packages","text":"section adapted Epidemiologist R Handbook125Packages shareable collections R code can contain functions, data, /documentation.\nPackages increase functionality R providing access additional functions suit variety needs.\nentirely possible work R without ever using package, recommend approach. wealth packages available, almost help reduce learning curve associated R amount time spent given analytical project.","code":""},{"path":"packages.html","id":"installing-a-package-with-install.packages-not-recommended","chapter":"20 Packages","heading":"20.1 Installing a package with install.packages (not recommended)","text":"order access functions within package, must first install package computer. collection R packages hosted internet CRAN website: CRAN(https://cran.r-project.org/), Comprehensive R Archive Network. packages must meet certain quality standards, regularly tested.R user feels package benefit broad audience, may choose submit package CRAN. process submitting package published CRAN beyond scope Chapter, ’s important point can create package use , share colleagues, submit CRAN. packages ’ll working book available CRAN, means can install using install.packages() function.package CRAN, can install running following code RStudio Console:Note name package needs inside quotation marks using install.packages() function.can run install.packages() functions within .R script! However choose , please make sure comment line(s) code install packages installed packages. Commenting install packages commands save time future need re-install packages time run script.","code":"\n# template for installing a package\ninstall.packages(\"package_name\")\n\n# example of installing a package\ninstall.packages(\"dplyr\")"},{"path":"packages.html","id":"pacman","chapter":"20 Packages","heading":"20.2 Installing a package with pacman (recommended approach)","text":"highly recommend using pacman package install packages instead using install.packages. first need install pacman install.packages(\"pacman\"), , never need use install.packages. course emphasize p_load() pacman, installs package necessary loads use current R session. way, can always keep commands R script without need comment line installing package.recommend begin R scripts packages need follows. Notice package name separate line, makes easy comment don’t need . Furthermore, package brief comment ’s utility. Also notice package names quotations. behaviour different install.packages requires package name quotation marks.","code":"\n# ------- packages-----------------------------------------------------------\n\n# This script uses the p_load() function from pacman R package, \n# which installs if package is absent, and loads for use if already installed\n\n# Ensures the package \"pacman\" is installed\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\n\npacman::p_load(\n     \n     # project and file management\n     #############################\n     here,     # file paths relative to R project root folder\n     rio,      # import/export of many types of data\n     openxlsx, # import/export of multi-sheet Excel workbooks \n     \n     # package install and management\n     ################################\n     remotes,  # install from github\n     \n     # General data management\n     #########################\n     tidyverse,    # includes many packages for tidy data wrangling and presentation\n          #dplyr,      # data management\n          #tidyr,      # data management\n          #ggplot2,    # data visualization\n          #stringr,    # work with strings and characters\n          #forcats,    # work with factors \n          #lubridate,  # work with dates\n          #purrr       # iteration and working with lists\n\n     # statistics  \n     ############\n     janitor,      # tables and data cleaning\n     gtsummary,    # making descriptive and statistical tables\n     broom,        # tidy up results from regressions\n\n     # plots - general\n     #################\n     #ggplot2,         # included in tidyverse\n     cowplot,          # combining plots  \n     # patchwork,      # combining plots (alternative)     \n     RColorBrewer,     # color scales\n     viridis,          # color-blind friendly palettes\n     \n     # routine reports\n     #################\n     knitr,\n     rmarkdown,        # produce PDFs, Word Documents, Powerpoints, and HTML files\n\n     # tables for presentation\n     #########################\n     flextable,        # HTML tables\n     kableExtra        # more customizable knitr::kable\n)"},{"path":"packages.html","id":"loading-a-package","chapter":"20 Packages","heading":"20.3 Loading a package","text":"package installed computer, re-install order use functions package. However, every time open RStudio want use package, need load package RStudio environment. way, R know look functions. can accomplish loading package R environment using p_load function pacman package.Loading package R environment signals R like access functions available us package. can load package, {dplyr} package,126 using following code:install package , use , load time start new R session.18,000 packages available CRAN. likely function names identical two different libraries. example, plyr Hmisc provide summarize() function. load plyr, Hmisc, summarize() refer Hmisc version. load packages opposite order, summarize() refer plyr version. can confusing. Instead, can explicitly refer specific functions: Hmisc::summarize() plyr::summarize(). order packages loaded won’t matter.:: operator makes explicit package function . might noticed Section 20.2 , loading packages, used:can read “use p_load function pacman library”. use convention using functions specific packages. avoid ambiguities.","code":"\nlibrary(pacman) # load the pacman library to acceess all its functions\n\n# template for loading a package\np_load(package_name)  \n\n# example of loading a package\np_load(dplyr)\npacman::p_load()"},{"path":"packages.html","id":"the-relationship-between-packages-and-functions","chapter":"20 Packages","heading":"20.4 The Relationship Between Packages and Functions","text":"Packages collection functions, designed specific dataset, field, /set tasks.\nFunctions individual components within package use interact data.put another way, R user might write series functions find needing use repeatedly variety projects.\nInstead re-writing (copying pasting) functions time need use , R user can collect individual functions inside package.\ncan load package time want use functions, using single line code instead tens tens thousands lines code.","code":""},{"path":"packages.html","id":"how-to-find-packages","chapter":"20 Packages","heading":"20.5 How to Find Packages","text":"begin R learning journey, bulk packages need use either already included install R available CRAN.\nCRAN TaskViews (https://cran.r-project.org/web/views/) one best resources seeing packages available might relevant work.\ngreat resources learn various R packages Twitter (following “#rstats” hashtag) well Google searches.\nR grown popularity, Google gotten significantly better returning R-related results.","code":""},{"path":"packages.html","id":"learning-more-about-a-package","chapter":"20 Packages","heading":"20.6 Learning More About a Package","text":"Sometimes look package, able identify function need continue way. times, may need (want!) learn specific package. Packages CRAN might come something called “vignette”, worked example using various functions within package. can access package’s vignette(s) CRAN TaskViews.Packages need submitted CRAN used public, many available directly respective developers via GitHub. Package authors may publish vignettes blog posts package, R users may also publish tutorials specific package. find GitHub looking information package, often , README file good information getting started package.example, Figure 20.1, show CRAN homepage casebase package.\nFigure 20.1: CRAN homepage casebase package. Much information package can found landing page. might also find information package website can found URL link boxed red.\n","code":""},{"path":"import.html","id":"import","chapter":"21 Data Import and Export","heading":"21 Data Import and Export","text":"section adapted Epidemiologist R Handbook127In page describe ways locate, import, export files:Use rio package flexibly import() export() many types filesUse package locate files relative R project root - prevent complications file paths specific one computerSpecific import scenarios, :\nSpecific Excel sheets\nMessy headers skipping rows\nGoogle sheets\ndata posted websites\nAPIs\nImporting recent file\nSpecific Excel sheetsMessy headers skipping rowsFrom Google sheetsFrom data posted websitesWith APIsImporting recent fileManual data entryR-specific file types RDS RDataExporting/saving files plots","code":""},{"path":"import.html","id":"overview-1","chapter":"21 Data Import and Export","heading":"21.1 Overview","text":"import “dataset” R, generally creating new data frame object R environment defining imported file (e.g. Excel, CSV, TSV, RDS) located folder directories certain file path/address.can import/export many types files, including created statistical programs (SAS, STATA, SPSS). can also connect relational databases.R even data formats:RDS file (.rds) stores single R object data frame. useful store cleaned data, maintain R column classes. Read section.RData file (.Rdata) can used store multiple objects, even complete R workspace. Read section.","code":""},{"path":"import.html","id":"the-rio-package","chapter":"21 Data Import and Export","heading":"21.2 The rio package","text":"R package recommend : rio. name “rio” abbreviation “R /O” (input/output).functions import() export() can handle many different file types (e.g. .xlsx, .csv, .rds, .tsv). provide file path either functions (including file extension like “.csv”), rio read extension use correct tool import export file.alternative using rio use functions many packages, specific type file. example, read.csv() (base R), read.xlsx() (openxlsx package), write_csv() (readr pacakge), etc. alternatives can difficult remember, whereas using import() export() rio easy.rio’s functions import() export() use appropriate package function given file, based file extension. See end page complete table packages/functions rio uses background. can also used import STATA, SAS, SPSS files, among dozens file types.","code":""},{"path":"import.html","id":"here","chapter":"21 Data Import and Export","heading":"21.3 The here package","text":"package function () make easy tell R find save files - essence, builds file paths.Used conjunction [R Project][projects], allows describe location files R Project relation R Project’s root directory (top-level folder). useful R project may shared accessed multiple people/computers. prevents complications due unique file paths different computers (e.g. \"C:/Users/Laura/Documents...\" “starting” file path place common users (R Project root). See Chapter 18 reminder R Projects.() works within R Project:package first loaded within R Project, places small file called “.” root folder R project “benchmark” “anchor”scripts, reference file R project’s sub-folders, use function () build file path relation anchorTo build file path, write names folders beyond root, within quotes, separated commas, finally ending file name file extension shown belowhere() file paths can used importing exportingFor example, , function import() provided file path constructed ().command ::(\"data\", \"linelists\", \"ebola_linelist.xlsx\") actually providing full file path unique user’s computer:beauty R command using () can successfully run computer accessing R project.unsure “.” root set , run function () empty parentheses. Read package link.","code":"\nlinelist <- rio::import(here::here(\"data\", \"linelists\", \"ebola_linelist.xlsx\"))\"C:/Users/Laura/Documents/my_R_project/data/linelists/ebola_linelist.xlsx\""},{"path":"import.html","id":"file-paths","chapter":"21 Data Import and Export","heading":"21.4 File paths","text":"importing exporting data, must provide file path. can one three ways:Recommended: provide “relative” file path packageProvide “full” / “absolute” file pathManual file selection","code":""},{"path":"import.html","id":"relative-file-paths","chapter":"21 Data Import and Export","heading":"21.4.1 “Relative” file paths","text":"R, “relative” file paths consist file path relative root R project. allow simple file paths can work different computers (e.g. R project shared drive sent email). described , relative file paths facilitated use package.example relative file path constructed () . Figure 21.1 show example directory computer, epib607.Rproj file. referred root directory project. file paths relative directory. example root directory /home/sahir/git_repositories/epib607.\nFigure 21.1: files epib607 R project. (1) R project file - indicates indeed created R project called epib607. .Rporj file always found root directory. (2) path directory seen file management application Finder (mac) Windows Explorer. (3) directory contains data.\ndata want load sub-folder called “inst” within , subfolder “data”, .rds file interest shown Figure 21.2\nFigure 21.2: sub-folder containing data want load. (1) linelist_cleaned.rds data file. (2) path directory seen file management application Finder (mac) Windows Explorer. (3) Indicator indeed RStudio project called epib607.\nload data, simply call rio::import() function. function requires location data file. use ::() locate datafile:","code":"\nknitr::include_graphics(here::here(\"inst\",\"figures\",\"proj1.png\"))\nknitr::include_graphics(here::here(\"inst\",\"figures\",\"proj2.png\"))\nlinelist <- rio::import(here::here(\"inst\",\"data\", \"linelist_cleaned.rds\"))"},{"path":"import.html","id":"absolute-file-paths","chapter":"21 Data Import and Export","heading":"21.4.2 “Absolute” file paths","text":"Absolute “full” file paths can provided functions like import() “fragile” unique user’s specific computer therefore recommended.example absolute file path, Laura’s computer folder “analysis”, sub-folder “data” within sub-folder “linelists”, .xlsx file interest.things note absolute file paths:Avoid using absolute file paths break script run different computerUse forward slashes (/), example (note: default Windows file paths)File paths begin double slashes (e.g. “//…”) likely recognized R produce error. Consider moving work “named” “lettered” drive begins letter (e.g. “J:” “C:”).One scenario absolute file paths may appropriate want import file shared drive full file path users.TIP: quickly convert \\ /, highlight code interest, use Ctrl+f (Windows), check option box “selection”, use replace functionality convert .","code":"\nlinelist <- rio::import(\"/home/sahir/git_repositories/epib607/inst/data/linelist_cleaned.rds\")"},{"path":"import.html","id":"import-data-from-excel","chapter":"21 Data Import and Export","heading":"21.5 Import data from Excel","text":"default, provide Excel workbook (.xlsx) rio::import(), workbook’s first sheet imported. want import specific sheet, include sheet name = argument. example:using () method provide relative pathway import(), can still indicate specific sheet adding = argument closing parentheses () function.","code":"\nmy_data <- rio::import(\"my_excel_file.xlsx\", which = \"Sheetname\")\n# Demonstration: importing a specific Excel sheet when using relative pathways with the 'here' package\nlinelist_raw <- rio::import(here::here(\"data\", \"linelist.xlsx\"), which = \"Sheet1\")  "},{"path":"import.html","id":"import_missing","chapter":"21 Data Import and Export","heading":"21.6 Missing values","text":"may want designate value(s) dataset considered missing. value R missing data NA, perhaps dataset want import uses 99, “Missing”, just empty character space \"\" instead.Use na = argument rio::import() provide value(s) within quotes (even numbers). can specify multiple values including within vector, using c() shown ., value “99” imported dataset considered missing converted NA R., values “Missing”, \"\" (empty cell), \" \" (single space) imported dataset converted NA R.","code":"\nlinelist <- rio::import(here::here(\"data\", \"my_linelist.xlsx\"), na = \"99\")\nlinelist <- rio::import(here::here(\"data\", \"my_linelist.csv\"), na = c(\"Missing\", \"\", \" \"))"},{"path":"import.html","id":"skip-rows","chapter":"21 Data Import and Export","heading":"21.7 Skip rows","text":"Sometimes, may want avoid importing row data. can argument skip = using import() rio .xlsx .csv file. Provide number rows want skip.Unfortunately skip = accepts one integer value, range (e.g. “2:10” work).","code":"\nlinelist_raw <- rio::import(\"linelist_raw.xlsx\", skip = 1)  # does not import header row"},{"path":"import.html","id":"import-from-google-sheets","chapter":"21 Data Import and Export","heading":"21.8 Import from Google sheets","text":"can import data online Google spreadsheet googlesheet4 package authenticating access spreadsheet., demo Google sheet imported saved. command may prompt confirmation authentification Google account. Follow prompts pop-ups internet browser grant Tidyverse API packages permissions edit, create, delete spreadsheets Google Drive.sheet “viewable anyone link” can try import .sheet can also imported using sheet ID, shorter part URL:Another package, googledrive offers useful functions writing, editing, deleting Google sheets. example, using gs4_create() sheet_write() functions found package.helpful online tutorials:basic Google sheets importing tutorialmore detailed tutorialinteraction googlesheets4 tidyverse","code":"\npacman::p_load(\"googlesheets4\")\nGsheets_demo <- read_sheet(\"https://docs.google.com/spreadsheets/d/1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY/edit#gid=0\")\nGsheets_demo <- read_sheet(\"1scgtzkVLLHAe5a6_eFQEwkZcc14yFUx1KgOMZ4AKUfY\")"},{"path":"import.html","id":"import_github","chapter":"21 Data Import and Export","heading":"21.9 Import from Github","text":"Importing data directly Github R can easy can require steps - depending file type. approaches:","code":""},{"path":"import.html","id":"csv-files","chapter":"21 Data Import and Export","heading":"21.9.1 CSV files","text":"can easy import .csv file directly Github R R command.Go Github repo https://github.com/sahirbhatnagar/knitr-tutorial, locate file 001-motivating-example/fat-data.csv, click itClick “Raw” button (see “raw” csv data, shown )Copy URL (web address)Place URL quotes within import() R command\nFigure 14.4: Getting link data .csv format GitHub repository.\n’ve copied URL address file, can use rio::import function follows:","code":"\ndf <- rio::import(\"https://raw.githubusercontent.com/sahirbhatnagar/knitr-tutorial/master/001-motivating-example/fat-data.csv\")"},{"path":"import.html","id":"xlsx-files","chapter":"21 Data Import and Export","heading":"21.9.2 XLSX files","text":"may able view “Raw” data files (e.g. .xlsx, .rds, .nwk, .shp)Go Github repo, locate file interest, click itClick “Download” button, shown belowSave file computer, import R","code":""},{"path":"import.html","id":"manual-data-entry","chapter":"21 Data Import and Export","heading":"21.10 Manual data entry","text":"","code":""},{"path":"import.html","id":"entry-by-rows","chapter":"21 Data Import and Export","heading":"21.10.1 Entry by rows","text":"Use tribble function tibble package tidyverse (online tibble reference).Note column headers start tilde (~). Also note column must contain one class data (character, numeric, etc.). can use tabs, spacing, new rows make data entry intuitive readable. Spaces matter values, row represented new line code. example:now display new dataset:","code":"\n# create the dataset manually by row\nmanual_entry_rows <- tibble::tribble(\n  ~colA, ~colB,\n  \"a\",   1,\n  \"b\",   2,\n  \"c\",   3\n  )"},{"path":"import.html","id":"entry-by-columns","chapter":"21 Data Import and Export","heading":"21.10.2 Entry by columns","text":"Since data frame consists vectors (vertical columns), base approach manual dataframe creation R expects define column bind together. can counter-intuitive epidemiology, usually think data rows ().CAUTION: vectors must length (number values).vectors can bound together using function data.frame():now display new dataset:","code":"\n# define each vector (vertical column) separately, each with its own name\nPatientID <- c(235, 452, 778, 111)\nTreatment <- c(\"Yes\", \"No\", \"Yes\", \"Yes\")\nDeath     <- c(1, 0, 1, 0)\n# combine the columns into a data frame, by referencing the vector names\nmanual_entry_cols <- data.frame(PatientID, Treatment, Death)"},{"path":"import.html","id":"pasting-from-clipboard","chapter":"21 Data Import and Export","heading":"21.10.3 Pasting from clipboard","text":"copy data elsewhere clipboard, can try using datapasta R package allows import data frame shown Figure 21.3:\nFigure 21.3: Copying data clipboard pasting data frame R.\n","code":""},{"path":"import.html","id":"export","chapter":"21 Data Import and Export","heading":"21.11 Export","text":"","code":""},{"path":"import.html","id":"with-rio-package","chapter":"21 Data Import and Export","heading":"21.11.1 With rio package","text":"rio, can use rio::export() function similar way rio::import(). First give name R object want save (e.g. linelist) quotes put file path want save file, including desired file name file extension. example:saves data frame linelist Excel workbook working directory (R Project root folder):save data frame csv file changing extension. example, also save file path constructed ():","code":"\nrio::export(linelist, \"my_linelist.xlsx\") # will save to working directory\nrio::export(linelist, here::here(\"data\", \"clean\", \"my_linelist.csv\"))"},{"path":"import.html","id":"import_rds","chapter":"21 Data Import and Export","heading":"21.12 RDS files","text":"Along .csv, .xlsx, etc, can also export/save R data frames .rds files. file format specific R, useful know work exported data R.classes columns stored, don’t cleaning imported (Excel even CSV file can headache!). also smaller file, useful export import dataset large.","code":"\nrio::export(linelist, here::here(\"data\", \"clean\", \"my_linelist.rds\"))"},{"path":"import.html","id":"import_rdata","chapter":"21 Data Import and Export","heading":"21.13 Rdata files and lists","text":".Rdata files can store multiple R objects - example multiple data frames, model results, lists, etc. can useful consolidate share lot data given project.example, multiple R objects stored within exported file “my_objects.Rdata”:Note: trying import list, use rio::import_list() import complete original structure contents.","code":"\nrio::export(my_list, my_dataframe, my_vector, \"my_objects.Rdata\")\nrio::import_list(\"my_list.Rdata\")"},{"path":"import.html","id":"saving-plots","chapter":"21 Data Import and Export","heading":"21.14 Saving plots","text":"Instructions save plots, created ggplot(), discussed depth [ggplot basics] page.brief, run ggsave(\"my_plot_filepath_and_name.png\") printing plot. can either provide saved plot object plot = argument, specify destination file path (file extension) save recently-displayed plot. can also control width =, height =, units =, dpi =.save network graph, transmission tree, addressed page [Transmission chains].","code":""},{"path":"import.html","id":"resources","chapter":"21 Data Import and Export","heading":"21.15 Resources","text":"R Data Import/Export ManualR 4 Data Science chapter data importggsave() documentationBelow table, taken rio online vignette. type data shows: expected file extension, package rio uses import export data, whether functionality included default installed version rio.","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"transition-to-r-from-excel-stata-sas","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"22 Transition to R from Excel, Stata, SAS","text":"entire section reproduced Epidemiologist R Handbook128Below, provide advice resources transitioning R.R introduced late 1990s since grown dramatically scope. capabilities extensive commercial alternatives reacted R developments order stay competitive! (read article comparing R, SPSS, SAS, STATA, Python).Moreover, R much easier learn 10 years ago. Previously, R reputation difficult beginners. now much easier friendly user-interfaces like RStudio, intuitive code like tidyverse, many tutorial resources.intimidated - come discover world R!","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"from-excel","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"22.1 From Excel","text":"Transitioning Excel directly R achievable goal. may seem daunting, can !true someone strong Excel skills can advanced activities Excel alone - even using scripting tools like VBA. Excel used across world essential tool epidemiologist. However, complementing R can dramatically improve expand work flows.","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"benefits","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"Benefits","text":"find using R offers immense benefits time saved, consistent accurate analysis, reproducibility, shareability, faster error-correction. Like new software learning “curve” time must invest become familiar. dividends significant immense scope new possibilities open R.Excel well-known software can easy beginner use produce simple analysis visualizations “point--click”. comparison, can take couple weeks become comfortable R functions interface. However, R evolved recent years become much friendly beginners.Many Excel workflows rely memory repetition - thus, much opportunity error. Furthermore, generally data cleaning, analysis methodology, equations used hidden view. can require substantial time new colleague learn Excel workbook troubleshoot . R, steps explicitly written script can easily viewed, edited, corrected, applied datasets.begin transition Excel R must adjust mindset important ways:","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"tidy-data-1","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"Tidy data","text":"Use machine-readable “tidy” data instead messy “human-readable” data. three main requirements “tidy” data, explained tutorial “tidy” data R:variable must columnEach observation must rowEach value must cellTo Excel users - think role Excel “tables” play standardizing data making format predictable.example “tidy” data case linelist used throughout handbook - variable contained within one column, observation (one case) ’s row, every value just one cell. can view first 50 rows linelist:main reason one encounters non-tidy data many Excel spreadsheets designed prioritize easy reading humans, easy reading machines/software.help see difference, fictional examples non-tidy data prioritize human-readability machine-readability:Problems: spreadsheet , merged cells easily digested R. row considered “header” clear. color-based dictionary right side cell values represented colors - also easily interpreted R (humans color-blindness!). Furthermore, different pieces information combined one cell (multiple partner organizations working one area, status “TBC” cell “Partner D”).Problems: spreadsheet , numerous extra empty rows columns within dataset - cause cleaning headaches R. Furthermore, GPS coordinates spread across two rows given treatment center. side note - GPS coordinates two different formats!“Tidy” datasets may readable human eye, make data cleaning analysis much easier! Tidy data can stored various formats, example “long” “wide”, principles still observed.","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"functions","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"Functions","text":"R word “function” might new, concept exists Excel formulas. Formulas Excel also require precise syntax (e.g. placement semicolons parentheses). need learn new functions work together R.","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"scripts","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"Scripts","text":"Instead clicking buttons dragging cells writing every step procedure “script”.\nExcel users may familiar “VBA macros” also employ scripting approach.R script consists step--step instructions. allows colleague read script easily see steps took. also helps de-bug errors inaccurate calculations.example R script:","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"excel-to-r-resources","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"Excel-to-R resources","text":"links tutorials help transition R Excel:R vs. ExcelRStudio course R Excel users","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"r-excel-interaction","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"R-Excel interaction","text":"R robust ways import Excel workbooks, work data, export/save Excel files, work nuances Excel sheets.true aesthetic Excel formatting can get lost translation (e.g. italics, sideways text, etc.). work flow requires passing documents back--forth R Excel retaining original Excel formatting, try packages openxlsx.","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"from-stata","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"22.2 From Stata","text":"Coming R StataMany epidemiologists first taught use Stata, can seem daunting move R. However, comfortable Stata user jump R certainly manageable might think. key differences Stata R data can created modified, well analysis functions implemented – learning key differences able translate skills.key translations Stata R, may handy review guide.General notesWorking directoryImporting viewing dataBasic data manipulationDescriptive analysisWhile list gives overview basics translating Stata commands R, exhaustive. many great resources Stata users transitioning R interest:https://dss.princeton.edu/training/RStata.pdfhttps://clanfear.github.io/Stata_R_Equivalency/docs/r_stata_commands.htmlhttp://r4stats.com/books/r4stata/","code":""},{"path":"transition-to-r-from-excel-stata-sas.html","id":"from-sas","chapter":"22 Transition to R from Excel, Stata, SAS","heading":"22.3 From SAS","text":"Coming SAS RSAS commonly used public health agencies academic research fields. Although transitioning new language rarely simple process, understanding key differences SAS R may help start navigate new language using native language.\noutlines key translations data management descriptive analysis SAS R.General notesWorking directoryImporting viewing dataBasic data manipulationDescriptive analysis","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Ayanian, John Z, Elizabeth Chrischilles, Robert B Wallace, Robert H Fletcher, Mona N Fouad, Catarina Kiefe, David P Harrington, et al. “Understanding Cancer Treatment Outcomes: Cancer Care Outcomes Research Surveillance Consortium.” Journal Clinical Oncology 22, . 15 (2004): 2992–6.Batra, Neale, Mathilde Mousset, Alex Spina, Isaac Florence, Liza Coyer, Aminata Ndiaye, Henry Laurenson-Schafer, et al. Epidemiologist R Handbook. Zenodo, 2021. https://epirhandbook.com/.Bovee, Estrellado, E. . Data Science Education Using R. London, England: Routledge, 2020. http://www.datascienceineducation.com/.Brewer, Cynthia . “ColorBrewer 2.0. Color Advice Cartography,” 2017. http://www.ColorBrewer.org.Chen, W, ZH Tang, XG Fan, Y Wang, DA Pike. “Maternal Investment Increases Altitude Frog Tibetan Plateau.” Journal Evolutionary Biology 26, . 12 (2013): 2710–5.Clarkson, Priscilla M, Eric P Hoffman, Edward Zambraski, Heather Gordish-Dressman, Amy Kearns, Monica Hubal, Brennan Harmon, Joseph M Devaney. “ACTN3 Mlck Genotype Associations Exertional Muscle Damage.” Journal Applied Physiology 99, . 2 (2005): 564–69.Clayton, David, Michael Hills. Statistical Models Epidemiology. OUP Oxford, 2013.Çetinkaya-Rundel, Mine, Johanna Hardin. Introduction Modern Statistics. OpenIntro, 2021. https://openintro-ims.netlify.app/.Du Toit, George, Graham Roberts, Peter H Sayre, Henry T Bahnson, Suzana Radulovic, Alexandra F Santos, Helen Brough, et al. “Randomized Trial Peanut Consumption Infants Risk Peanut Allergy.” N Engl J Med 372 (2015): 803–13.Foulkes, Andrea S. “Genetic Association Studies.” Applied Statistical Genetics R, 1–27. Springer, 2009.Galton, Francis. “Regression Towards Mediocrity Hereditary Stature.” Journal Anthropological Institute Great Britain Ireland 15 (1886): 246–63.Greenstein, Robert J, James McElhinney, Devaprasad Reuben, Adrian J Greenstein. “Colonic Vascular Ectasias Aortic Stenosis: Coincidence Causal Relationship?” American Journal Surgery 151, . 3 (1986): 347–51.Healy, Kieran. Data Visualization: Practical Introduction. Princeton University Press, 2018.———. Data Visualization: Practical Introduction. Princeton: Princeton University Press, 2019. http://www.socviz.co.Heyde, EC. “Gastrointestinal Bleeding Aortic Stenosis.” N Engl J Med 259 (1958): 196.Jorenby, Douglas E, J Taylor Hays, Nancy Rigotti, Salomon Azoulay, Eric J Watsky, Kathryn E Williams, Clare B Billing, et al. “Efficacy Varenicline, \\(\\alpha\\)4\\(\\beta\\)2 Nicotinic Acetylcholine Receptor Partial Agonist, Vs Placebo Sustained-Release Bupropion Smoking Cessation: Randomized Controlled Trial.” Jama 296, . 1 (2006): 56–63.Meier, Paul, others. “Biggest Public Health Experiment Ever: 1954 Field Trial Salk Poliomyelitis Vaccine.” Statistics: Guide Unknown. San Francisco: Holden-Day, 1972, 2–13.Miettinen, Olli S. “Theoretical Epidemiology: Principles Occurrence Research Medicine.” Theoretical Epidemiology: Principles Occurrence Research Medicine, xxii–359, 1985.Miettinen, Olli Sakari. Epidemiological Research: Terms Concepts. Springer Science & Business Media, 2011.Okabe, M., K. Ito. “Color Universal Design (CUD): Make Figures Presentations Friendly Colorblind People,” 2008. http://jfly.iam.u-tokyo.ac.jp/color/.R Core Team. R: Language Environment Statistical Computing. Vienna, Austria: R Foundation Statistical Computing, 2019. https://www.R-project.org/.RStudio Team. RStudio: Integrated Development Environment R. Boston, MA: RStudio, Inc., 2015. http://www.rstudio.com/.Stone, M., D. Albers Szafir, V. Setlur. “Engineering Model Color Difference Function Size.” 22nd Color Imaging Conference. Society Imaging Science Technology, 2014.Telford, R. D., R. B. Cunningham. “Sex, Sport, Body-Size Dependency Hematology Highly Trained Athletes.” Medicine Science Sports Exercise 23 (1991): 788–94.Thompson, Paul D, Niall Moyna, Richard Seip, Thomas Price, Priscilla Clarkson, Theodore Angelopoulos, Paul Gordon, et al. “Functional Polymorphisms Associated Human Muscle Size Strength.” Medicine & Science Sports & Exercise 36, . 7 (2004): 1132–9.Vu, Julie, David Harrington. Introductory Statistics Life Biomedical Sciences, 2020. https://www.openintro.org/book/biostat/.Wickham, Hadley. Ggplot2: Elegant Graphics Data Analysis. Springer-Verlag New York, 2016. https://ggplot2.tidyverse.org.———. “Tidy Data.” Journal Statistical Software 59, . 10 (2014). https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf.Wickham, Hadley, Romain François, Lionel Henry, Kirill Müller. Dplyr: Grammar Data Manipulation, 2021. https://CRAN.R-project.org/package=dplyr.Wickham, Hadley, Garrett Grolemund. R Data Science. O’Reilly, 2018.Wilke, Claus. Fundamentals Data Visualization, 2020. https://clauswilke.com/dataviz/.Yakir, Benjamin. “Introduction Statistical Thinking (R, Without Calculus).” Hebrew University, 2011.Zeileis, Achim, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D. McWhite, Paul Murrell, Reto Stauffer, Claus O. Wilke. “colorspace: Toolbox Manipulating Assessing Colors Palettes.” Journal Statistical Software 96, . 1 (2020): 1–49. https://doi.org/10.18637/jss.v096.i01.","code":""}]
